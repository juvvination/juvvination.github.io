{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has [79 videos!! in this yt playlist](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o). I am creating one section per chapter for now.\n",
    "\n",
    "# Additional videos/resources after this\n",
    " - [Jay Alammar's illustrated transformer](https://jalammar.github.io/illustrated-transformer/) _all of his other blog posts are worth a read too they say_\n",
    "\n",
    "# Installing\n",
    "\n",
    "```shell\n",
    "mamba install transformers\n",
    "```\n",
    "\n",
    "## Fetching models and tokenizers\n",
    "\n",
    "There are tons of different models (_different networks and weights_) and tokenizers. I think the tokenizers are a byproduct of the training of the model on it's inputs: they are also neural.\n",
    "\n",
    "Flesh this section out from https://huggingface.co/docs/transformers/installation as needed.\n",
    "\n",
    "# Boilerplate\n",
    "\n",
    "Import transformers as well as some diagramming libs (_ `nb_js_diagrammers` allows mermaid and others while `iplantuml` allows plantuml magics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# The next two are for my diagramming. Nothing to do with HF\n",
    "%load_ext nb_js_diagrammers\n",
    "import iplantuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Video 2 - Pipeline function\n",
    "\n",
    "Uses the `pipeline` object from `transformers`. Has many different types of pipelines (_presumably composed of multiple pieces per the name_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%plantuml\n",
    "@startmindmap\n",
    "* pipeline\n",
    "** **sentiment-analysis**\n",
    "** **zero-shot-classification** classfies along user supplied categories\n",
    "** Hello\n",
    "@endmindmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot classification\n",
    "\n",
    "zero-shot would mean, no additional learning needed. This takes a bunch of user supplied labels and classifies input text into those (as weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Are these models auto quantized to git GPU memory ?\n",
    " - defaulted to `gpt2`\n",
    " - Specified `distilgpt2` was 330M in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated text pipeline\n",
    "# Defaulted to gpt2\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this couse, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill mask\n",
    "\n",
    "> Defaulted to `distilroberta-base` which was 331M in size.\n",
    "\n",
    "Filling some missing piece of a text\n",
    "\n",
    "The following code will print out two most likely completions for the missing word. For each\n",
    " - Lists the `token_str` which replaces the masked word\n",
    " - The `score` which is the probability of seeing this completion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "> This defaulted to `dbmdz/bert-large-cased-finetuned-conll03-english` which was 1.3G in size\n",
    "\n",
    "```python\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn\")\n",
    "```\n",
    "\n",
    "results in the following json.\n",
    "\n",
    "```json\n",
    "[{'entity_group': 'PER',\n",
    "  'score': 0.9986171,\n",
    "  'word': 'Sylvain',\n",
    "  'start': 11,\n",
    "  'end': 18},\n",
    " {'entity_group': 'ORG',\n",
    "  'score': 0.97779936,\n",
    "  'word': 'Hugging Face',\n",
    "  'start': 33,\n",
    "  'end': 45},\n",
    " {'entity_group': 'LOC',\n",
    "  'score': 0.9889684,\n",
    "  'word': 'Brooklyn',\n",
    "  'start': 49,\n",
    "  'end': 57}]\n",
    "```\n",
    "\n",
    "The args\n",
    " - `Grouped Entities` means to group multiple words that belong to the same entity together. `Hugging` and `Face` \n",
    "\n",
    "- This is important for me so should study this in more detail. \n",
    "- Also how to use the Spacy visualizers to visualize this info ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=False)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "> defaults to distilbert-base-cased-distilled-squad which is 261M in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "> defaults to sshleifer/distilbart-cnn-12-6 which is 1.2G in size!\n",
    "\n",
    "The summary for the following article is \n",
    "\n",
    "_' First time U.S. troops have been killed by enemy fire in the Middle East since Gaza war . US officials say drone attack was launched by Iran-backed militants and appeared to come from Syria . President Joe Biden vows to hold those responsible for the attack on a US base in Jordan .'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked a random thing from the internet instead of coppying his text\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "US President Joe Biden vowed to hold \"to account\" those responsible for a drone attack on a US outpost in Jordan, which killed three US Army soldiers and injured at least 34, marking the first time US troops have been killed by enemy fire in the Middle East since the beginning of the Gaza war.\n",
    "\n",
    "US Central Command confirmed the deaths and said eight personnel had to be medically evacuated from Jordan. The number of wounded is expected to rise.\n",
    "\n",
    "The Islamic Resistance in Iraq, an umbrella group for several Iran-backed militias in the country, said it attacked a number of places along the Jordan-Syria border on Sunday â€” including a camp near the US base in Jordan where soldiers were killed.\n",
    "\n",
    "US officials have said the drone that killed the US service members at Tower 22 was launched by Iran-backed militants and appeared to come from Syria. The US government has not yet named a specific militia they hold responsible.\n",
    "\n",
    "Iran denied it played any role in the attack.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "Translates from one language to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 4 - What is transfer learning\n",
    "\n",
    "**Leverage the knowledge in a model trained for a specific task on a new task**\n",
    " - Initialize weights from model A onto model B\n",
    " - Use foundational models (_those trained on lots of data_) and transfer that knowledge to one trained on a smaller dataset.\n",
    " - The head (_last layer in the deep network_) of the source model is thrown away (_replaced with random weights_) prior to fine-tuning. \n",
    "   - If an image recongnition model needs to be used in a cat/dog classification task say. Then \n",
    "     - replace head with a 2 output classification layer (_random weights_)\n",
    "     - train on smaller dog/cat dataset\n",
    " - Pick a pre-trained model as close in objectives to the final model.\n",
    "\n",
    "ðŸ‘‰ Transfer learning based on a foundational model (_with lots of data_) ends up being way more accurate than training from scratch on your specialized task's dataset. ImageNet is trained on 1.2 million images for instance and BERT on a huge word corpus.\n",
    " - ImageNet's 1.2 million images are supervized (_labels provided for each image_)\n",
    " - GPT-2 was trained on 40GB of internet data that was pre-processed heavily.\n",
    " - BERT etc are unsupervized and so work on way more larger images. Their process is to guess the next word (_already known so no need to provide supervized data_) etc where the correct answer is already known.\n",
    "\n",
    "> Note that the transfer learning inherits the learning and the biases.\n",
    "> - ImageNet has mostly US and Western EU images\n",
    "> - Text generation for female pronouns have a bias toward physical characteristics as opposed to make pronouns\n",
    "\n",
    "\n",
    "![](./img/transfer-learning-concept.png \"Transfer Learning concept\")\n",
    "\n",
    "![](./img/transfer-learning-heads.png \"Transfer Learning head change\")\n",
    "\n",
    "# Video 5,6,7,8 - The transformer network - encoders, decoders and encoder-decoders\n",
    "\n",
    "> Note: ðŸ‘‰ Incomplete notes. After a whiel I just watched all of them to get an idea. Eventually Watch all 4 videos to take further notes as needed. For now, I have some superficial understanding.\n",
    "\n",
    "> Note: You can choose separate encoder and decoder models to make up an encoder-decoder set based on what they each excel at. \n",
    ">\n",
    "> Wonder if masked language generation can be used for co-reference resolution ?\n",
    "\n",
    "Shows arch from the original Vaswani paper: _Attention is all you need_. Simplifies it to an `encode` & `decoder` block to teach some high-level concepts\n",
    "\n",
    "![](./img/transformers-details.png \"Transformers in detail\")\n",
    "\n",
    "![Encoder decoder blocks](./img/transformers-encoder-decoder-blocks.png \"Encode Decoder Blocks\")\n",
    "\n",
    " - Watch them all.\n",
    " - Encoder\n",
    "   - ðŸ‘‰ Embeds semantic understanding of the training material\n",
    "   - converts text to numbers. Embeddings (_I understand_) or feautures he says.\n",
    "   - accepts inputs and converts them to a high-level representation\n",
    "     - Calls them feature vector (_also embeddings I guess. Feature space is neurally learned_)     \n",
    "   - `bi-directional` here means that it takes into account words that come to the left and those that come to the right of the word being encoded.\n",
    "   - `self-attention` seems to mean that the word is not encoded in isolation but uses the context of surrounding words (_both left and word in bi-directional schemes_)\n",
    " - Decoder\n",
    "   - `auto-regressive`\n",
    "   - `uni-directional`: The attention context of a word is just the words to the left (_usually_) or right.\n",
    "     - also referred to as `masked self-attention` where a part of the context is masked (_from being the context i.e._)\n",
    "     - If usding left-context, this would be great at generating words that come to the right: completion tasks.\n",
    "   - Uses outputs from encoder alongside other inputs to generate a prediction. This prediction will be re-used in future iterations, hence the term: `auto-regressive`\n",
    "     - In a completion task, is it adding a single token, then back to input and add one more token etc ?\n",
    "- can be used together (`sequence-to-sequence`) or separately.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "![](./img/encoder-schematic.png)\n",
    "\n",
    "Take the example of encoding a sentence\n",
    "\n",
    "![](./img/encoder-words-to-vec.png)\n",
    " - One sequence of numbers (_a vector of some fixed dimension_) per input word\n",
    "   - The size of the vector is the _dimensionality_ of the vector space of the features.\n",
    "   - Naively assigning one word to a dimension will lead to a high-dimensional space (_high compute and storage expense not to mention sparse_)\n",
    "   - Can pick randomly smaller dimension and see how the neural nets create the embedding as the training progresses. _Embedding is the act of embedding a feature into a fixed dimensional space with low to no loss of knowledge_\n",
    " - Each vector is a feature vector or tensor. \n",
    " - **Each word _in the initial sequence_ affects the representation of every other word in the sequence**. _Is the sequence a sentence?_\n",
    "\n",
    "**Bi-directionality** \n",
    "\n",
    "![](./img/encoder-bi-directionlaity.png)\n",
    "  - The representation contains the value of a word (_more precisely, it's centextual embedding_)\n",
    "    - **Contextual**: This is not the representation of just the word. It is the value in the context of it's surrounding words. bi-directional to include words to it's left and to it's right. Hence _contextualized value_. This is done via the _self-attention_ mechanism.\n",
    "    - **Embedding** embedding a word from a high-dimensional space (one dim per word) into a lower dimensional space\n",
    "\n",
    "### Applicability of encoder only models\n",
    "\n",
    " - Good at extracting meaningful information _because of the bi-directional context_ ?\n",
    " - Sequence clasification (Sentiment analysis)\n",
    " - Question Answering\n",
    " - **Masked language modeling** (MLM): Guessing a randomly masked word in a sequence/sentence. _This is pretty much the main training method for bi-directional encoding so it stands to reason that they do well here_\n",
    " - NLU (_Natural language modeling_)\n",
    " - Examples: BERT, RoBERTa, ALBERT\n",
    "\n",
    "## Decoder\n",
    "\n",
    "![](./img/decoder-schematic.png)\n",
    "\n",
    "![](./img/decoder-words-to-vec.png)\n",
    "\n",
    " - Similar to encoder in that this also converts words to vectors. Each word to a fixed-dimension vector/tensor.\n",
    " - Similarly also called a feature-vector or tensor (_also an embedding into a usually lower dimensional space_)\n",
    "\n",
    "![](./img/decoder-vec-per-word.png)\n",
    "\n",
    "**One big difference with encoders is that these are unidirectional** i.e., the context included (_attention_) is in just one direction: either to the left or the words that is being decoded.\n",
    " - This is called _masked self attention_\n",
    " - If the words to the right are removed from the context, then they are considered being _masked from attention_.\n",
    "\n",
    "![](./img/decoder-words-to-vec-unidirectional.png)\n",
    "\n",
    "**When should these be used**\n",
    " - The strength is mainly in that the context is restricted to the **left** (_usually. Can be right too I guess if the language is left-to-right_). Unidirectionlaity is a strength here because it enables specific tasks.\n",
    " - Great at causal tasks: Generating sequences given a left context (_for languages that go left-to-right_)\n",
    " - **NLG**: Natural language generation\n",
    " - Examples: GPT-2, GPT Neo\n",
    "\n",
    "Example:\n",
    " - Start with _My_ as the input to the decoder\n",
    " - Model outpus _name_ as the most likely next word (_actually outputs a number-sequence, which is mapped by a language-modeling-head to a word_). Language modeling head is the final layer that converts the low dimentional vector to a single word in a high dimensional space ? De-Embedding is a thing ?\n",
    " - **This is where the auto-regressive** part comes in\n",
    " - Now add this generated word to the previous input sequence and send in _My name_ to the model\n",
    " - Repeat till a stop condition (_end of sentence of some special token_) is received\n",
    " - _My_ â†’ _My name_ â†’ _My name is_ â†’ _My name is Sylvain._\n",
    " - Starting from a single word, we have a full sentence!\n",
    "\n",
    "> When they say GPT-2 has a context of 1024 words (_tokens actually but simplify_). This means litreally the _left context_, while generating the 1023r'd word, it still has memory of the first word in the sequence. After that the generation loses the context of the first word.\n",
    "\n",
    "## Encoder Decoder\n",
    "\n",
    "The two work together in the following way\n",
    " - Encoder _Words â†’ Contexted Vec_\n",
    " - Send encoder output to Decoder\n",
    " - Give decoder a sequence **alongside** the encoder output\n",
    "   - Initial seq is empty: some sentinel value\n",
    "   - Keep looping like usual till end of generation. _Note that all generation is in context of the encoder output_\n",
    "\n",
    "The following shows a concrete langauge translation (_sequence to sequence transduction_) which makes it clearer\n",
    " - The _encoded(\"Welcome to NYC\")_ is the context for the generation. _Likely training used english vector + actual french translation vector_ for training so they have semantic similarity or some such.\n",
    " - Generator generates\n",
    "   - _encoded(\"Welcome to NYC\")_: START â†’ `Bienvenu`\n",
    "   - _encoded(\"Welcome to NYC\")_: `Bienvenu` â†’ `a`\n",
    "   - _encoded(\"Welcome to NYC\")_: `Bienvenu a` â†’ `NYC`\n",
    "   - _encoded(\"Welcome to NYC\")_: `Bienvenu a NYC` â†’ STOP\n",
    "\n",
    "![](./img/encoder-decoder-translation-task.png)\n",
    "\n",
    "Encoder and decoder are different networks with different weights and embeddings. In above case\n",
    " - Encoder understands english sentences\n",
    " - Decoder can generate french in the context of the encoder\n",
    "\n",
    "### Where do they shine\n",
    "\n",
    " - Sequence to sequence tasks\n",
    "   - Many to many\n",
    "   - Translation\n",
    "   - Summarization\n",
    " - Output length is independent of input length (_In the case of language translation for sure_)\n",
    "  - Summarization for instance is always smaller than the input text\n",
    "  - Elaboration usually more\n",
    "  - Translation between languages can be any\n",
    "  - This allows for different context sizes between encoders and decoders (_just operational for GPU memory or is this relevant along other dimensions too ?_)\n",
    " - Examples\n",
    "   - BART\n",
    "   - ProphetNet\n",
    "   - mT5\n",
    "   - M2m100\n",
    "   - T5\n",
    "   - Pegasus\n",
    "   - MarianMT\n",
    "   - mBART\n",
    "   - and many others\n",
    " - Can mix and match _I am sure there are details here_. _Encoder : Decoder_ pairs. Pick each for their proven worth on specific tasks.\n",
    "   - BERT    : GPT-2\n",
    "   - BERT    : BERT\n",
    "   - RoBERTa : RoBERTa   \n",
    "\n",
    "\n",
    "# Video 9, 10 - What happens inside a pipeline function (pyTorch or otherwise)\n",
    "\n",
    "Uses the `sentiment-analysis` pipeline as an example\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "  \"I hate this so much!\",\n",
    "])\n",
    "```\n",
    "\n",
    "![](./img/pipeline-classifier-flow.png)\n",
    " - Tokenizer: Text â†’ Vectors/Numbers\n",
    " - Model : Numbers â†’ Logits\n",
    " - PostProcessing: Logits â†’ Predictions\n",
    "\n",
    "## Tokenizer\n",
    "\n",
    "![](./img/pipeline-tokenizer-details.png)\n",
    "\n",
    " - Raw text  â†’ Tokens _(whole pre-processing steps here_)\n",
    " - Tokens â†’ Sentencified Tokens _(Add begin/end special tokens_)\n",
    " - Sentencified Tokens  â†’ InputID _(map tokens to numbers based on vocabulary. Lookups or embeddings_) \n",
    "\n",
    "![](./img/pipeline-tokenizer-details-code.png)\n",
    "\n",
    " - AutoTokenizer loads tokenizer for any model checkpoint. `AutoTokenizer.from_pretrained(<checkpoint>)`\n",
    " - Truncation asks to truncate any input longer than what the model can handle. _Is there a warning though ?_\n",
    " - Makes all inputs the same lengths by 0 padding \n",
    " - `attention_mask` is nice to see. Simply masks out the padding in this case.\n",
    " - `return_tensors=\"pt\"` asks to return a pyTorch tensor\n",
    "\n",
    "## Model\n",
    "\n",
    "![](./img/pipeline-model-details.png)\n",
    " - Convert input_id's (_numbers_) to logits (_what it is?_)\n",
    "\n",
    "### AutoModel\n",
    "\n",
    "> AutoModel is meant to be used only for finetuning ? If one of the `AutoModelForXXX` is not suitable, then load the base pretrained model and then transfer train it ?\n",
    "\n",
    "![](./img/pipeline-model-details-code.png)\n",
    " - `AutoModel.from_pretrained(<checkpoint>)` loads a model **without its pretraining head**. _What!! What does it replace it with then ? I thought this was only for transfer-learning in which case a new head has to be trained_\n",
    " - This outputs a `high-dimensional tensor` (_ok, since this is a sentiment model, removal of the head means removal of the final layer which has two dimensions (pos, neg). Why remove it ?_). \n",
    " - The second last-layer (_A hidden layer per deep learning. Input and Output layers are named and the rest are hidden_)\n",
    " - `print(outputs.last_hidden_state.shape)` shows the dimension as `tensor(2, 16, 768)`\n",
    "  - 2 sentences (_array of sequences_)\n",
    "  - of size 16 (_sequence length_)\n",
    "  - hidden size of 768 (_neurons in last layer essentially_)\n",
    "\n",
    "### AutoModelForXXX\n",
    "\n",
    "> Each AutoModelForXXX method loads a model suitable for a specific task at hand. This includes a task-specific head!. Note that sequence classifier uses `distilbert-base-uncased-finetuned-sst-2-english`. So likely:\n",
    "> \n",
    "> Base â†’ `distilbert-base-uncased` which is then finetuned with \n",
    "> Head â†’ `sst2-english` (_sequence sentiment 2 outputs ?_)\n",
    ">\n",
    "> There is one AutoModel for each NLP task in the transformers library\n",
    "\n",
    "![](./img/pipeline-modelForClassification-details-code.png)\n",
    "\n",
    "The outputs from this stage are not probabilities yet. These are logits\n",
    "\n",
    "## Post Processing\n",
    "\n",
    "![](./img/pipeline-postprocessing-details.png)\n",
    "\n",
    " - `torch.nn.functional.softmax(output.logits, dim=-1)` is used to apply a [softmax layer](https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78)\n",
    "   - Usually the last layer in any DNN. \n",
    "   - Scales logits to probabilities\n",
    "- `model.config.is2label` provides the labels for the id/index which map to the index/order of the proability outputs. So in this case, map output `{0.0402, 0.9598}` using `{0: 'NEGATIVE', 1 : 'POSITIVE'}` to `{'NEGATIVE': 4.02%, 'POSITIVE': 95.98%}`\n",
    "\n",
    "\n",
    "## Impressions\n",
    "\n",
    "I now see what one of the internet comments was saying about this being very transparent under the hood. I wonder how much Sylvain and his FastAI background, interation with Jeremy contributed here: a lot I would think. This is great architecture aiming for generality and simplicity.\n",
    "\n",
    "\n",
    "# Video 11 - Instantiate a Transformers model\n",
    "\n",
    "`AutoModel` allows one to load any model from the hub: `AutoModel.from_pretrained(\"checkpointName\")`\n",
    " - Downloads `config` and `model` file\n",
    " - `checkpoint | local folder`\n",
    " - What each of these mean are likely a p\n",
    " - if _folder_ needs a valid config file and a weights file\n",
    "\n",
    "![](./img/v11-instantiate-via-automodel.png)\n",
    "\n",
    "![](./img/v11-instantiate-via-automodel-details.png)\n",
    "\n",
    "## Loading Config\n",
    "\n",
    "> If you want to modify the config before loading the model ?\n",
    ">\n",
    "> Uses `AutoConfig`\n",
    "\n",
    "![](./img/v11-instantiate-config.png)\n",
    "\n",
    "## Whats in a config\n",
    "\n",
    "![](./img/v11-whats-in-a-config.png)\n",
    "\n",
    "## Train a checkpointed model from scratch\n",
    "\n",
    "While I am not entirely clear about what the `Config`, `ConfigClass`, `ModelClass`, `Model`, `Architecture` do. Likely there is some redundancy and loose terminology. Nevertheless, if there is a checkpointed (_and hence useful_) trained model. You can use it on your own data with customizations as needed. For instance, to train a blank `BERT` model with a change: `num_hidden_layers: 12 â†’ 10`, do the following.\n",
    "\n",
    "![](./img/v11-instantiate-bert-train-from-scratch.png)\n",
    "\n",
    "> Note: A config loaded from a `checkpoint` is a collection of config values that worked in the past.  There could be several other BERT models with different configs on different input corpuses.\n",
    "\n",
    "### Saving after training\n",
    "\n",
    "```python\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_model  = BertModel(bert_config)\n",
    "\n",
    "# Training code\n",
    "# Data, epochs and all that\n",
    "\n",
    "# Save..\n",
    "# This will go to the process's CWD\n",
    "bert_model.save_pretrained(\"my_bert_model\")\n",
    "```\n",
    "\n",
    "> This can then be pushed to the Hub as well if intended for publishing.\n",
    "\n",
    "### Load saved custom model\n",
    "\n",
    "> From CWD. Can we specify a folder name as well ?\n",
    "\n",
    "```python\n",
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"my-bert-model\")\n",
    "```\n",
    "\n",
    "# Video 13, 14, 15, 16 - Tokenizers\n",
    "\n",
    "> Also see [NLP Word Embeddings.ipynb](./NLP%20Word%20Embeddings.ipynb) and [NLP_TraditionalTextFeatureEngineering](./NLP_TraditionalTextFeatureEngineering.ipynb)\n",
    "\n",
    "## History and motivation\n",
    "\n",
    "A typical neural-net's input layer takes N numerical inputs and outputs M numerical outputs. Using such a network on non-numerical data requires the conversion of such data to numbers in the first place and then converting the final output unmbers to the output domain (usually different from the input domain).\n",
    "\n",
    "In the case of an image-classifier for instance, the inputs are the pixels (_one input per pixel. Hence the usual limitation on image size to avoid blowing up input parameters_) and the outputs can point to one of N available labels. Even here, to reduce data headaches, the outputs might map the label strings to IDs. A simple nunmberical mapping from `text â†’ int`\n",
    "\n",
    "In the case of text though, you have to map the input text to numbers as well. Say you are operating the universe of words, we could consider mapping each word to a unique ID. One way of doing this is a 1:1 mapping like [one hot encoding](https://en.wikipedia.org/wiki/One-hot) which each word will map to a distinct value/vector (_where only one bit is on: the hot bit_). This way each bit can map to one of the inputs of the neural-net. The problems start arising when you consider that the english language alone has roughly 170k words. Thats a 170k vector per word and a tremendous amount of matrix math on huge matrices where the vectors are very sparse.\n",
    "\n",
    "With this bg, we can look at the other videos that try to explain what tokenization (_the process of converting words or whatever the input is to inputIDs or tokens_) \n",
    "\n",
    "\n",
    "## Word based\n",
    "\n",
    " - Split a sentence into words (_split on spaces, punctuations etc_)\n",
    " - Each word gets assigned an ID/Code/Token\n",
    " - A word has some semantic information (_used in certain contexts and with certain words etc_) so this type of tokensization is expected to be powerful.\n",
    " - However `dog` and `dogs` can get two entirely different tokens. _Not sure why this is being presented as a big issue. Isn't lemmatization that is done in traditional NLP pre-processing done here as well. Or we want to learn about dog and dogs and hence this issue ?_\n",
    " - 170,000 words in just english so potentially 170,000 codes. Thats a large vocabulary and hence a large vector. Huge compute and storage needs.\n",
    " - Strategy to limit vocabulary size\n",
    "   - Limit to 10,000 most popular words. Any unknown words gets an `OOV / UNKNOWN` token for _Out of vocabulary_\n",
    "   - Compromize because all unknowsn words have the same rep and a problem when using new novel text that has a lot of OOV words.\n",
    "\n",
    "## Character based\n",
    "\n",
    "This has been developed to handle the flaws of character based tokensization.\n",
    "\n",
    " - Split text into chars instead of words\n",
    " - Unlike large word vocab, a limited character vocab _256 is usually enough ?_\n",
    " - **Even never seen words wil still be composed mostly of the same chars. So we'll have a much smaller chances of OOV chars**\n",
    " - However, characters don't hold as much information as words.\n",
    " - Large number of input tokens since there are a lot of characters vs words in a given input text.\n",
    "\n",
    "## Subword based\n",
    "\n",
    "Developed to overcome the shortcomings in both word-based and character based encodings.\n",
    "\n",
    "**word based**\n",
    "  - Very large vocabularies\n",
    "  - Large quantity of OOV tokens when limiting vocab size\n",
    "  - Loss of meaning across similar words (dog, dogs for instance)\n",
    "\n",
    "**character based**\n",
    "  - very long sequences (_1 token per char_)\n",
    "  - less meaningul individual tokens\n",
    "\n",
    "\n",
    "The current (2024) guidelines seem to be:\n",
    " - Frequently used words should not be split into smaller subwords\n",
    " - Rare words should be decomposed into meaningful subwords\n",
    "\n",
    "![](./img/v16-subword-decomposition-1.png) \n",
    "\n",
    "![](./img/v16-subword-decomposition-rare-words-1.png)\n",
    "\n",
    "### Detecting meaning across words\n",
    "\n",
    "Same root or same extension allows the model to understand syntactic or semantic similarity in text.\n",
    "\n",
    "![](./img/v16-subword-decomposition-rare-words-2.png)\n",
    "\n",
    "\n",
    "### Marking start-of-word or suffix tokens\n",
    "\n",
    "To help with whether a token is a common suffix or a common start word, some models signal it via the token itself. For instance, `BERT` starts a prefix token with `##` (_based on word bees? algo_). Other models may do things differently.\n",
    "\n",
    "![](./img/v16-subword-mark-suffix-token.png)\n",
    "\n",
    "\n",
    "There are many different sub-word tokenization algorithms that can be used to generated tokens.\n",
    "\n",
    "![](./img/v16-subword-tokenization-algos.png)\n",
    "\n",
    "> What about neurally learned tokenization ? I thought fixed algo tokenization is usually superceded by the learned varieties.\n",
    "\n",
    "\n",
    "# Video 17 - The tokenizer pipeline\n",
    "\n",
    "The tokenizer takes text as input and outputs numbers the associated model can make use of\n",
    " - You can see it use subword tokens since input length and output length differ (_could also be terminals etc_)\n",
    "\n",
    "![](./img/v17-using-tokenizer-1.png)\n",
    "\n",
    "\n",
    "![](./img/v17-inside-tokenizer.png)\n",
    "\n",
    "## Tokenize first: text to subword tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# bert uses ## preceding suffixes.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Lets try to tokenize!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Albert based tokenizers put a _ infront of all words that have a space in front of them.\n",
    "# Apparently a convention shared by all sentence based tokenizers. What it is\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "tokens = tokenizer.tokenize(\"Lets try to tokenize!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs next: string tokens to numbers\n",
    "\n",
    "Here we perform the final part of the tokenization by mapping the textual tokens to numbers. Note that the same tokensize that generated the string tokens should be used to convert them to numeric IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# bert uses ## preceding suffixes.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Let's try to tokenize!\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ready IDs last: Add special tokens\n",
    "\n",
    "Turns out we need to perform a final step of adding special tokens (sentence-start, sentence-end etc. Whats the exact list here?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode input_ids back to text tokens\n",
    "\n",
    "There is a method to go the reverse way. For example, to check what the text-tokens correspondong to the special tokens are, we can run the model-ready inputs-ids through the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These can be decoded into text as well\n",
    "# ok. So Decode is from input_id to text tag\n",
    "print(tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta tokensizer uses HTML style special tokens \n",
    "# Note the use of tokensizer(\"string\") instead of tokenizer.tokenize(\"string\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "print((inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 18,19 - Batching inputs together\n",
    "\n",
    "The standard HF API, allows multiple inputs (sentences) to be sent in for it's various pipelines (_sentiment analysis etc_). \n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this.\",\n",
    "]\n",
    "```\n",
    "\n",
    "This seems painless. The `padding=True` seems out of place (_shouldn't it do this automatically ?_) but otherwise, all is simple. However, a few things happen under the hood which is good to understand. I am glad they add these kinds of internal details in the videos.\n",
    "\n",
    "## How does batching work.\n",
    "\n",
    " - In general sentences we pass to the model don't have the same length\n",
    " - Each sentence is converted to a vector (1d tensor)\n",
    " - Multiple sentences are added as rows to a 2D tensor (matrix) which is sent in as a single batched input\n",
    " - Since each sentence has a potentially different size, the smaller ones are all padded to the length of the longest one.\n",
    " - Padding is via a known token called `tokenizer.pad_token_id` which the model knows.\n",
    "\n",
    "\n",
    "### converting each sentence to it's own 1d tensor\n",
    "\n",
    "![](./img/v18-sentence-to-1d_tensor.png)\n",
    "\n",
    "Here you see that each tensor has a different length. This is all good if you want to process each separately.\n",
    "\n",
    "### batching the inputs into one tensor\n",
    "\n",
    "Converting this directly to a tensor will fail because it is not rectangular.\n",
    "\n",
    "![](./img/v18-nonrect-ids-to-tensor.png)\n",
    "\n",
    "The way we fix this is to pad the smaller tensors. Truncating the larger one is an idea but a terrible one. Note that the pads are not random values, they should be `tokenizer.pad_token_id`\n",
    "\n",
    "![](./img/v18-pad-smaller-tensor.png)\n",
    "\n",
    "## running the singles and batched inputs through the model\n",
    "\n",
    "This shows the outputs gotten by executing the model on the single as well as the batched inputs.\n",
    "\n",
    "![](./img/v18-executing-single-batched-inputs.png)\n",
    "\n",
    "Note that the batched inputs (_second sentence of the batch_) do differ from the single ones. This is because even though we did pad the inputs, there is an attention layer that is paying attention to the pads as well. _Note that this is incrmentally examining what the internals do. Not that a developer using the external API will ever run into this as it is all taken care of by the HF library_.\n",
    "\n",
    "![](./img/v18-attention-on-pads.png)\n",
    "\n",
    "To fix this, we need to mask the attention layer to they ignore the padded values.\n",
    "\n",
    "![](./img/v18-mask-attention-on-padding.png)\n",
    "\n",
    "Putting it all together, we get the following low level code which works as expected.\n",
    "\n",
    "![](./img/v18-padded-and-attention-masked-batch.png)\n",
    "\n",
    "\n",
    "## Normal higher level batched input usage\n",
    "\n",
    "![](./img/v18-high-level-batched-input-API.png)\n",
    "\n",
    "# Video 20, 21 - Huggingface datasets overview\n",
    "\n",
    "Don't think I'll need this right now but good to know in case I want to validate some model behavior.\n",
    "\n",
    " - [HF Datasets Reference](https://huggingface.co/docs/datasets/en/index)\n",
    " - [Apache Arrow](https://arrow.apache.org/) to facilitate in-memory datasets (_that are disk mapped so very little is actually loaded in memory_)\n",
    " - Text, Audio, Vision, Tabular etc datasets\n",
    " - ðŸ‘‰ can be streamed in as opposed to completely downloaded in one-shot.\n",
    " - ðŸ‘‰ can directly give pyTorch tensors via `Dataset.with_format(\"torch\")`. _`Dataset` is a wrapper over an Apache arrow table_\n",
    " - Simple API to fetch many publicly available datasets\n",
    " - Where on the web do they catalog this ?\n",
    " - new python package `datasets` ?\n",
    "\n",
    " ```python\n",
    " from datasets import load_dataset\n",
    "\n",
    " raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    " raw_datasets\n",
    "\n",
    " >> DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 3668    \n",
    "    }),\n",
    "    validation: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows : 408\n",
    "    }),\n",
    "    test : Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows : 1725\n",
    "    })\n",
    " })\n",
    " ```\n",
    "\n",
    " for instance the **glue** dataset contains pairs of sentences.\n",
    "  - think `features` == `columns`\n",
    "\n",
    "ðŸ‘‰ The files in the dataset are saved to disk using [Apache Arrow](https://arrow.apache.org/). This means, we can load just a slice of the dataset into memory as needed without loading the whole thing into RAM.\n",
    " - To access a single row, use `raw_datasets[\"train\"][6]`\n",
    " - To access a slice of the above day, `raw_datasets[\"train\"][:5]` will load rows [0, 1, 2, 3, 4]\n",
    "\n",
    "## Exploring the dataset structures\n",
    "\n",
    "A simple `print(raw_datasets)` dumps out the outline we see above\n",
    "\n",
    "The `raw_datasets.features` attribute shpws more information abuot the features. _This is one of the confusing aspects of python. There is a `__repr()`, `__print()` likely which allows a structure to print a custom version of itself. So it is not always clear that something is a dictionary or not_.\n",
    "\n",
    "```python\n",
    "raw_datasets[\"train\"].features\n",
    "\n",
    "> { 'sentence1': Value(dtype='string', id=None),\n",
    "    'sentence2': Value(dtype='string', id=None),\n",
    "    'label': ClassLabel(num_classes=2, name=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
    "    'idx': Value(dtype='int32', id=None)}\n",
    "```\n",
    "\n",
    "From the above, we can read that label-index:0 â†’ `not_equivalent` and label:1 â†’ `equivalent`\n",
    "\n",
    "## Example tokenization of this dataset\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # max_length ? Needs max_length=xxx and then pads till that ? Why not max_length=xxx and padding=True ?\n",
    "    # If truncating to max_length, will there be a warning or accumulation of such warnings ?\n",
    "    #\n",
    "    # Normally, we tokensize a single string. This sending in two strings needs some investigation. Since it is \n",
    "    # being applied directly on the tokensizer, this is the __call__ method. \n",
    "    # Look at https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer\n",
    "    #\n",
    "    # arg1: text (str, List[str], List[List[str]], optional) â€” The sequence or batch of sequences to be encoded. \n",
    "    # Each sequence can be a string or a list of strings (pretokenized string). If the sequences are \n",
    "    # provided as list of strings (pretokenized), you must set is_split_into_words=True \n",
    "    # (to lift the ambiguity with a batch of sequences).\n",
    "    #\n",
    "    # arg2: text_pair (str, List[str], List[List[str]], optional) â€” The sequence or batch of sequences to be encoded. \n",
    "    # Each sequence can be a string or a list of strings (pretokenized string). If the sequences are provided\n",
    "    #  as list of strings (pretokenized), you must set is_split_into_words=True (to lift the ambiguity\n",
    "    #  with a batch of sequences).\n",
    "    #\n",
    "    # Still don't understand what special things are done when the inputs are text_pairs. How is the pairing \n",
    "    # relevant to tokenizing ? Since labels are provided, during training we can mark the sentence-pair as similar\n",
    "    # which means keep the vector embedding close otherwise, keep vector embedding far apart ? Hence send tokens\n",
    "    # for the pair in one shot ?\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128    \n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function)\n",
    "print(tokenized_datasets.column_names)\n",
    "\n",
    "# This map is not a generic map function. It adds new 'keys' to existing keys in raw_datasets.\n",
    "# ALso seems to apply to each table (underlying Apache Arrow tables)\n",
    "# tokenize adds the following columns:\n",
    "#  'attention_mask', 'idx', 'token_type_ids'\n",
    "```\n",
    "\n",
    "## Example batched tokenization of this dataset.\n",
    "\n",
    "> Batching is an operational thing to optimize the usage of GPU. Fit as much into one batch as the free memory of the GPU allows to speed up training. During inference, same logic applies. Additionally, you can batching multiple inference requests into one batch as well to tradeoff a slightly increased latency with much higher throughput. An impactful topic.\n",
    "\n",
    "We can send multiple inputs to the tokenizer. If you look at the arguments to the `tokenizer.__call__` method, the first arg can be \n",
    " - `str`\n",
    " - `List[str]`\n",
    " - `List[List[str]]`\n",
    " - can be strings or pre-tokenized strings (_if pre-tokenized set `is_split_into_words=True`_)\n",
    "\n",
    "So basically send in a list of strings. This is accomplished via the `dataset.map`'s `batched=True` argument. The actual `tokensize_function` does not have to change since it accepts `str`,`List[str]` or `List[List[str]]`.\n",
    "\n",
    "![](./img/v20-batched-dataset-mapping.png)\n",
    "\n",
    "## Example preparation for training\n",
    "\n",
    " - Remove columns not needed for training. _Remove `idx`, `sentence1`, `sentence2`_\n",
    " - rename columns as needed. _HF models expect the column `labels` so rename `label` -> `labels`\n",
    " - convert to ML runtime's format (_pyTorch for instance_)\n",
    "\n",
    " ![](./img/v20-prepare-tokenized-dataset-for-training.png)\n",
    "\n",
    "If needed, a smaller sample of the dataset can be sliced out using the `select` method: \n",
    "\n",
    "```python\n",
    "    small_teain_dataset = tokenized_datasets[\"train\"].select(range(100))\n",
    "```\n",
    "\n",
    "# Video 22,23 - Pre Processing sentence pairs\n",
    "\n",
    "Problems like\n",
    " - **Identifying duplicate questions** (_Supervized. Labels provided with the pairs_)\n",
    "    - `What are the best resources for learning Morse code` **not duplicate** of `What is Morse code?`\n",
    "    - `How does an IQ test work and what is determined from an IQ test?` **duplicate** of `How does IQ test works?`\n",
    "    - etc\n",
    "    - To answer: `Are these two questions duplicates?`\n",
    " - **Does the first sentence imply the other or not**\n",
    "   - This uses labels\n",
    "     - *Contradiction* - False. Opposite of imply\n",
    "     - *neutral* - Neither true nor false\n",
    "     - *entailment* - Yes, it implies\n",
    "   - `Fun for only children` **contradicts** `Fun for adults and children`\n",
    "   - `Well you're a mechanics student right?` **neutral** toward `Yeah well you're a student right?`\n",
    "   - `The other men were shuffeld around` **entailement** of `The other men shuffled` _sounds non sensical. \"were shuffled around\"  is not \"shuffled around\"_ Anyway, I get the point of how the labeling works for training.\n",
    "\n",
    "## sentence-pair datasets\n",
    "\n",
    "The [GLUE](https://gluebenchmark.com/) benchmark is an academic bench mark for text classification. It currently provides 10 datasets out of which 8 are sentence pairs\n",
    " - MRPC _Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent_\n",
    " - STS-B - _The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5._\n",
    " - QQP _Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent._\n",
    " - MNLI _Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations_\n",
    " - QNLI _Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator)._ \n",
    " - RTE _Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. The authors of the benchmark combined the data from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli et al., 2009). Examples are constructed based on news and Wikipedia text._ \n",
    " - WNLI - _Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices._ **CoRef ?**\n",
    "\n",
    " The single sentence ones are \n",
    "  - SST-2 - _Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. It uses the two-way (positive/negative) class split, with only sentence-level labels._ \n",
    "  - COLA - _Corpus of Linguistic Acceptability_ whether it is a grammatical sentence\n",
    "\n",
    "## BERT training objectives on sentence pairs\n",
    "\n",
    "[Bert was trained to achieve dual objectives](https://www.linkedin.com/pulse/what-bert-how-trained-high-level-overview-suraj-yadav/) _Sylvain is many times hard to understand and it is hard to provide context for so much in such little time anyway. I looked around to get more info on BERT objectives_. BERT optimizes both of the following objectives simultaneously.\n",
    "  - **Masked language modeling** (MSM) Predict masked word from other words using a bi-directional context.\n",
    "  - **Next sentence prediction** (NSP) Predict whether two sentences occur consecutively or not. The input is the concatenation of the two sentences with a `[SEP]` token, additionally a special classification token (_token to indicate classification task on the entire input_):`[CLS]` is inserted at the beginning.\n",
    "    - During training a binary classification layer is added on top of the BERT model to perform this binary classification. It takes the hidden state rep of the `[CLS]` token ?? and outputs a probability score.\n",
    "\n",
    " ![NSP Example](./img/v22-nsp-example.png)\n",
    "\n",
    " The transformer library has a nice API to deal with pairs of sentences (_single pair or batched_)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Arg1 and Arg2 can be both\n",
    "# str for singles\n",
    "# List[str] or List[List[str]] for batched inputs.\n",
    "tokenizer(\"My name is Sylvain.\", \"I work at Hugging Face.\")\n",
    "```\n",
    "\n",
    "This outputs a single token stream (the pair is encoded as one input)\n",
    " - token_type_ids: show shich one for the first sentence and which for the second. `0` indicates sentence 1 and `1` the second that is supposed to be classified as occuring next or not.\n",
    " - mask includes it all.\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': [...],\n",
    "    'token_type_ids' : [0, 0, 0,..., 1,1,1],\n",
    "    'attention_mask' : [..]\n",
    "}\n",
    "```\n",
    "\n",
    "## sentence pair through the BERT model\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "batch = tokenizer(\n",
    "    [\"My name is Sylvain.\", \"Going to the cinema\"],\n",
    "    [\"I work at Hugging face.\", \"This movie is great!\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**batch)\n",
    "```\n",
    "\n",
    "# Video 24 - Dynamic padding\n",
    "\n",
    " - We need to pad sentences of different lengths so we can send em all in as a batch.\n",
    " - When we have a huge dataset\n",
    "   - can pad em all to the longest string in the dataset (_global but likely very wasteful_)\n",
    "   - pad then when we compose the batch (_obvious. I was expecting a more aha moment_). This is called *dynamic batching*. Downside is that batch shapes are different and this can slow things down on an accelerator ??\n",
    "\n",
    "\n",
    "Some details on how to use the dataCollator, dynamic batching on CPUs fixed batchig on GPUs ?\n",
    "**re-watch with more detailed notes if needed**\n",
    "\n",
    "# Video 25 - The trainer API\n",
    "\n",
    "Skipping for now.\n",
    "\n",
    "# Videos 26..29 - TensorFlow specific \n",
    "Skipping for now.\n",
    "\n",
    "# Videos 30 - Write your training loop in PyTorch\n",
    "Skipping for now.\n",
    "\n",
    "# Videos 31 - Supercharge PyTorch training with Accelerate\n",
    "Skipping for now.\n",
    "\n",
    "# Video 32, 33, 34, 35 - ModelHub push\n",
    "Skipping for now.\n",
    "\n",
    "# Video 36,37,38,39,40,41 - Custom Datasets\n",
    "Skipping for now.\n",
    "\n",
    "# Video 42 - Text embeddings and semantic search\n",
    "\n",
    "! already have multiple docs on embedding and their use in semantic search. Will be very useful to find out how the HF folk expose that functionality.\n",
    "\n",
    "Start with just the basic high level concepts\n",
    " - represent text as an array of numbers of a vector\n",
    " - usually use an encoder transformer model _Recall that an eccoder because of it's bi-directional context is especially good at capturing language meaning and syntax. As long as the loss-function is defined as low vector-distance for similar and high for dissimilar, wouldn't any DL model work ok though ?_\n",
    "\n",
    "![Text embeddings of some statements](./img/v42-embeddings-of-some-statements.png)\n",
    " - Simply reading the vector components shows that the `I took my dog for a walk` is very similar to `I took my cat for a walk`\n",
    "\n",
    "## Vector similarity\n",
    "\n",
    "When figuring how different two vectors are, one measure to use is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    " - distance close to 180 degrees can be a measure of opposite-ness\n",
    " - details related to normalizing the vectors (_as shown below_) affecting the scores. How does the magnitude of the vector come into play ?\n",
    " - [PyTorch - Cosine Similarity](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html) - _cosine similarity along dim which defaults to 1. PyTorch and others deal with tensors which are n-dimensional constructs. Dimension here corresponds to which dimension of the tensor to be used as the vector. Right ?._\n",
    " - [PyTorch - Cosine loss](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html) shows how to use cosine similarity to compute the loss.\n",
    "\n",
    "\n",
    "![cosine similarity](./img/v42-cosine-similarity.png)\n",
    "\n",
    "## Hairyness\n",
    "\n",
    " - BERT produces one vector per token! A 384 dim vector!\n",
    " - Some math is performed to get a vector per sentence (_Do I need to check the math behind this ?_)\n",
    " - Exposes to additional tools to actually plot distances\n",
    "\n",
    "## Detailed study\n",
    "\n",
    "This is an important topic for me. Split off into it's own notebook at [./NLP_HuggingFace_Embeddings.ipynb](./NLP_HuggingFace_Embeddings.ipynb)\n",
    "\n",
    "# Video 43 - Training a new tokenizer\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=DJimQynXZsQ&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o&index=43\n",
    "\n",
    "**When is an existing tokensizer not applicable**\n",
    "\n",
    " - Corpus is in a different language\n",
    "   - Sometimes, too many tokens are generated\n",
    "   - Lots of `[UNK]` tokens\n",
    "   - Too long tokens might clash against model limits\n",
    "   - Learned NSP or MLM objectives are inapplicable.\n",
    " - Uses new characters\n",
    "   - Lots of `[UNK]` tokens\n",
    " - Uses a specific vocab (different domain: medical, law etc)\n",
    " - Different style (_from a century back, say_)\n",
    "\n",
    "## To train a new tokenizer\n",
    "\n",
    " - Gather data: the corpus of text\n",
    " - Choose a tokensizer architecture or a new arch (_if you have the expertise_)\n",
    " - Train\n",
    " - Save\n",
    "\n",
    "\n",
    "All `FastTokenizers` from HF have this method `AutoTokenizer.train_new_from_iterator(text_iterator, vocab_size, new_special_tokens=None, special_tokens_map=None,**kwargs)` to train a tokenizer using a known architecture on a new corpus.\n",
    "\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Python corpus from online public sources and github\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Is this an iterator yielding 1000 at a time ?\n",
    "def get_training_corpus_iterator():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx: start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "\n",
    "training_corpus_iter = get_training_corpus_iterator()\n",
    "\n",
    "# Start with a GPT2 architecture\n",
    "old_tokesizer = AutoTokensizers.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Use a vocabulary of 52000 (Why?) for the new Tokenizer\n",
    "# Wonder how long this takes on my PC with a 2080Ti. Will it even ?\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(trainintg_corpus_iter, 52000)\n",
    "```\n",
    "\n",
    "# Video 44 - Why are fast tokenizers called fast\n",
    "\n",
    "> Fast ones are backed by rust. Capable of parallel\n",
    "\n",
    "Sylvain uses the `glue` dataset's `mnli` dataset for testing. This has \n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset\n",
    "# This has multiple datasets: \n",
    "#    train, \n",
    "#    validation_mathced, validation_mismatched, \n",
    "#    test_matched and test_mismatched\n",
    "# Each with features: `premise`, `hypothesis`, `label` and `idx`\n",
    "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "# There is a `use_fast=True` arg here making it fast! Has to be available \n",
    "# right ?\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "# And use this method to run the tokenizer\n",
    "def tokensize(tokenizer, examples):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\", truncation=True]\n",
    "    )\n",
    "\n",
    "# On jupyter use %time to time the calls\n",
    "# Using partially applied fucntions here instead of the examples\n",
    "from functools import partial\n",
    "\n",
    "# Tokensize with fast vs fast\n",
    "# He says the fast one is 4 times faster\n",
    "fast_tokenized_datasets = raw_datasets.map(\n",
    "    partial(tokenize, fast_tokenenizer)\n",
    "    )\n",
    "\n",
    "slow_tokenized_datasets = raw_datasets.map(\n",
    "    partial(tokenize, slow_tokenenizer)\n",
    "    )    \n",
    "\n",
    "# However, since it is paralell capable\n",
    "# using batch makes exploits its speed best\n",
    "# This shows a 20x speedup!\n",
    "fast_tokenized_datasets = raw_datasets.map(\n",
    "    partial(tokenize, fast_tokenenizer),\n",
    "    batched=True\n",
    "    )\n",
    "\n",
    "slow_tokenized_datasets = raw_datasets.map(\n",
    "    partial(tokenize, slow_tokenenizer),\n",
    "    batched=True\n",
    "    )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "\n",
    "# Gah! Dashed.\n",
    "# Some wierd download problems with this. \n",
    "# Spent a lot of time debugging and found some issues. Must be common\n",
    "# Turned out I needed to move to latest 2.17.0 datasets from the old one I had.\n",
    "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# And use this method to run the tokenizer\n",
    "def v44_tokenize(tokenizer, examples):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\"], truncation=True\n",
    "    )\n",
    "\n",
    "v44_tokenize_with_fast = partial(v44_tokenize, fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# However, since it is paralell capable\n",
    "# using batch makes exploits its speed best\n",
    "# This shows a 20x speedup!\n",
    "fast_tokenized_datasets = raw_datasets.map(v44_tokenize_with_fast, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run the above on my Ryzen 16core CPU + 2080Ti, it works great. 11.9 s vs his 12.1s (_his numbers are from 2 years ago but likely mostly GPU dependent. My CPU and GPU are also old_). Awesome that the 2080Ti is upto these kinds of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 45 - Fast tokenizer superpowers\n",
    "\n",
    " - Fast (_Rust based_)\n",
    " - Loaded via `AutoTokensize.from_pretrained( checkpointName, fast=True)`. _fast=True is the default so you don't explicitly see this_\n",
    " - And have new features\n",
    "\n",
    "\n",
    "## New features of fast tokenizers\n",
    " - Skips empty spaces in tokenization\n",
    " - Standardizes things like start of word symbols and words split across multiple tokens. Not easy to figure out which word a token belongs to as each has a different way of doing it.\n",
    "   - RoBERTa uses `G`\n",
    "   - T5 uses `_`\n",
    "   - BERT uses nothing for pre but uses `##` for trailing parts.\n",
    "\n",
    "## word-ids\n",
    "\n",
    "Fast tokensizers, along with standard method of `encodings.tokens()` also have a `encodings.word_ids()` which keep track of which word each token belongs to. This prints out a word-id for each token. When multiple tokens have the same word-id, you know they are for the splits of the same word.\n",
    "\n",
    "## offset_mapping\n",
    "\n",
    "There is a new `return_offsets_mapping=True` argument when tokenizing which adds a new data-field to the output: `encodings[\"offset_mapping\"]` that returns the span of characters each token comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Deliberately introduce spaces\n",
    "encoding  = tokenizer(\n",
    "    \"Let's talk about tokenizers     superpowers.\",\n",
    "    return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_str = \"Let's talk about tokenizers     superpowers.\"\n",
    "\n",
    "# Explore the output\n",
    "# Notice no-tokens for empty spaces\n",
    "# ['[CLS]', 'Let', \"'\", 's', 'talk', 'about', 'token', '##izer', '##s', 'super', '##power', '##s', '.', '[SEP]']\n",
    "print(encoding.tokens())\n",
    "\n",
    "# Shows that word-id:5,6 are split into 3 tokens each.\n",
    "# [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, 7, None]\n",
    "print(encoding.word_ids())\n",
    "\n",
    "# Show for instance: that token:Let has offset (0,3). (char_idx=start, char_idx < end; ++char_idx)\n",
    "# [(0, 0), (0, 3), (3, 4), (4, 5), (6, 10), (11, 16), (17, 22), (22, 26), (26, 27), (32, 37), (37, 42), (42, 43), (43, 44), (0, 0)]\n",
    "print(encoding[\"offset_mapping\"])\n",
    "\n",
    "# Print out sub-strings belonging to tokens this way.\n",
    "print(inp_str[32:42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These new features of the tokensizer are super useful for special tasks where you need to map the tokens back to their spans etc.\n",
    "\n",
    "# Video 46,47 - Inside the token classification pipeline\n",
    "\n",
    "Resources\n",
    "  - NLP Course: https://huggingface.co/learn/nlp-course/chapter7/2\n",
    "  - Video: https://www.youtube.com/watch?v=0E7ltQB7fM8&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o&index=46\n",
    "\n",
    "`token-classification` is one of the supported HF pipelines. This is a generic problem which covers assigning a label/classification to token. This can be done along many lines\n",
    " - NER _Find entities: names of people, locations, organizations etc_\n",
    " - POS _Parts of speech. Mark each word in a sentences are corresponding to a particular part of speech (noun, verb, adjective etc)_\n",
    " - Chunking _Find tokens that belong to the same entity. This task (combinable with POS or NER) can be formulated as one label, usually `B-` to any token at the beginning of a chunk and another label, usually `I-` to tokens inside a token and a third label, usually `0`, to tokens that do not belong to any chunk._\n",
    " - Several others..\n",
    "\n",
    "When you use `pipeline(\"token-classification\")`, it performs a NER classification\n",
    " - Tokens are classified into one N (`Person`, `Organization`, `Location`) classes with one class for unclassified tokens `Misc`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# This is a straightforward method which many times comes up with many small tokens \n",
    "# for each word.\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging face in Brooklyn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can form grouping (by recogning B-, I- tokens, start/end of tokens etc) to combine tokens related to one\n",
    "# entity in one place. Insteaf of I-PER, I-LOC, we ge a PER, LOC etc.\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging face in Brooklyn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline In Details\n",
    "\n",
    "This has the usual steps of `Tokenization â†’ Model â†’ Post Processing`. In the following code you'll see that\n",
    " - `My name is Sylvain and I work at Hugging Face in Brooklyn.` gets broken down into 19 tokens\n",
    " - These 19 tokens have 9 outputs each which correspond to the probabilities associated with each of the possible 9 token classifier labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 - Tokenization\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "\n",
    "# Phase 2 - Model execution to generate logits\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 - Post processing\n",
    "import torch\n",
    "\n",
    "# classification label probabilities\n",
    "# [NumBatches x NumTokens x NumLabels]\n",
    "with torch.no_grad():\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "\n",
    "#print(probabilities)\n",
    "\n",
    "# argmax returns the indices of the tensor with max values\n",
    "# along the supplied dimension (-1 or 2 in this case for the last dim)\n",
    "predictions   = probabilities.argmax(dim=-1).tolist()\n",
    "#print(predictions)\n",
    "\n",
    "# The label -> name mapping is stored here\n",
    "#print(model.config.id2label)\n",
    "\n",
    "# Print out more information per-token\n",
    "# - classification\n",
    "# - text span (Note that this uses the fast-tokenizer's offsets)\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "\n",
    "    # unclassified tokens have the \"O\" label (letter O not number 0)\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred],\n",
    "             \"word\": tokens[idx], \"start\": start, \"end\":end}\n",
    "        )\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all tokens that belong to a word together into one-word\n",
    "# Implementation of pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "import numpy as np\n",
    "\n",
    "probabilities_list = probabilities.tolist()\n",
    "label_map = model.config.id2label\n",
    "results = []\n",
    "idx = 0\n",
    "\n",
    "\n",
    "# Note: Either (B-xxx)|(I-xxx)*\n",
    "#         -or- (I-xxx)* (I-yyy)  # Where yyy != xxx. i.e, till a label transition occurs\n",
    "# The label-transition pattern is more universal since not all classifiers use B-/I- patterns\n",
    "while idx < len(predictions):\n",
    "    pred   = predictions[idx]\n",
    "    label  = label_map[pred]\n",
    "    scores = []\n",
    "\n",
    "    # unclassified tokens have the \"O\" label (letter O not number 0)\n",
    "    if label != \"O\":        \n",
    "        # Remove the B- or I- tag\n",
    "        label    = label[2:]\n",
    "        start, _ = offsets[idx]        \n",
    "        \n",
    "        # grab till transition to a different label\n",
    "        while idx < len(predictions) and label_map[predictions[idx]] == f\"I-{label}\":\n",
    "            _, end = offsets[idx]\n",
    "            scores.append(probabilities_list[idx][pred])\n",
    "            idx += 1\n",
    "        \n",
    "        word = example[start:end]\n",
    "        #print(scores)\n",
    "\n",
    "        results.append(\n",
    "            {\"entity_group\": label,\n",
    "             \"score\": sum(scores)/len(scores),\n",
    "             \"word\" : word,\n",
    "             \"start\": start,\n",
    "             \"end\": end}\n",
    "        )\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Parts of Speech tagging (POS)\n",
    "\n",
    "While the videos do not show how POS tagging is done, I found some POS examples by searching for `POS` under [`token classification tasks](https://huggingface.co/models?pipeline_tag=token-classification) and found a model with usage doc. Also shows a different way to assemble a Pipeline from tokenizer and model. _There are several such POS models so good_\n",
    " - [QCRI/bert-base-multilingual-cased-pos-english](https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english) is recent:  Jan 2023\n",
    "\n",
    "```json\n",
    "[\n",
    "    {'entity': 'PRP$', 'score': 0.9995228, 'index': 1, 'word': 'My', 'start': 0, 'end': 2}, \n",
    "     {'entity': 'NN', 'score': 0.9995235, 'index': 2, 'word': 'name', 'start': 3, 'end': 7}, \n",
    "     {'entity': 'VBZ', 'score': 0.9995004, 'index': 3, 'word': 'is', 'start': 8, 'end': 10}, \n",
    "     {'entity': 'NNP', 'score': 0.9951285, 'index': 4, 'word': 'Jorge', 'start': 11, 'end': 16}, \n",
    "     {'entity': 'CC', 'score': 0.9996544, 'index': 5, 'word': 'and', 'start': 17, 'end': 20}, \n",
    "     {'entity': 'PRP', 'score': 0.9996605, 'index': 6, 'word': 'I', 'start': 21, 'end': 22}, \n",
    "     {'entity': 'VBP', 'score': 0.99771214, 'index': 7, 'word': 'live', 'start': 23, 'end': 27}, \n",
    "     {'entity': 'IN', 'score': 0.99966407, 'index': 8, 'word': 'in', 'start': 28, 'end': 30},\n",
    "     {'entity': 'NNP', 'score': 0.9996543, 'index': 9, 'word': 'Mountain', 'start': 31, 'end': 39}, \n",
    "     {'entity': 'NNP', 'score': 0.9995591, 'index': 10, 'word': 'View', 'start': 40, 'end': 44}, \n",
    "     {'entity': ',', 'score': 0.9998921, 'index': 11, 'word': ',', 'start': 44, 'end': 45}, \n",
    "     {'entity': 'NNP', 'score': 0.99960035, 'index': 12, 'word': 'California', 'start': 46, 'end': 56}, \n",
    "     {'entity': '.', 'score': 0.9999255, 'index': 13, 'word': '.', 'start': 56, 'end': 57}\n",
    "     ]\n",
    "```     \n",
    "\n",
    "Joining `Mountain` and `View` together because they are both NNP ? Need to come up with rules for POS just like for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "\n",
    "# Large model. pytorch_weights.bin is 712M\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "outputs = pipeline(\"My name is Jorge and I live in Mountain View, California.\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'T5Coref' from 'transformers' (/home/vamsi/mambaforge/envs/ml/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/tliu/asp-coref-flan-t5-large/tree/main\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Coref, AutoTokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# It's config.json says it needs Transformers 4.33.3\u001b[39;00m\n\u001b[1;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtliu/asp-coref-flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'T5Coref' from 'transformers' (/home/vamsi/mambaforge/envs/ml/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# https://huggingface.co/tliu/asp-coref-flan-t5-large/tree/main\n",
    "# While it says the following should work, even after upgrading to latest transformers, nothing works.\n",
    "#  Does it need TF to be installed somehow\n",
    "#from transformers import T5Coref, AutoTokenizer\n",
    "##\n",
    "## It's config.json says it needs Transformers 4.33.3\n",
    "##checkpoint = \"tliu/asp-coref-flan-t5-large\"\n",
    "#\n",
    "#tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "##model = T5Coref.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 48,49 - Inside the question answering pipeline\n",
    "\n",
    "This pipeline can extract answers from a given context: _some text that contains the answer_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Transformers is backed by the three most popular deep learning libraries - Jax, PyTorch and TensorFlow - wth a seamless\n",
    " integration between them. It's straightforward to train your models with one before loading them for \n",
    " inference with the other\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which deep learning libraries back Transformers?\"\n",
    "\n",
    "# This responds with 'Jax, PyTorch and TensorFlow'\n",
    "question_answer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet shows a small context, however this works for very long contexts as well. We'll show how later in this section.\n",
    "\n",
    "The usual flow is used here: `tokenizer -> Model -> Postprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Video 50 - What is normalization\n",
    "\n",
    "# Video 51 - What is pre-tokenization\n",
    "\n",
    "# Video 52 - Byte-pair encoding tokenization\n",
    "\n",
    "# Video 53 - WordPiece tokenization\n",
    "\n",
    "# Video 54 - Unigram tokenization\n",
    "\n",
    "# Video 55 - Building a new tokenizer\n",
    "\n",
    "# Video 56 - Data processing for token classification\n",
    "\n",
    "# Video 57 - Data processing for masked language modeling\n",
    "\n",
    "# Video 58 - What is perplexity\n",
    "\n",
    "# Video 59 - What is domain adaptation\n",
    "\n",
    "# Video 60 - Data processing for translation\n",
    "\n",
    "# Video 61 - What is the BLEU metric\n",
    "\n",
    "# Video 62 - Data processing for summarization\n",
    "\n",
    "# Video 63 - What is the ROUGE metric\n",
    "\n",
    "# Video 64 - Data processing for causal Language modeling\n",
    "\n",
    "# Video 65 - Using a custom loss function\n",
    "\n",
    "# Video 66 - Data processing for Question answering\n",
    "\n",
    "# Video 67,68 - Post processing in Question Answering (pyTorch)\n",
    "\n",
    "# Video 69 - Data collators - A tour\n",
    "\n",
    "# Video 70 - What to do when you get an error\n",
    "\n",
    "# Video 71 - Using a debugger in a notebook\n",
    "\n",
    "# Video 72 - Using a debugger in a terminal\n",
    "\n",
    "# Video 73 - Asking for help on the forums\n",
    "\n",
    "# Video 74,75 - Debugging the training pipeline (PyTorch)\n",
    "\n",
    "# Video 76 - Writting a good issue/bug-report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ - HFArgumentParser\n",
    "\n",
    "See \n",
    " - [Automatically Generate Python CLI](https://python.plainenglish.io/how-to-automatically-generate-command-line-interface-for-python-programs-e9fd9b6a99ca)\n",
    " - [Python Data classes](https://realpython.com/python-data-classes/)\n",
    "\n",
    "> The HFArgumentParser takes a bunch of dataclass objects _(new in Python 3.7: supercharged dicts and tuples)_ which contain fields. This then dynamically generates CLI args for an underlying ArgumentParser and packs the parsed args back into the data-classes.\n",
    "\n",
    "Data classes look like plain classes with a lot of the boiler plate removed. Looks a lot like Rust and Scala case classes.\n",
    "  - lots of boiler plate removed\n",
    "  - automatic _repr_ to give good output\n",
    "\n",
    "In the context of the TANL code-base, here is one data-classes\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments for the Trainer.\n",
    "    \"\"\"\n",
    "    output_dir: str = field(\n",
    "        default='experiments',\n",
    "        metadata={\"help\": \"The output directory where the results and model weights will be written.\"}\n",
    "    )\n",
    "    \n",
    "    zero_shot: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Zero-shot setting\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "and this is how the args are parsed\n",
    "\n",
    "```python\n",
    "second_parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "second_parser.set_defaults(**defaults)\n",
    "model_args, data_args, training_args = second_parser.parse_args_into_dataclasses(remaining_args)\n",
    "```\n",
    "\n",
    " - It uses additional data-classes (`ModelArguments` and `DataTrainingArguments`)\n",
    " - One of the returned item is `training_args`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
