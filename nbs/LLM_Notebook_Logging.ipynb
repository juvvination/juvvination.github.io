{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging in Jupyter notebooks\n",
    "\n",
    "When evolving your notebook code, as it becomes more complex, your logging attempts will be forced to keepup: usually taking this path.\n",
    "\n",
    " - add `print` statements in your cells\n",
    " - tire of constant commenting/uncommenting the `prints`\n",
    " - research logging and learn that you can use python's loggers in notebooks! Yay!\n",
    " - research further and figure out that you can direct builting loggers for your imports as well.\n",
    " - Look at this verbose output 😖!! Discover the power of visual separation with colors! \n",
    "\n",
    "This notebook demonstrates the above evolution. \n",
    "\n",
    "## Basic Logging\n",
    "\n",
    "The cell below shows how you'd setup basic logging. The `reload(logging)` is needed because of the singleton aspect of the config. In a jupyter environment _(and you might face this in your standalone scripts as well)_, turns out the something in the infrastructure already initializes the logger. Once that is done, subsequent calls to `logging.basicConfig` do not have any effect. This is where `reload` comes in as it clears the module state allowing the next `logging.basicConfig` to take effect.\n",
    "\n",
    "> This is also the reason why you will need to restart the notebook kernel if you want to change your `logging.basicConfig` args. Switch from `logging.DEBUG` to `logging.INFO` say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging \n",
    "# Note that module needs to be reloaded for our config to take as Jupyter already configures it\n",
    "# which makes all future configs no-ops.\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.DEBUG, \n",
    "                    datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is how you would introduce loggging into your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:54:00 DEBUG:My debug statement\n",
      "01:54:00 WARNING:My warning\n",
      "01:54:00 ERROR:My error\n"
     ]
    }
   ],
   "source": [
    "def my_func():\n",
    "    # do stuff and then\n",
    "    logging.debug(\"My debug statement\")\n",
    "    logging.warning(\"My warning\")\n",
    "    logging.error(\"My error\")\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆 should show the three log statements you just printed out from the cell above.\n",
    "\n",
    "You can setup a variable in a top-most cell `MY_LOG = logging.WARNING` and use that in `basicConfig(.. level=MY_LOG)` or directly change the `logging.basicConfig` statement to set it for a specific notebook run. Play with adjusting the `level` argument and notice how the output gets filtered.\n",
    "\n",
    "## Configure for colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:54:03 DEBUG:Checking if OPENAI_API_KEY is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# You could do one of the two.\n",
    "# Either paste your OpenAI Key here or put it in secrets\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import userdata\n",
    "  logging.debug(\"Tryign to fetch OPENAI_API_KEY from your secrets. Remember to make it available to this notebook\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "logging.debug(\"Checking if OPENAI_API_KEY is available\")\n",
    "assert(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Control logging from imports - OpenAI - Env vars\n",
    "\n",
    "With LLMs, for instance, there are many times when you want visibility into low level decision making. Particularly those that might cause latency spikes, like HTTP response codes, hitting rate-limits and automatic retries. Many of these log statements will also turn out to be discovery and make you research the API in more detail as you suddenly notice something that might be relevant to control or atleast surface.\n",
    "\n",
    "This section will demonstrate controlling `OpenAI` logging when using their LLM APIs. Their documentation shows that they use `OPENAI_LOG` environment variale to control their loggers. Other LLM vendors will work similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "import os\n",
    "os.environ[\"OPENAI_LOG\"]=\"debug\"\n",
    "\n",
    "import openai\n",
    "\n",
    "# Expects a OPENAI_API_KEY env var\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0) -> str:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:43:34 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "07:43:34 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "07:43:34 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "07:43:34 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f0620a6c050>\n",
      "07:43:34 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f0620a11d90> server_hostname='api.openai.com' timeout=5.0\n",
      "07:43:34 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f0620b81590>\n",
      "07:43:34 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "07:43:34 DEBUG:send_request_headers.complete\n",
      "07:43:34 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "07:43:34 DEBUG:send_request_body.complete\n",
      "07:43:34 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "07:43:37 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 15:43:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'1987'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199977'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_42f8010912ce98f066f32aa995474eb9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qFi58zuIMl1LyLr_cE8JzRyB78eZA9gd0_WyBeSEfPc-1741189417-1.0.1.1-gMtH8qk4C535dHnvUFXSGpdiC4yYtjFBT0F_14yrVBgRBmrjutAOZH8SxvvlG7xVk8sT7f7jDAOohCU0CkwyAbZoPZD23uO_ndkTO36Tmm8; path=/; expires=Wed, 05-Mar-25 16:13:37 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=6NbLuTXkEmFd9Xno2K2rplDs4WAR02OF_It5nUQXIck-1741189417765-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91babad46f3a2abb-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "07:43:37 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "07:43:37 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "07:43:37 DEBUG:receive_response_body.complete\n",
      "07:43:37 DEBUG:response_closed.started\n",
      "07:43:37 DEBUG:response_closed.complete\n",
      "07:43:37 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 05 Mar 2025 15:43:37 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-uxl7oko9mdo17utucmetfrwn'), ('openai-processing-ms', '1987'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199977'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '6ms'), ('x-request-id', 'req_42f8010912ce98f066f32aa995474eb9'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=qFi58zuIMl1LyLr_cE8JzRyB78eZA9gd0_WyBeSEfPc-1741189417-1.0.1.1-gMtH8qk4C535dHnvUFXSGpdiC4yYtjFBT0F_14yrVBgRBmrjutAOZH8SxvvlG7xVk8sT7f7jDAOohCU0CkwyAbZoPZD23uO_ndkTO36Tmm8; path=/; expires=Wed, 05-Mar-25 16:13:37 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=6NbLuTXkEmFd9Xno2K2rplDs4WAR02OF_It5nUQXIck-1741189417765-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '91babad46f3a2abb-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "07:43:37 DEBUG:request_id: req_42f8010912ce98f066f32aa995474eb9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
      "\n",
      "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
      "\n",
      "During sunrise and sunset, the sky can appear red or orange because the sunlight has to pass through a greater thickness of the atmosphere. This longer path scatters the shorter blue wavelengths out of our line of sight, allowing the longer red wavelengths to dominate the view.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"Why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice two things 👆\n",
    "\n",
    " - Those logs 😍\n",
    " - Such log! much noise! Where's my actual output at 😟\n",
    "\n",
    "We'll get to decluttering the visual a bit later.\n",
    "\n",
    "## Control logging when no env vars are available\n",
    "\n",
    "> This is useful when you want to change things other than log levels as well. See the formatter example below\n",
    "\n",
    "It's nice that OpenAI provides the `OPENAL_LOG` env var: very easy to control that. However, in cases where you don't have access to such a variable, you can manipulate the logger directly: you have to get to the logger in use.\n",
    "\n",
    "### Examine the loggers avaiable.\n",
    "\n",
    "Note that the loggers are usually initialized at the module level on first use. So you'll likely need to exercise some code to get to them. ALl of this just to get the logger by name. Once you know the name, it usually doesn't change unless some major revision occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['httpx', 'rich', 'openai', 'openai._legacy_response', 'openai._response', 'openai._base_client', 'openai.resources.beta.realtime.realtime', 'openai.resources.beta.realtime', 'openai.resources.beta', 'openai.resources', 'openai.audio.transcriptions', 'openai.audio', 'openai.resources.uploads.uploads', 'openai.resources.uploads', 'httpcore.http11', 'httpcore', 'httpcore.connection', 'httpcore.proxy']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " * openai\n",
       " * openai._legacy_response\n",
       " * openai._response\n",
       " * openai._base_client\n",
       " * openai.resources.beta.realtime.realtime\n",
       " * openai.resources.beta.realtime\n",
       " * openai.resources.beta\n",
       " * openai.resources\n",
       " * openai.audio.transcriptions\n",
       " * openai.audio\n",
       " * openai.resources.uploads.uploads\n",
       " * openai.resources.uploads"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Use this to explore available loggers\n",
    "# If there is a logger and you are not provided an env-var top control log level, \n",
    "# you can directly call logger.setLevel(Logging.DEBUG) to collect logs.\n",
    "def get_available_loggers():\n",
    "    return [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "\n",
    "\n",
    "# To see them all.\n",
    "all_loggers = get_available_loggers()\n",
    "print([l.name for l in all_loggers])\n",
    "\n",
    "# Say we are interested only in openai\n",
    "# Long list, I want this formatted nicely. Markdown formatting is easy enough to generate\n",
    "# compared to HTML\n",
    "openai_logger_names = [l.name for l in get_available_loggers() if 'openai' in l.name]\n",
    "display(Markdown(\n",
    "    \"\\n\".join([f\" * {item}\" for item in openai_logger_names])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the log level directly on the selected logger\n",
    "\n",
    "The query above shows a logger called `openai`: likely the root logger with individual sub-modules having child loggers. This is how one normally does things so that while testing a sub-module, you can set it's log-level to `INFO` say while reducing the noise down to `ERROR` for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we want to customize the 'openai` logger. It likely is inherited by the openai.xxx child-loggers\n",
    "# but not sure if they copy parent settings on it or will always reference it. Basically, you may have to \n",
    "# customize the individual ones or see if root-logger customization carries through.\n",
    "oai_logger = list(filter(lambda l: l.name == \"openai\", all_loggers))[0]\n",
    "\n",
    "# Since we already have all_loggers, I am using a filter on it.\n",
    "# However, once you know the name, you can also use\n",
    "# 👉  oai_logger = logging.root.manager.loggerDict.get('openai')\n",
    "#-----------------------\n",
    "# Set the level directly\n",
    "oai_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Distinguishing log output from your cell output \n",
    "\n",
    "The main problem _(as illustrated in a previous call to OpenAI's completion API)_ is that of noise. Simply too much stuff and it takes attention away from the output you really care about. Thankfully, there are several easy solutions. The simplest would be to make use of Jupyter notebook's builtin markdown renderer _(also immensely useful when you have LLM output in markdown or want to convert something to markdown for some easy formatting)_. \n",
    "\n",
    "> Definitely pays to know your markdown. Mich simpler and less verbose than HTML.\n",
    "\n",
    "### Use a markdown separator\n",
    "\n",
    "Simply throw in a markdown separator add/or a markdown section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def markdown_separator(section_name = None):\n",
    "    if section_name:\n",
    "        display(Markdown(f\"----\\n### {section_name}\\n\"))\n",
    "    else:\n",
    "        display(Markdown(f\"----\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:00:53 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "08:00:53 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "08:00:53 DEBUG:close.started\n",
      "08:00:53 DEBUG:close.complete\n",
      "08:00:53 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "08:00:53 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb520f460>\n",
      "08:00:53 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9fb5256e70> server_hostname='api.openai.com' timeout=5.0\n",
      "08:00:53 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb520f130>\n",
      "08:00:53 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "08:00:53 DEBUG:send_request_headers.complete\n",
      "08:00:53 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "08:00:53 DEBUG:send_request_body.complete\n",
      "08:00:53 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "08:00:56 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 16:00:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'2857'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199978'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_2588771bb8c89d359131e92b4420abcf'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bad4343da82b5d-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "08:00:56 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "08:00:56 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "08:00:56 DEBUG:receive_response_body.complete\n",
      "08:00:56 DEBUG:response_closed.started\n",
      "08:00:56 DEBUG:response_closed.complete\n",
      "08:00:56 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 16:00:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '2857', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199978', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_2588771bb8c89d359131e92b4420abcf', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bad4343da82b5d-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "08:00:56 DEBUG:request_id: req_2588771bb8c89d359131e92b4420abcf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "----\n",
       "# OpenAI Response\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
       "\n",
       "During sunrise and sunset, the sun is lower on the horizon, and its light has to pass through a greater thickness of the atmosphere. This increased distance scatters the shorter blue wavelengths out of our line of sight, allowing the longer wavelengths like red and orange to dominate, which is why the sky can appear red or orange during those times."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print separator\n",
    "markdown_separator(\"OpenAI Response\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color the cell output\n",
    "\n",
    "Take advantage of the `IPython.display.Html` object and render any HTML that you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:11:31 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "12:11:31 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "12:11:31 DEBUG:close.started\n",
      "12:11:31 DEBUG:close.complete\n",
      "12:11:31 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "12:11:31 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb4b20710>\n",
      "12:11:31 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9fb5256e70> server_hostname='api.openai.com' timeout=5.0\n",
      "12:11:31 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb4ad1440>\n",
      "12:11:31 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "12:11:31 DEBUG:send_request_headers.complete\n",
      "12:11:31 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "12:11:31 DEBUG:send_request_body.complete\n",
      "12:11:31 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "12:11:32 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 20:11:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'1730'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199977'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_3598ed167d0ac44066274bca84715b22'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bc43570d372eeb-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "12:11:32 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "12:11:32 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "12:11:33 DEBUG:receive_response_body.complete\n",
      "12:11:33 DEBUG:response_closed.started\n",
      "12:11:33 DEBUG:response_closed.complete\n",
      "12:11:33 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 20:11:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '1730', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199977', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_3598ed167d0ac44066274bca84715b22', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bc43570d372eeb-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "12:11:33 DEBUG:request_id: req_3598ed167d0ac44066274bca84715b22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>OpenAI Response</b><br><hr><br>The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a blue sky during the day. \n",
       "\n",
       "During sunrise and sunset, the sun is lower on the horizon, and its light has to pass through a greater thickness of the atmosphere. This increased distance scatters the shorter blue wavelengths out of our line of sight, allowing the longer wavelengths, like red and orange, to dominate the sky's appearance at those times.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For displaying HTML and Markdown responses from ChatGPT\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Enhance with more Html (fg-color, font, etc) as needed but title is usually a good starting point.\n",
    "def colorBox(txt, title=None):\n",
    "    if title is not None:\n",
    "        txt = f\"<b>{title}</b><br><hr><br>{txt}\"\n",
    "\n",
    "    display(HTML(f\"<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'>{txt}</div>\"))\n",
    "\n",
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "colorBox(res, title=\"OpenAI Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color the log output to quickly zero in on errors\n",
    "\n",
    "Code below mostly copied from https://stackoverflow.com/questions/68807282/rich-logging-output-in-jupyter-ipython-notebook 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class DisplayHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        message = self.format(record)\n",
    "        display(message)\n",
    "\n",
    "class HTMLFormatter(logging.Formatter):\n",
    "    level_colors = {\n",
    "        logging.DEBUG: 'lightblue',\n",
    "        logging.INFO: 'dodgerblue',\n",
    "        logging.WARNING: 'goldenrod',\n",
    "        logging.ERROR: 'crimson',\n",
    "        logging.CRITICAL: 'firebrick'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            '<span style=\"font-weight: bold; color: green\">{asctime}</span> '\n",
    "            '[<span style=\"font-weight: bold; color: {levelcolor}\">{levelname}</span>] '\n",
    "            '{message}',\n",
    "            style='{'\n",
    "        )\n",
    "    \n",
    "    def format(self, record):\n",
    "        record.levelcolor = self.level_colors.get(record.levelno, 'black')\n",
    "        return HTML(super().format(record))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the cells above releavels a logger called `openai`\n",
    "# Lets target that and change it's formatter\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "oai_logger = logging.root.manager.loggerDict.get('openai')\n",
    "oai_logger.addHandler(handler)\n",
    "oai_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "colorBox(res, title=\"OpenAI Response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
