{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52900caa-03c3-44ef-a8fc-7fc52643a29e",
   "metadata": {},
   "source": [
    "# Processing and Understanding Text - Dipanjan Sarkar\n",
    "\n",
    "My effort to grok NLP enough to figure out what I need for my tasks. This is part of my mid-up approach (bottom being HMMs etc) to learning NLP. I am also going top-down in trying to understand how to do the tasks in ChatGPT and others. For now the focus is on actually trying to figure out what `nlp task` or combinations of `nlp tasks` covers my requirements so I can get started building a prototype.\n",
    "\n",
    "## References\n",
    " - [A Practitioner's guide to NLP - 2018](./AI/NLP/pdfs/A%20Practitioner's%20Guide%20to%20Natural%20Language%20Processing%20(Part%20I)%20%E2%80%94%20Processing%20%26%20Understanding%20Text%20_%20by%20Dipanjan%20Sarkar.pdf). _This is an old doc but has so much classical NLP references that I think it a must to understand it_\n",
    "\n",
    "# Downloading test data from the web\n",
    "\n",
    "\n",
    "There are many corpora available in various sizes. Dipanjan wants to use a small/current dataset by scraping a news summarization site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e43a179d-2bf4-43f6-9236-d76dd34acf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_js_diagrammers extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_js_diagrammers\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext nb_js_diagrammers\n",
    "import iplantuml\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c79362b-0d01-44e9-9130-687c1660dc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intel urges you to improve business performanc...</td>\n",
       "      <td>Intel shared that Intel vPro has demonstrated ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Challenge is on: Tech Mahindra CEO after OpenA...</td>\n",
       "      <td>Tech Mahindra's MD &amp; CEO CP Gurnani tweeted, \"...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think it is for Jack Dorsey to clarify why h...</td>\n",
       "      <td>Commenting on Jack Dorsey's claim that the Ind...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Be mindful of wearing Reddit gear in public: C...</td>\n",
       "      <td>Reddit CEO Steve Huffman sent an e-mail to the...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accenture to invest $3 bn in AI, double AI tal...</td>\n",
       "      <td>Accenture is set to invest $3 billion in AI ov...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Realme Vice President Madhav Sheth quits compa...</td>\n",
       "      <td>Madhav Sheth, Realme Vice President and Presid...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google to track employee badges to ensure work...</td>\n",
       "      <td>Google has updated its hybrid working policy, ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Many women quit TCS, HR cites work-from-office...</td>\n",
       "      <td>TCS is witnessing a higher attrition rate amon...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US judge temporarily blocks Microsoft's $69-bi...</td>\n",
       "      <td>A judge has granted a request by regulators in...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Googlers left the company to create over 2,000...</td>\n",
       "      <td>Google CEO Sundar Pichai told Bloomberg, \"Goog...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline   \n",
       "0  Intel urges you to improve business performanc...  \\\n",
       "1  Challenge is on: Tech Mahindra CEO after OpenA...   \n",
       "2  I think it is for Jack Dorsey to clarify why h...   \n",
       "3  Be mindful of wearing Reddit gear in public: C...   \n",
       "4  Accenture to invest $3 bn in AI, double AI tal...   \n",
       "5  Realme Vice President Madhav Sheth quits compa...   \n",
       "6  Google to track employee badges to ensure work...   \n",
       "7  Many women quit TCS, HR cites work-from-office...   \n",
       "8  US judge temporarily blocks Microsoft's $69-bi...   \n",
       "9  Googlers left the company to create over 2,000...   \n",
       "\n",
       "                                        news_article news_category  \n",
       "0  Intel shared that Intel vPro has demonstrated ...    technology  \n",
       "1  Tech Mahindra's MD & CEO CP Gurnani tweeted, \"...    technology  \n",
       "2  Commenting on Jack Dorsey's claim that the Ind...    technology  \n",
       "3  Reddit CEO Steve Huffman sent an e-mail to the...    technology  \n",
       "4  Accenture is set to invest $3 billion in AI ov...    technology  \n",
       "5  Madhav Sheth, Realme Vice President and Presid...    technology  \n",
       "6  Google has updated its hybrid working policy, ...    technology  \n",
       "7  TCS is witnessing a higher attrition rate amon...    technology  \n",
       "8  A judge has granted a request by regulators in...    technology  \n",
       "9  Google CEO Sundar Pichai told Bloomberg, \"Goog...    technology  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_urls = ['https://inshorts.com/en/read/technology',\n",
    "             'https://inshorts.com/en/read/sports',\n",
    "             'https://inshorts.com/en/read/world']\n",
    "\n",
    "def build_dataset(seed_urls):\n",
    "    news_data = []\n",
    "    for url in seed_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        \n",
    "        news_articles = [{'news_headline': headline.find('span', \n",
    "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
    "                          'news_article': article.find('div', \n",
    "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                          'news_category': news_category}\n",
    "                         \n",
    "                            for headline, article in \n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        \n",
    "    df =  pd.DataFrame(news_data)\n",
    "    df = df[['news_headline', 'news_article', 'news_category']]\n",
    "    return df\n",
    "\n",
    "news_df = build_dataset(seed_urls)\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9578b3fe-3ba1-4878-8690-d5820a84c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news_category\n",
       "technology    25\n",
       "sports        25\n",
       "world         24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of articles we have available\n",
    "news_df.news_category.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9c155a5-0c51-415d-8018-0e215881e7cf",
   "metadata": {},
   "source": [
    "# Text wrangling and pre-processing step\n",
    "\n",
    "These **pre-processing** steps were covered in more detail in the [NLP_PreProcessing notebook](./NLP_PreProcessing.ipynb), read that if you want to recap the motivation behind all the pre-processing. The code developed in that notebook is simply copied over here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e87c4d9-e10e-4a5c-9a7f-6fb2e4618691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download stop words if not installed already. \n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops_set = set(stops)\n",
    "stops_set.remove('no')   # ??\n",
    "stops_set.remove('not')  # ??\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# old: nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "# Download model first using\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "import iplantuml\n",
    "\n",
    "#--------------------------------------------\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#--------------------------------------------\n",
    "def remove_accented_characters(text):\n",
    "    return unicodedata.normalize(\n",
    "        'NFKD', text\n",
    "    ).encode(\n",
    "        'ascii', 'ignore'\n",
    "    ).decode('utf-8', 'ignore')\n",
    "\n",
    "#--------------------------------------------\n",
    "def expand_contractions(text, contraction_mapping = contractions.contractions_dict):\n",
    "    \n",
    "    # Build a complex pattern from the ORd keys of the contractions dict\n",
    "    contractions_pat = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                  flags = re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "\n",
    "        # Don't like this. Should simply pre-process the map to have all keys LCd\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "            if   contraction_mapping.get(match) \\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        \n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pat.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "#--------------------------------------------\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "#--------------------------------------------\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#--------------------------------------------\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stops_set]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stops_set]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_characters(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d39da0-1ed5-4d86-bd81-c4dae5836256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_text': 'Challenge is on: Tech Mahindra CEO after OpenAI CEO\\'s \\'hopeless\\' remark for India. Tech Mahindra\\'s MD & CEO CP Gurnani tweeted, \"To all those asking...Yes, the challenge is on...Tech Mahindra\\'s AI think tank is in action already.\" His tweet came after ChatGPT maker OpenAI\\'s CEO Sam Altman remarked it will be \"totally hopeless\" for Indian AI startups to compete with his company. Altman, however, said that his remark was \"taken out of context\".',\n",
       " 'clean_text': 'challenge tech mahindra ceo openai ceof hopeless india tech mahindras md ceo cp gurnani tweet ask yes challenge tech mahindra ai think tank action already tweet come chatgpt maker openais ceo sam altman reed totally hopeless indian ai startup compete company altman however say take context'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining headline and article text\n",
    "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]\n",
    "\n",
    "# pre-process text and store the same\n",
    "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
    "norm_corpus = list(news_df['clean_text'])\n",
    "\n",
    "# show a sample news article\n",
    "news_df.iloc[1][['full_text', 'clean_text']].to_dict()\n",
    "\n",
    "# Once cleaned, the clean form can be saved to disk for \n",
    "# later processing\n",
    "# news_df.to_csv('news_cleaned.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90855d7-99da-432a-b1b7-266fbf31d9a4",
   "metadata": {},
   "source": [
    "# Understanding language syntax and structure\n",
    "\n",
    "In english, words combine together to form phrases, clauses and sentences. \n",
    " - Structure and order is crucial here\n",
    " - Remove all ordering and simply dump the words of sentence into an unorderd bag of word loses all of the sentence's meaning.\n",
    " - Structure is crucial for understanding the text and deriving further information from it.\n",
    "\n",
    "Some of the typical _(as of 2018 atleast)_ parsing techniques for understanding text syntax are \n",
    " - Parts of speech _(POS)_ tagging\n",
    " - Shallow parsing or chunking\n",
    " - Constituency parsing\n",
    " - Dependency parsing\n",
    "\n",
    "## Parts of speech (POS) tagging\n",
    "\n",
    "_Parts of speech_ are specific ctategories (lexical/grammatical) to which words are assigned based on their syntactic context and role. Among the major categories are\n",
    " \n",
    " - **N(oun)** - Words that depicits an object or entity which could be living or non-living. POS tag symbol is **N**\n",
    " - **V(erb)** - Words used to describe action, state or occurance. There are several subcategories like auxiliary, reflexive, transitive (and many more). POS tag symbol is **V**\n",
    " - **Adj(ective)** Words used to describe or qualify other words, typically nounds or noun phrases. The phrase _beautiful flower_ has the noun **(N)** _flower_ which is described and qualifed by the adjective (Adj) _beautiful_. The POS tag symbol is **ADJ**.\n",
    " - **Adv(erb)** Words that act as modifiers for other words including nounds, adjectives or other adverbs. The phase _very beautiful flower_ has the adverb (ADV) _very_.\n",
    "\n",
    "\n",
    "There are many other categrories. spacy and nltk both use the [penn treebank POS tags]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2db7bd-6763-4f56-ae9d-9a86fc7cb84e",
   "metadata": {},
   "source": [
    "## Penn Treebank POS tags\n",
    "(https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) which are listed here (_generated by chatGPT for me :-)_ )\n",
    "| POS Tag | Description                       |\n",
    "|---------|-----------------------------------|\n",
    "| CC      | Coordinating conjunction          |\n",
    "| CD      | Cardinal number                   |\n",
    "| DT      | Determiner                        |\n",
    "| EX      | Existential there                 |\n",
    "| FW      | Foreign word                      |\n",
    "| IN      | Preposition or subordinating conjunction |\n",
    "| JJ      | Adjective                         |\n",
    "| JJR     | Adjective, comparative            |\n",
    "| JJS     | Adjective, superlative            |\n",
    "| LS      | List item marker                  |\n",
    "| MD      | Modal                             |\n",
    "| NN      | Noun, singular or mass            |\n",
    "| NNS     | Noun, plural                      |\n",
    "| NNP     | Proper noun, singular             |\n",
    "| NNPS    | Proper noun, plural               |\n",
    "| PDT     | Predeterminer                     |\n",
    "| POS     | Possessive ending                 |\n",
    "| PRP     | Personal pronoun                  |\n",
    "| `PRP$`  | Possessive pronoun                |\n",
    "| RB      | Adverb                            |\n",
    "| RBR     | Adverb, comparative               |\n",
    "| RBS     | Adverb, superlative               |\n",
    "| RP      | Particle                          |\n",
    "| SYM     | Symbol                            |\n",
    "| TO      | to                                |\n",
    "| UH      | Interjection                      |\n",
    "| VB      | Verb, base form                   |\n",
    "| VBD     | Verb, past tense                  |\n",
    "| VBG     | Verb, gerund or present participle |\n",
    "| VBN     | Verb, past participle             |\n",
    "| VBP     | Verb, non-3rd person singular present |\n",
    "| VBZ     | Verb, 3rd person singular present |\n",
    "| WDT     | Wh-determiner                     |\n",
    "| WP      | Wh-pronoun                        |\n",
    "| `WP$`   | Possessive wh-pronoun             |\n",
    "| WRB     | Wh-adverb                         |\n",
    "\n",
    "\n",
    "\n",
    "As this shows, the basic **N** tag for noun is further subdivided into singular noun _(NN)_, singular proper noun _(NNP)_ and plural nouns _(NNS)_. \n",
    "\n",
    "The process of classifying and labeling words with their POS tags is called POS tagging. processing like \n",
    "\n",
    " - narrowing down on noun phrases\n",
    " - word sense disambigauation\n",
    " - etc \n",
    "\n",
    "## Universal POS tags\n",
    "\n",
    "Universal Dependencies is a newer alternative to the penn tree bank. UD offers a set of language semantics and structures that are applicable \n",
    "in multiple languages. Not just english\n",
    "\n",
    " UD Tag | Description\n",
    " --- | ---\n",
    "ADJ | adjective\n",
    "ADP | adposition\n",
    "ADV | adverb\n",
    "AUX | auxiliary\n",
    "CCONJ | coordinating conjunction\n",
    "DET | determiner\n",
    "INTJ | interjection\n",
    "NOUN | noun\n",
    "NUM | numeral\n",
    "PART | particle\n",
    "PRON | pronoun\n",
    "PROPN | proper noun\n",
    "PUNCT | punctuation\n",
    "SCONJ| subordinating conjunction\n",
    "SYM| symbol\n",
    "VERB| verb\n",
    "X| other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade4ec79-53cc-4eb6-bc99-3d3931611f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>Tag type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Challenge</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tech</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mahindra</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CEO</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>after</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CEO</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'</td>\n",
       "      <td>``</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hopeless</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'</td>\n",
       "      <td>''</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>remark</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>India</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS tag Tag type\n",
       "0   Challenge      NN     NOUN\n",
       "1          is     VBZ      AUX\n",
       "2          on      IN      ADP\n",
       "3           :       :    PUNCT\n",
       "4        Tech      NN     NOUN\n",
       "5    Mahindra     NNP    PROPN\n",
       "6         CEO     NNP    PROPN\n",
       "7       after      IN      ADP\n",
       "8      OpenAI     NNP    PROPN\n",
       "9         CEO     NNP    PROPN\n",
       "10         's     POS     PART\n",
       "11          '      ``    PUNCT\n",
       "12   hopeless      JJ      ADJ\n",
       "13          '      ''    PUNCT\n",
       "14     remark      NN     NOUN\n",
       "15        for      IN      ADP\n",
       "16      India     NNP    PROPN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a basic pre-processed corpus, don't lowercase so we can get POS context\n",
    "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
    "                          text_lemmatization=False, special_char_removal=False)\n",
    "\n",
    "# demo for POS tagging for sample news headline\n",
    "sentence = str(news_df.iloc[1].news_headline)\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "# POS tagging with Spacy \n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb489df-308f-472a-bf27-1d96d9c430e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Challenge</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on:</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tech</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mahindra</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CEO</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>after</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CEO's</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'hopeless'</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>remark</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>India</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word POS tag\n",
       "0    Challenge     NNP\n",
       "1           is     VBZ\n",
       "2          on:      JJ\n",
       "3         Tech     NNP\n",
       "4     Mahindra     NNP\n",
       "5          CEO     NNP\n",
       "6        after      IN\n",
       "7       OpenAI     NNP\n",
       "8        CEO's     NNP\n",
       "9   'hopeless'     POS\n",
       "10      remark      NN\n",
       "11         for      IN\n",
       "12       India     NNP"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging with nltk\n",
    "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb972f6-23e4-4ae0-8d70-dd69362e61e6",
   "metadata": {},
   "source": [
    "## Shallow parsing or chunking\n",
    "\n",
    "Groups of words make up phrases. **Shallow Parsing** _(AKA Light parsing or chunking)_ is about breaking text down into it's constituent parts (tokens/words) and then grouping then together into highter level phrases. This incudes POS tagging as well as as phrases.\n",
    " - Phrases retain their order and structure\n",
    " - For major types of phrases\n",
    "   - **Noun phrase (NP)** A phrase where the Noun acts as a head word (??), a noun phrase acts as a subject or object to a verb\n",
    "   - **Verb phrase (VP)** A phrase where the verb acts as a head word.\n",
    "   - **Adjective phrase (ADJP)**\n",
    "   - **Adverb phrase (ADVP)**\n",
    "   - **Prepositional phrase (PP)**\n",
    "\n",
    "> Chunking is built using POS tags as it's contituent items\n",
    "\n",
    "### Chunking training data\n",
    "\n",
    "The **conll2000** corpus provided by nltk is used to train the shallow parser model. This corpus is available with chunking annotations. A `help(conll2000)` gives the following info\n",
    " - three columns: `words`, `pos`, `chunk`.\n",
    " - Maybe it is documented elsewhere but not clear what `chunked_sents()` returns\n",
    " - around 10k records are being used for training _(and 48 for testingN)_\n",
    " - _conll_ stands for _conference on natural language ..._\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdadcee-8538-405f-84ba-ea763a747b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3335620e-55ab-47fa-a1e2-d299ee13bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10900 48\n",
      "(S\n",
      "  Chancellor/NNP\n",
      "  (PP of/IN)\n",
      "  (NP the/DT Exchequer/NNP)\n",
      "  (NP Nigel/NNP Lawson/NNP)\n",
      "  (NP 's/POS restated/VBN commitment/NN)\n",
      "  (PP to/TO)\n",
      "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
      "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "  (NP a/DT freefall/NN)\n",
      "  (PP in/IN)\n",
      "  (NP sterling/NN)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT past/JJ week/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "data = conll2000.chunked_sents()\n",
    "train_data = data[:10900]\n",
    "test_data = data[10900:]\n",
    "\n",
    "print(len(train_data), len(test_data))\n",
    "print(train_data[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af1b4a2b-ea62-446e-afb6-19c069bbdc35",
   "metadata": {},
   "source": [
    "Each item is of type `nltk.tree.tree.Tree` and when printed looks like this\n",
    "\n",
    "```\n",
    "(S\n",
    "  Chancellor/NNP\n",
    "  (PP of/IN)\n",
    "  (NP the/DT Exchequer/NNP)\n",
    "  (NP Nigel/NNP Lawson/NNP)\n",
    "  (NP 's/POS restated/VBN commitment/NN)\n",
    "  (PP to/TO)\n",
    "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
    "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
    "  (NP a/DT freefall/NN)\n",
    "  (PP in/IN)\n",
    "  (NP sterling/NN)\n",
    "  (PP over/IN)\n",
    "  (NP the/DT past/JJ week/NN)\n",
    "  ./.)\n",
    "```\n",
    "\n",
    "You can kinda see _(in a lispy syntax)_\n",
    " - Sentence\n",
    " - Broken up into chunks\n",
    " - Each chunk with POS tagged words\n",
    "\n",
    "The following functions are used to process this further\n",
    " - `tree2conlltags` to get triples of `(word, tag, chunk-tags)`\n",
    " - `conlltags2tree` to generate a parse tree from these triples.\n",
    "\n",
    "\n",
    "The chunk tags use the `IOB` format which means the following\n",
    " - `I` stands for _Inside_\n",
    "   - **I-** prefix before a tag indicates that it is inside a tag\n",
    " - `O` stands for _Outside_\n",
    "   - **O** tag _(not a prefix)_ indicates that the token it not part of a chunk\n",
    " - `B` stands for _Beginning_\n",
    "   - **B-** prefix before a tag indicates that it is the beginning of a chunk.\n",
    "   - `B-` tags are followed by `I-` tags and end with a new phrase `B-`:prefix or a non-chunk with `O`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a415680-b19e-431e-ac0e-57ecc3df25e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chancellor', 'NNP', 'O'),\n",
       " ('of', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('Exchequer', 'NNP', 'I-NP'),\n",
       " ('Nigel', 'NNP', 'B-NP'),\n",
       " ('Lawson', 'NNP', 'I-NP'),\n",
       " (\"'s\", 'POS', 'B-NP'),\n",
       " ('restated', 'VBN', 'I-NP'),\n",
       " ('commitment', 'NN', 'I-NP'),\n",
       " ('to', 'TO', 'B-PP'),\n",
       " ('a', 'DT', 'B-NP'),\n",
       " ('firm', 'NN', 'I-NP'),\n",
       " ('monetary', 'JJ', 'I-NP'),\n",
       " ('policy', 'NN', 'I-NP'),\n",
       " ('has', 'VBZ', 'B-VP'),\n",
       " ('helped', 'VBN', 'I-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('prevent', 'VB', 'I-VP'),\n",
       " ('a', 'DT', 'B-NP'),\n",
       " ('freefall', 'NN', 'I-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('sterling', 'NN', 'B-NP'),\n",
       " ('over', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('past', 'JJ', 'I-NP'),\n",
       " ('week', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "# Sample code to convert the tree to an array of triples\n",
    "wtc = tree2conlltags(train_data[1])\n",
    "wtc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a4d0b67-ef32-49c3-a81e-a10222351fbf",
   "metadata": {},
   "source": [
    "Develop the following new functions\n",
    " - `conll_tag_chunks` which extracts the tag and chunk_tag only (skipping thw word)\n",
    " - `combined_tagger` which tags multiple taggers with backoff taggers _(no idea what this means yet)_\n",
    "\n",
    "Then develop a function `NGramTagChunker` that \n",
    " - takes in tagged sentences as training input\n",
    " - Gets their _(word, POS tag, Chunk Tag)_ **WTC** triple and train a\n",
    "   - BigramTagger with a **Unigram** tagger as the backoff tagger _(ok, starting to make some sense now)_\n",
    " - define a `parse` method to perform shallow parsing on new sentences.\n",
    "\n",
    "We'll use this `NGramTagChunker` to train on the **conll2000** dataset. Train on **train_data** and evaluate on **test_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e626f6dc-938a-42fb-8521-1117577668bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1488.0,168.0\" width=\"1488px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"6.45161%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Chancellor</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.22581%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.15054%\" x=\"6.45161%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.52688%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.60215%\" x=\"8.60215%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68.75%\" x=\"31.25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Exchequer</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.625%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.9032%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.06452%\" x=\"17.2043%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"46.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Nigel</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.3333%\" x=\"46.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Lawson</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.2366%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"14.5161%\" x=\"25.2688%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"18.5185%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">'s</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">POS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.25926%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"37.037%\" x=\"18.5185%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">restated</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.037%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"44.4444%\" x=\"55.5556%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">commitment</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.7778%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.5269%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.15054%\" x=\"39.7849%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.8602%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.0538%\" x=\"41.9355%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"14.2857%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.14286%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"21.4286%\" x=\"14.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">firm</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.7143%\" x=\"35.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">monetary</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.5714%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"28.5714%\" x=\"71.4286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">policy</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.7143%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.4624%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.9785%\" x=\"56.9892%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"19.2308%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">has</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.61538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"30.7692%\" x=\"19.2308%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">helped</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.6154%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.3846%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.6923%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"34.6154%\" x=\"65.3846%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">prevent</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.6923%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.9785%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.52688%\" x=\"70.9677%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"28.5714%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"71.4286%\" x=\"28.5714%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">freefall</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.2857%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.7312%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.15054%\" x=\"78.4946%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5699%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.37634%\" x=\"80.6452%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">sterling</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.22581%\" x=\"86.0215%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">over</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.6344%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.13978%\" x=\"89.2473%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.2941%\" x=\"29.4118%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">past</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.0588%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.2941%\" x=\"64.7059%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">week</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.3529%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.8172%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.6129%\" x=\"98.3871%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.1935%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Chancellor', 'NNP'), Tree('PP', [('of', 'IN')]), Tree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), Tree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), Tree('PP', [('to', 'TO')]), Tree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), Tree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), Tree('NP', [('a', 'DT'), ('freefall', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('sterling', 'NN')]), Tree('PP', [('over', 'IN')]), Tree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Needs svgling. If needed\n",
    "# pip install svgling\n",
    "display(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4f34c7-440f-4d14-86dc-33e7e517249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extact pos and chunk tags from sentences ignoring the word\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [ [(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "# Trains multiple taggers with backoff taggers\n",
    "# New to me, this concept of how nlkt etc use taggers and backoff taggers!\n",
    "def combined_tagger(train_data, taggers, backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9e7405-926f-4fe4-9448-e2cb908db26b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            ---\n",
       "title: Taggers\n",
       "---\n",
       "classDiagram    \n",
       "    NGramTagger &lt;|-- UnigramTagger\n",
       "    NGramTagger &lt;|-- BigramTagger\n",
       "    NGramTagger &lt;|-- TrigramTagger\n",
       "    ContextTagger &lt;|-- NGramTagger\n",
       "    SequentialBackoffTagger &lt;|-- ContextTagger\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"500\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x7fb7e899f820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 500\n",
    "\n",
    "---\n",
    "title: Taggers\n",
    "---\n",
    "classDiagram    \n",
    "    NGramTagger <|-- UnigramTagger\n",
    "    NGramTagger <|-- BigramTagger\n",
    "    NGramTagger <|-- TrigramTagger\n",
    "    ContextTagger <|-- NGramTagger\n",
    "    SequentialBackoffTagger <|-- ContextTagger    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e4af8a-3831-4bbb-94f0-a9a456c17a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  90.0%%\n",
      "    Precision:     82.1%%\n",
      "    Recall:        86.3%%\n",
      "    F-Measure:     84.1%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag   import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# define the chunker class\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "\n",
    "    # train_sentences: A nltk Tree\n",
    "    def __init__(self, train_sentences, tagger_classes = [UnigramTagger, BigramTagger]):\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "    \n",
    "    def parse(self, tagged_sentence):\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "\n",
    "        pos_tags       = [tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags     = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags       = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag) in zip(tagged_sentence, chunk_tags)]\n",
    "        \n",
    "        return conlltags2tree(wpc_tags)\n",
    "\n",
    "# train chunker model\n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "# evaluate model on test data\n",
    "# Sarkar used evaluate(test_data) but that is now deprecated\n",
    "print(ntc.accuracy(gold=test_data))\n",
    "                     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7533984a-581c-461d-8cf5-7a6bebf8fd83",
   "metadata": {},
   "source": [
    "This gets an accuacy of 90% _(No change between 2018 and 2023 which shows that the underlying model hasn't changed or improved any)_. Test this on the sample news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c11fa5-fe97-49a3-b873-dd80ea263a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Challenge/NNP)\n",
      "  (VP is/VBZ)\n",
      "  (NP on:/JJ Tech/NNP Mahindra/NNP CEO/NNP)\n",
      "  (PP after/IN)\n",
      "  (NP OpenAI/NNP CEO's/NNP)\n",
      "  (NP 'hopeless'/POS remark/NN)\n",
      "  (PP for/IN)\n",
      "  (NP India/NNP))\n"
     ]
    }
   ],
   "source": [
    "chunk_tree = ntc.parse(nltk_pos_tagged)\n",
    "print(chunk_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47fe9e52-98f6-40c8-b5dd-27be101a7b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,768.0,168.0\" width=\"768px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"11.4583%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Challenge</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.72917%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.20833%\" x=\"11.4583%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.0625%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.0833%\" x=\"16.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"19.2308%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">on:</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.61538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"23.0769%\" x=\"19.2308%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Tech</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.7692%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.4615%\" x=\"42.3077%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Mahindra</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.5385%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"19.2308%\" x=\"80.7692%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CEO</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.3846%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.2083%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.29167%\" x=\"43.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">after</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.3958%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.625%\" x=\"51.0417%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">OpenAI</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CEO's</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.8542%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.8333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"60%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">'hopeless'</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">POS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">remark</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.0833%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.20833%\" x=\"87.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">for</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.1042%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.29167%\" x=\"92.7083%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">India</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.3542%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [('Challenge', 'NNP')]), Tree('VP', [('is', 'VBZ')]), Tree('NP', [('on:', 'JJ'), ('Tech', 'NNP'), ('Mahindra', 'NNP'), ('CEO', 'NNP')]), Tree('PP', [('after', 'IN')]), Tree('NP', [('OpenAI', 'NNP'), (\"CEO's\", 'NNP')]), Tree('NP', [(\"'hopeless'\", 'POS'), ('remark', 'NN')]), Tree('PP', [('for', 'IN')]), Tree('NP', [('India', 'NNP')])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(chunk_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e519f49-dd3b-4981-a0f5-46cb56f351c9",
   "metadata": {},
   "source": [
    "From the above analysis, you can see it detect\n",
    "\n",
    " Chunk Tag | Text \n",
    " --- | --- \n",
    " Noun Phrase | Twitter(NN) Spaces(NNP) team(NN)\n",
    " `down` | ??\n",
    " Verb Phrase | to(TO) roughly(VB)\n",
    " Noun Phrase | 3(CD) employees(NNS)\n",
    " Prepositional Phrase | from(IN)\n",
    " Noun Phrase | 100:(CD) Report(NN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc0bb17a-5f13-456c-9878-c297f13da3fb",
   "metadata": {},
   "source": [
    "# Constituency Parsing\n",
    "\n",
    "Constituent-based grammars _(like an actual parser generator grammar)_ are used to analyze sentences and determine their constituent parts. These grammars model the structure of the sentence in terms of a heirarchy of their constituent parts.\n",
    " - each word belongs to a specific category and forms the head word of different phrases _(each word is a head ??)_\n",
    " - the phrases are formed based on rules called _phrase structure rules_ which form the core of the constituency grammars. They determine\n",
    "   - what words are used to construct the phrases or constituents\n",
    "   - how the constituents are ordered\n",
    "   - generic representation is $S \\rightarrow AB$ represents a structure $S$ which consists of two constituents $A$ and $B$ in that order. $S \\rightarrow NP VP$ signifies a sentence which is built from a _Noun Phrase(NP)_ followed by a _Verb Phrase(VP)_.\n",
    "\n",
    "## Stanford NLP references\n",
    "His examples use the `StanfordParser` java libs. `StanfordParser` is a PCFG parser _(Probabilistic Context Free Grammar)_ where it holds multiple possible structures in memory while parsing a sentence and picks the highest probability one at the end.\n",
    "\n",
    "> Some requests online for something alternative to StanforParser as it always has a deterministic output even when sentence is ambiguous. One of the responses was to look at http://maltparser.org. Not sure if there is a way to tweak StanfordNLP itself.\n",
    ">\n",
    "> Also StanfordNLP is now (2023) part of the `stanza` package which needs to be installed via `mamba install -c stanfordnlp stanza`. It seems to still use java. If it works out, it will be simpler to use it from scala.\n",
    "\n",
    "The code in the article was as follows\n",
    "```python\n",
    "# set java path\n",
    "import os\n",
    "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "                   \n",
    "result = list(scp.raw_parse(ce))`\n",
    "print\n",
    "```\n",
    "\n",
    "## Upgrade example to stanza\n",
    "\n",
    "Stanza related information has been moved to [NLP_StanfordCoreNLP_Stanza.ipynb](./NLP_StanfordCoreNLP_Stanza.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0cee8d1-b4c2-446a-b226-5e0433ebcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time download of english language model. There are many other models\n",
    "# These are saved under ~/stanza_resources\n",
    "# import stanza\n",
    "# stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e2784e9-f43b-4748-b8bb-77b920642d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 22:55:38 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c87fa6ef9bf4ecb94dfd14d5a348bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 22:55:39 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-06-14 22:55:39 INFO: Using device: cuda\n",
      "2023-06-14 22:55:39 INFO: Loading: tokenize\n",
      "2023-06-14 22:55:40 INFO: Loading: pos\n",
      "2023-06-14 22:55:40 INFO: Loading: lemma\n",
      "2023-06-14 22:55:40 INFO: Loading: constituency\n",
      "2023-06-14 22:55:41 INFO: Loading: depparse\n",
      "2023-06-14 22:55:41 INFO: Loading: sentiment\n",
      "2023-06-14 22:55:41 INFO: Loading: ner\n",
      "2023-06-14 22:55:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Challenge', 3, 'nsubj')\n",
      "('is', 3, 'cop')\n",
      "('on', 0, 'root')\n",
      "(':', 7, 'punct')\n",
      "('Tech', 7, 'compound')\n",
      "('Mahindra', 7, 'compound')\n",
      "('CEO', 3, 'parataxis')\n",
      "('after', 14, 'case')\n",
      "('OpenAI', 10, 'compound')\n",
      "(\"CEO's\", 14, 'nmod:poss')\n",
      "(\"'\", 10, 'case')\n",
      "('hopeless', 14, 'amod')\n",
      "(\"'\", 14, 'punct')\n",
      "('remark', 7, 'nmod')\n",
      "('for', 16, 'case')\n",
      "('India', 14, 'nmod')\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza_nlp = stanza.Pipeline('en')  # Setup a neural pipeline in english\n",
    "doc = stanza_nlp(str(news_df.iloc[1].news_headline))\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a08d56-d4d0-4454-bc3b-a57cf2996d0e",
   "metadata": {},
   "source": [
    "# Dependency parsing\n",
    "\n",
    "Dependency parsing adds to annotation of a constituency parsning by also including semantic dependency information. The basic principle is that\n",
    "\n",
    "> In any sentence, all words except one have some relationship or dependency on other words in the setence. The word that has no dependency is called the **root word**. The verb is taken as the root of the sentence in most cases. All other sentence words are directly or indirectly linked to the root verb using **links** which are the dependencies.\n",
    "\n",
    " - All words _except one_ in a sentence are dependent on other words\n",
    " - The word that is not dependent on any is called the **root word**\n",
    " - The verb is usually the root _(when is it not ?)_\n",
    " - The links form the semantic dependencies\n",
    "\n",
    "## Universal dependencies\n",
    "\n",
    "The dependency parse diagrams do not use the penn-treebank tags. They use a more moden set of tags which are generalized to apply to multiple languages (_not just english_). See\n",
    " - Dependeny list reference at [Universal dependencies relationships](https://universaldependencies.org/u/dep/index.html)\n",
    " - There are **37** basic universal dependencies\n",
    " - Each language can define additional subtypes of one of the 37 basic ones. Such subtypes are prefixed with the basic type. For instance `aux:pass` is a `passive auxiliary` which is a sub-type of `aux`.\n",
    "   - Some subtypes apply to many language\n",
    " - There are also [enhanced dependencies](https://universaldependencies.org/u/overview/enhanced-syntax.html) for additional complex dependency representation.\n",
    "\n",
    "> At this point, we are getting deep into linguistics without a solid grounding in it. Totally a case for having an expert advisor or partner on board.\n",
    "\n",
    "\n",
    "## Spacy and dependencies\n",
    "\n",
    "> Online forums suggest that spacy is much faster than StanfordNLP for some tasks. Need to time it and figure out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c68b7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]<---Challenge[nsubj]--->['CEO']\n",
      "--------\n",
      "['Challenge']<---is[ROOT]--->['on', ':', 'after']\n",
      "--------\n",
      "[]<---on[prep]--->[]\n",
      "--------\n",
      "[]<---:[punct]--->[]\n",
      "--------\n",
      "[]<---Tech[compound]--->[]\n",
      "--------\n",
      "['Tech']<---Mahindra[compound]--->[]\n",
      "--------\n",
      "['Mahindra']<---CEO[appos]--->[]\n",
      "--------\n",
      "[]<---after[prep]--->['remark']\n",
      "--------\n",
      "[]<---OpenAI[compound]--->[]\n",
      "--------\n",
      "['OpenAI']<---CEO[poss]--->[\"'s\"]\n",
      "--------\n",
      "[]<---'s[case]--->[]\n",
      "--------\n",
      "[]<---'[punct]--->[]\n",
      "--------\n",
      "[]<---hopeless[amod]--->[]\n",
      "--------\n",
      "[]<---'[punct]--->[]\n",
      "--------\n",
      "['CEO', \"'\", 'hopeless', \"'\"]<---remark[pobj]--->['for']\n",
      "--------\n",
      "[]<---for[prep]--->['India']\n",
      "--------\n",
      "[]<---India[pobj]--->[]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Print each link separately\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in sentence_nlp:\n",
    "    print(dependency_pattern.format(word=token.orth_, \n",
    "                                  w_type=token.dep_,\n",
    "                                  left=[t.orth_ \n",
    "                                            for t \n",
    "                                            in token.lefts],\n",
    "                                  right=[t.orth_ \n",
    "                                             for t \n",
    "                                             in token.rights]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a873cf6f",
   "metadata": {},
   "source": [
    "\n",
    "## Stanza and dependencies\n",
    "\n",
    "> Anything beyond the standard dependencies needs Java and the coreNLP package to be installed\n",
    ">\n",
    "> use `stanza.install_corenlp()` which installs it into `~/stanza_corenlp`\n",
    "\n",
    "### The different kinds of dependencies\n",
    "\n",
    "The Stanza Python API by default only supports `standard Universal Dependencies`. The CoreNLP server product _(java)_ however supports multiple types of universal dependencies (all through the same `depparse` module that is used by stanza)\n",
    " - BasicDependenciesAnnotation\n",
    "   - _(aka basic)_ - just the 37 standard UD dependencies\n",
    "   - Easy to parse, small set\n",
    " - EnhancedDependenciesAnnotation : _(aka enhanced)_\n",
    "   - From the JSON response, this is **likely** available under the `sentence[enhancedDependencies]` key.\n",
    " - EnhancedPlusPlusDependenciesAnnotation : _(aka enhanced++)_\n",
    "   - From the JSON response, this is available under the `sentence[enhancedPlusPlusDependencies]` key.\n",
    "   - I heard about this from [coreNLP Dtree Visualizer](https://github.com/doug919/corenlp_dtree_visualizer) which converts the CoreNLP dependency tree into a format that can be visualized by spacy!.\n",
    "   - Deterministic transformations on the basic dependencies\n",
    "   - Lot more semantic information\n",
    "\n",
    "```python\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "with ud_enhancer.UniversalEnhancer(language=\"en\") as enhancer:\n",
    "    depparseFromStanza = nlp(\"This is a test\")\n",
    "    depparseEnhanced = enhancer.process(depparseFromStanza)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f12197f-eddc-4329-a854-e549ffc755f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Challenge', 3, 'nsubj')\n",
      "('is', 3, 'cop')\n",
      "('on', 0, 'root')\n",
      "(':', 7, 'punct')\n",
      "('Tech', 7, 'compound')\n",
      "('Mahindra', 7, 'compound')\n",
      "('CEO', 3, 'parataxis')\n",
      "('after', 14, 'case')\n",
      "('OpenAI', 10, 'compound')\n",
      "(\"CEO's\", 14, 'nmod:poss')\n",
      "(\"'\", 10, 'case')\n",
      "('hopeless', 14, 'amod')\n",
      "(\"'\", 14, 'punct')\n",
      "('remark', 7, 'nmod')\n",
      "('for', 16, 'case')\n",
      "('India', 14, 'nmod')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "851490e2-9cbf-43d2-bc1f-be83fea1164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://github.com/stanfordnlp/stanza/issues/359\n",
    "# To get enhanced stanza dependencies. This is one way\n",
    "# Needs the StanfordCoreNLP java process to be running\n",
    "import stanza.server.ud_enhancer as ud_enhancer\n",
    "# ud_enhancer.process_doc(doc, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba89899-2518-40fc-a6b9-a5abc41d10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 22:56:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2accd7344ae74d6ca0e0ac0240317781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 22:56:06 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-06-14 22:56:06 INFO: Using device: cuda\n",
      "2023-06-14 22:56:06 INFO: Loading: tokenize\n",
      "2023-06-14 22:56:06 INFO: Loading: pos\n",
      "2023-06-14 22:56:07 INFO: Loading: lemma\n",
      "2023-06-14 22:56:07 INFO: Loading: depparse\n",
      "2023-06-14 22:56:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/stanfordnlp/stanza/issues/359\n",
    "enh_ud_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "with ud_enhancer.UniversalEnhancer(language=\"en\") as enhancer:\n",
    "    depparseFromStanza = enh_ud_nlp(\"John said that he loved mila\")\n",
    "    depparseEnhanced = enhancer.process(depparseFromStanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd96bd27-8305-4d02-a234-a33a6b0a3534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5cd21ef3c0c746bdaf17dfeff4fe7249-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">said</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">he</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PRP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">loved</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mila</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5cd21ef3c0c746bdaf17dfeff4fe7249-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This gets the enhancedPlusPludDependencies\n",
    "#print(depparseEnhanced.sentence[0].enhancedPlusPlusDependencies)\n",
    "\n",
    "from hillops.nlp.viz import get_displaycy_from_stanza_dependencies\n",
    "tree = get_displaycy_from_stanza_dependencies(\n",
    "    depparseEnhanced.sentence[0].token,\n",
    "    depparseEnhanced.sentence[0].enhancedPlusPlusDependencies)\n",
    "\n",
    "displacy.render(tree, style=\"dep\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
