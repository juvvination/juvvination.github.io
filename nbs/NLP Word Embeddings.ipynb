{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608972c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d987406-6e27-41e3-9702-00f904d736df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word Embeddings in NLP\n",
    "\n",
    "## Resources\n",
    "\n",
    "**From Ruder**\n",
    "\n",
    " - [Ruder - On Word Embeddings - Part 1](https://www.ruder.io/word-embeddings-1/),  [📃PDF](./pdfs/On%20word%20embeddings%20-%20Part%201.pdf)\n",
    " - [Ruder - On Word Embeddings - Part 2]( ), [📃PDF](./pdfs/On%20word%20embeddings%20-%20Part%202%20-%20Approximating%20the%20Softmax%20for%20Learning%20WordEmbeddings.pdf)\n",
    " - [Ruder - On Word embeddings - Part 3 - The secret ingredients of word2vec](https://www.ruder.io/secret-word2vec/). [📃PDF](./pdfs/On%20word%20embeddings%20-%20Part%203%20-%20The%20secret%20ingredients%20of%20word2vec.pdf) \n",
    "\n",
    "**Other**\n",
    " - [📽️ Embeddings for everything](https://www.youtube.com/watch?v=JGHVJXP9NHw)\n",
    " - [📽️ A complete overview of word embeddings](https://www.youtube.com/watch?v=5MaWmXwxFNQ)\n",
    "\n",
    "**Even older history**\n",
    "  - [Brown clustering](https://en.wikipedia.org/wiki/Brown_clustering)\n",
    "  - [Latent semantic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "\n",
    "# Motivations\n",
    "\n",
    "Why do we even need to do this conversion to numbers and deal with embedding.\n",
    "\n",
    "## Text to numbers\n",
    "\n",
    "Machine learning models all use numberical representations. The underlying `regression` and `loss` function logic all operate in terms of numbers. To put such ML models to use for NLP, the text has to converted to a numerical representation. This representational need is what eventually motivated embeddings.\n",
    "\n",
    "**Per Ruder**: _Word embeddings refer to dense representation of words in a low-dimensional vector space_. He particularly focuses on _neural word embeddings_, i.e., word embedings learned by a neural network.\n",
    "\n",
    "## Search space\n",
    "\n",
    "This come later on the scene but is a crucial motivator of advanced techniques.\n",
    " - In the target embedding space, we can target semantic similarity. i.e., vectors close to each other are semantically similar\n",
    " - Current techniques allows embedding words, sentences or entire documents into a semantically meaningful embedding space.\n",
    " - Research techniques also allow embedding of mixed media: text, images, videos etc into the same target space so you can locate text and the images/videos are that are semantically similar.\n",
    "\n",
    "This video by a google researcher at berkely, [Embeddings for everything: Search in the neural network era](https://www.youtube.com/watch?v=JGHVJXP9NHw) shows how they approach training embeddings for such a search space. There is no magic here, simply the use of training combined multiple encoding schemes to achieve such a mixed media search. The generalization of this to a search problem is pretty powerful.\n",
    " - Dual and Multi encoder models\n",
    " - [Recipe2vec - How word2vec helps us discover related tasty recipes](https://www.youtube.com/watch?v=RTyHP_PiX9M)\n",
    "   - uses word2vec with skip-gram techniques. Played around and ended up with 70 dimension starting space and ended with 2 dimensions ?  \n",
    "   - t-sne for dimensionality reduction (PCA is another option). Hyperparameters matter a lot\n",
    "   - building recipe-vec from word-vecs of ingredients. _Lots of scope here for stages, cooking temps, styles etc as well_.\n",
    "   - testing prototyping steps\n",
    "   - productioning things\n",
    "   - using cosine similarity to get similar recipes from the database\n",
    "   - Retrain their model every 12 hours.\n",
    "   - Lots of good questions about the actual modeling. _This always needs careful thought_ and how they measured success rates in the product.\n",
    "\n",
    "## Neural embeddings\n",
    "\n",
    " If you look back to encoding, even huffman encoding, jpeg blocks and maybe encryption schemes are all ways of using optimized numerical representations. What `neural embedding` or more appropriately, `neural embedding search` aims to do is searching for a good embedding. This is done via standard back-propogation apprach: an optimization problem. The design of the cost function etc uses.\n",
    "\n",
    " - Cross entropy\n",
    " - Cosine similarity\n",
    "\n",
    "## What high dimensional vector space ?\n",
    "\n",
    "Quickly found a resource to explain what this _high dimensional_ space is in the first place\n",
    " \n",
    " - [Youtube - A brief history of word embeddngs](https://www.youtube.com/watch?v=5MaWmXwxFNQ)\n",
    " \n",
    "## History - one hot encoding\n",
    "\n",
    "> Each word has a representation\n",
    "\n",
    "This is where it all starts: a simple, naive mapping from text to numerical representation. The idea has is to establish a vocabulary of the text we are training on. i.e, a set of the words used in the text. Given that vocabulary, each word is represented as a one-hot vector (_i.e, just one bit is hot: 1 and the rest are 0_).\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Vocab &= \\begin{vmatrix} I & You & Bag & Apple & Cat & Dog \\end{vmatrix} \\\\\n",
    "Rep_{bag} &= \\begin{vmatrix}0 & 0 & 1 & 0 & 0 & 0 \\end{vmatrix}\n",
    "\\end{align}\n",
    "$  \n",
    "\n",
    " - Long vector as long as the size of the vocabulary. $\\begin{vmatrix}V\\end{vmatrix} = n$ say.\n",
    " - Each dimension of the vector is one unique word in the vocabulary. _You can see this creating a large vocabulary, thus creating the `high dimensional` space. A problem which we will later have to solve_\n",
    " - One word is encoded per vector: each word representation is a vector `n` long with a `1` for it's word axis and `0` everywhere else\n",
    "\n",
    "The location of the hot bit is an index into the vocabulary. _Not sure why a approach using a simple numerical index into the vocbulary was discarded. Some constraint related to all inputs having to be the same sized tensor ?_.\n",
    "\n",
    "If the size of this vocabulary is `n`, then we can think of each one-hot representation as belonging to a `n dimensional` space with each word defining an axis. Hence the _high dimensional vector space_ terminology\n",
    "\n",
    "This introduces some complications because of the high dmensionality, size of the vocabulary.\n",
    "   - Large memory use for one.\n",
    "   - Inefficient computation as you are mostly multiplying zeros.\n",
    "   - Sparse vectors (_just the one bit in a sea of zeros, Inefficient use of memory space_)\n",
    " \n",
    "## History - Word counting approaches \n",
    "\n",
    "> Try to pack entire sentence into one representation\n",
    "\n",
    "### Bag of words\n",
    "\n",
    " - Each word in vocabulary is assigned a dimension in the vector space of size `n`\n",
    " - Ignores order in which words occur, just counts the number of times each word occurs in a sentence\n",
    " - A sentence representation is a vector of size `n` with a hot bit for each word that is in the sentence.\n",
    " \n",
    "For instance, the represetnation of _He arrives precisely when he mans to_ in a contrived vocabulary is as follows. This shows multi-hot where the actual count of each word is listed in the vector representation.\n",
    " \n",
    " $\n",
    " \\begin{align}\n",
    " Vocab &= \\begin{vmatrix} A & He & Is & When & Never & to & Means ...\\end{vmatrix} \\\\\n",
    " rep &= \\begin{vmatrix} 0 & 2 & 0 & 1 & 0 & 1 & 1 & ... \\end{vmatrix}\n",
    " \\end{align}\n",
    " $\n",
    " \n",
    " ### n-gram \n",
    " \n",
    " > 2-gram in this case\n",
    " > Similar to bag-of-words approach but take two words at a time with one word overlapping.\n",
    "\n",
    "For instance, the represetnation of _He arrives precisely when he mans to_ in a contrived vocabulary is as follows. \n",
    " - _Note that never late ends the previous sentence hence no overlap into the next 2-gram_. \n",
    " - only _he arrives_ has an encoding in the 2-gram vocab.\n",
    " \n",
    " $\n",
    " \\begin{align}\n",
    " Vocab &= \\begin{vmatrix} A \\ wizard & wizard\\ is & is\\ never& never\\ late &  he\\ arrives & ...\\end{vmatrix} \\\\\n",
    " rep &= \\begin{vmatrix} 0 & 0 & 0 & 0 & 1 & ... \\end{vmatrix}\n",
    " \\end{align}\n",
    " $\n",
    "\n",
    "# What are we embedding into ?\n",
    "\n",
    "Given the above approachs ranging from _one hot_ to _n-grams_, we are starting out with a large vocabulary and hence a large vector space for the representation.\n",
    "\n",
    "We want to **embed** these vectors from such a large space into a *lower dimensional vector space*. This embedding has multiple goals\n",
    " - We achieve a dense represenation which is space/memory efficient. i.e., Vector components are not mostly zero\n",
    " - We find a lower-dimension space to embed in. i.e., the dimensionality of the embedding space is less than the size of the vocabulary.\n",
    " - vector distance in the embedded space reflect _similarity_ of words.\n",
    " \n",
    "\n",
    "## What does similarity mean ?\n",
    "\n",
    " Similarity in the NLP context means\n",
    "  - `Contextual similarity` - $w_1$ is closer to $w_2$ if they occur close together in multiple texts.\n",
    "  - `Semantically similar` - $w_1$ is closer to $w_2$ if it means similar things. Eg. `king` and `queen` are as close as `man` and `woman`.\n",
    "  \n",
    "Similarity in a vector space is reflected in vector distances. Closer means more similar and so on.  \n",
    "  \n",
    "Looks like this is a design feature as well. Certain word embeddings can be designed for contextual similarity of a certain kind. For instance. Given word embeddings of $V_{queen}$, $V_{king}$, $V_{man}$ & $V_{woman}$, we could expect that\n",
    "\n",
    "$ V_{Queen} = V_{king} + (V_{man} - V_{woman})$\n",
    "\n",
    "# How are the embeddings created\n",
    "\n",
    "A high level description that doesn't make much sense to me right now is that this is done via a neural training\n",
    " - Words from vocabulary is input\n",
    " - frst layer is the embedding layer which embeds these into a lower vector space\n",
    "   - $E_{}\n",
    " - remaining layers are the model itself\n",
    " - trough many cycles of backprop of the model itsef, the embedding layer is also fine tuned and finally yields the embedding as the weights/parameters of the embedding layer.\n",
    "\n",
    "There are some details related to how things are trained\n",
    " - loss functions\n",
    " - words as tokens but then trouble dealing with new unseen words\n",
    " - word fragments as tokens (Fasttext), better at dealing with new unseen words\n",
    " - global contexts (Glove)\n",
    "\n",
    "there are some videos that talk about how to actually code the embedding layer in pytorch, update the embedding in the backprop step and so on. Right now, not sure that adds any value to my hunt for solutioning my problem\n",
    " - TODO [How to train word embeddings using the WikiText2 dataset in pytorch](https://www.youtube.com/watch?v=N-WfUrdgdFw)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa234711-2f80-43de-b6ae-5583dd3ba1b9",
   "metadata": {},
   "source": [
    "# Using word embeddings in 2023\n",
    "\n",
    "This video, [A complete overview of word embeddings](https://www.youtube.com/watch?v=5MaWmXwxFNQ) has a very valuable section which shows how some pre-trained word embeddings can be used.\n",
    " - You can train your own.\n",
    "   - Needs a lot of data\n",
    "   - Needs a lot of time and compute resources\n",
    " - Use released embeddings as-is\n",
    " - Use released embeddings and fine-tune on your own data\n",
    "\n",
    "> Needs `conda install gensim`. I don't feel like `!pip install --upgrade gensim` as using `pip` blindly has broken jupyter for me in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ae34ea-9174-40b6-b144-bb4b03ebcf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "info=api.info()\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + \"...\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d42447-c545-4715-85e9-97e7d5dd247c",
   "metadata": {},
   "source": [
    "## Gensim embedding models\n",
    "\n",
    "The block above shows me a bunch of embedding models\n",
    "\n",
    " - `conceptnet-numberbatch-17-06-300` (1917247 records): ConceptNet Numberbatch consists of state...\n",
    " - `fasttext-wiki-news-subwords-300` (999999 records): 1 million word vectors trained on Wikipe... - \n",
    " - `glove-twitter-100` (1193514 records): Pre-trained vectors based on  2B tweets,.. - .\n",
    " - `glove-twitter-200` (1193514 records): Pre-trained vectors based on 2B tweets, . - ..\n",
    " - `glove-twitter-25` (1193514 records): Pre-trained vectors based on 2B tweets,  - ...\n",
    " - `glove-twitter-50` (1193514 records): Pre-trained vectors based on 2B tweets, -  ...\n",
    " - `glove-wiki-gigaword-100` (400000 records): Pre-trained vectors based on Wikipedia -  2...\n",
    " - `glove-wiki-gigaword-200` (400000 records): Pre-trained vectors based on Wikipedi - a 2...\n",
    " - `glove-wiki-gigaword-300` (400000 records): Pre-trained vectors based on Wikiped - ia 2...\n",
    " - `glove-wiki-gigaword-50` (400000 records): Pre-trained vectors based on Wikipe - dia 2...\n",
    " - `word2vec-google-news-300` (3000000 records): Pre-trained vectors trained on a p - art of...\n",
    " - `word2vec-ruscorpora-300` (184973 records): Word2vec Continuous Skipgram vec\n",
    "\n",
    "Three groups\n",
    " - glove\n",
    " - fasttext\n",
    " - word2vec\n",
    "\n",
    "## Using word2vec, glove and fasttext\n",
    "\n",
    "Using embeddings in general. If you look at TODO, naive embeddings are simply a large table which maps each word into an embedding vector of some fixed length. So I'd think the API is a simple layer on top of a lookup table. The table is the trained magic here.\n",
    "\n",
    "The three models are downloaded in the cell below. Note that they total 2.5GB so only download em if you want to pay with em. Maybe download the smaller one for testing out the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b861e09-edee-4fc4-9317-483aa876ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very beefy downloads. \n",
    "# Download size = 1.6GB\n",
    "# Load time = \n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#glove-twitter-50 is 200MB\n",
    "# Download Size = 200MB\n",
    "# Load time     = 25s\n",
    "glove = api.load('glove-twitter-50')\n",
    "\n",
    "# fasttext-wiki-news-subwords-300 is 1GB\n",
    "# Download Size = 1GB\n",
    "# Loadtime = 125 s\n",
    "fasttext = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e49988-e109-468c-ad2b-86a05ad4f3d2",
   "metadata": {},
   "source": [
    "Functions for *glove*. The others are similar. There are a lot of these!\n",
    "\n",
    " - `add_lifecycle_event`\n",
    " - `add_vector`\n",
    " - `add_vectors`\n",
    " - `allocate_vecattrs`\n",
    " - `closer_than`\n",
    " - `cosine_similarities`\n",
    " - `distance`\n",
    " - `distances`\n",
    " - `doesnt_match`\n",
    " - `evaluate_word_analogies`\n",
    " - `evaluate_word_pairs`\n",
    " - `expandos`\n",
    " - `fill_norms`\n",
    " - `get_index`\n",
    " - `get_mean_vector`\n",
    " - `get_normed_vectors`\n",
    " - `get_vecattr`\n",
    " - `get_vector`\n",
    " - `has_index_for`\n",
    " - `index2entity`\n",
    " - `index2word`\n",
    " - `index_to_key`\n",
    " - `init_sims`\n",
    " - `intersect_word2vec_format`\n",
    " - `key_to_index`\n",
    " - `lifecycle_events`\n",
    " - `load`\n",
    " - `load_word2vec_format`\n",
    " - `log_accuracy`\n",
    " - `log_evaluate_word_pairs`\n",
    " - `mapfile_path`\n",
    " - `most_similar`\n",
    " - `most_similar_cosmul`\n",
    " - `most_similar_to_given`\n",
    " - `n_similarity`\n",
    " - `next_index`\n",
    " - `norms`\n",
    " - `rank`\n",
    " - `rank_by_centrality`\n",
    " - `relative_cosine_similarity`\n",
    " - `resize_vectors`\n",
    " - `save`\n",
    " - `save_word2vec_format`\n",
    " - `set_vecattr`\n",
    " - `similar_by_key`\n",
    " - `similar_by_vector`\n",
    " - `similar_by_word`\n",
    " - `similarity`\n",
    " - `similarity_unseen_docs`\n",
    " - `sort_by_descending_frequency`\n",
    " - `unit_normalize_all`\n",
    " - `vector_size`\n",
    " - `vectors`\n",
    " - `vectors_for_all`\n",
    " - `vectors_norm`\n",
    " - `vocab`\n",
    " - `wmdistance`\n",
    " - `word_vec`\n",
    " - `words_closer_than`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1127b11d-428d-40eb-8481-e459a295c46a",
   "metadata": {},
   "source": [
    "### Most similar to a given word\n",
    "\n",
    "Returns semantically similar (distance in the embedding space) to the given word and sorts by similarity. Fasttext seems to be the best followeb by glove with word2vec looking the least precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4a63487-ca9f-4c86-a707-452e37ac3a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Similarity from word2vec"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " Similar Word | Similarity Metric\n",
       "  --          | --\n",
       "  Tea | 0.7009037137031555\n",
       "teas | 0.6727380156517029\n",
       "shape_Angius | 0.6323482394218445\n",
       "activist_Jamie_Radtke | 0.5863860845565796\n",
       "decaffeinated_brew | 0.5839535593986511\n",
       "planter_bungalow | 0.575829029083252\n",
       "herbal_tea | 0.5731174349784851\n",
       "coffee | 0.5635291337966919\n",
       "jasmine_tea | 0.548339307308197\n",
       "Tea_NASDAQ_PEET | 0.5402543544769287\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Similarity from fasttext"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " Similar Word | Similarity Metric\n",
       "  --          | --\n",
       "  tea- | 0.7728265523910522\n",
       "coffee | 0.7583760619163513\n",
       "teas | 0.731768786907196\n",
       "cuppa | 0.7301388382911682\n",
       "teabags | 0.6973742246627808\n",
       "Tea | 0.6826096773147583\n",
       "tea-drinking | 0.6748528480529785\n",
       "teabag | 0.6707128882408142\n",
       "tea-making | 0.6683591604232788\n",
       "tea-bags | 0.6638833284378052\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Similarity from glove"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " Similar Word | Similarity Metric\n",
       "  --          | --\n",
       "  coffee | 0.8929038643836975\n",
       "milk | 0.8667818903923035\n",
       "wine | 0.8507667183876038\n",
       "cream | 0.8502466678619385\n",
       "ice | 0.8362609148025513\n",
       "juice | 0.8177550435066223\n",
       "beer | 0.8157102465629578\n",
       "sugar | 0.8099128007888794\n",
       "cake | 0.8080540895462036\n",
       "drink | 0.8000376224517822\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Print results out as markdown.\n",
    "from string import Template\n",
    "mdT = Template(\"\"\"\n",
    " Similar Word | Similarity Metric\n",
    "  --          | --\n",
    "  $lines\n",
    "\"\"\")\n",
    "\n",
    "def printSimilarityTable(ret):    \n",
    "    display(Markdown(\n",
    "        mdT.substitute(\n",
    "            lines = \"\\n\".join(map( lambda x: f\"{x[0]} | {x[1]}\", ret))\n",
    "      )\n",
    "    ))        \n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"#### Similarity from word2vec\"))\n",
    "printSimilarityTable(wv.most_similar(\"tea\"))\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"#### Similarity from fasttext\"))\n",
    "printSimilarityTable(fasttext.most_similar(\"tea\"))\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"#### Similarity from glove\"))\n",
    "printSimilarityTable(glove.most_similar(\"tea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5190d7c3-ca89-4f63-b20b-c8839e3ea59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Distance between `tea` and `coffee`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - Word2vec : 0.43647074699401855 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - Glove : 0.10709607601165771 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - Fasttext : 0.2416238784790039 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"Distance between `tea` and `coffee`\"))\n",
    "display(Markdown(f' - Word2vec : {wv.distance(\"tea\", \"coffee\")} '))\n",
    "display(Markdown(f' - Glove : {glove.distance(\"tea\", \"coffee\")} '))\n",
    "display(Markdown(f' - Fasttext : {fasttext.distance(\"tea\", \"coffee\")} '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "806cdb35-ce4b-4685-b453-d61c35c49f30",
   "metadata": {},
   "source": [
    "### Vector math in embedded space (analogy making)\n",
    "\n",
    "This is essentially doing $King - Man + Woman$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ac85404-c449-4d14-bd74-630bfe6c473d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9314123392105103),\n",
       " ('monarch', 0.858533501625061),\n",
       " ('princess', 0.8476566672325134),\n",
       " ('Queen_Consort', 0.8150269389152527),\n",
       " ('queens', 0.8099815249443054),\n",
       " ('crown_prince', 0.8089976906776428),\n",
       " ('royal_palace', 0.8027306795120239),\n",
       " ('monarchy', 0.8019613027572632),\n",
       " ('prince', 0.800979733467102),\n",
       " ('empress', 0.7958388328552246)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d547f85-3926-4ada-a471-47d7217a671f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
