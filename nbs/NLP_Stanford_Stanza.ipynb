{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Core NLP Products\n",
    "\n",
    "I was introduced to this via the [Understanding language Syntax and structure - A practitioners guide](https://www.kdnuggets.com/2018/08/understanding-language-syntax-and-structure-practitioners-guide-nlp-3.html). My [NLP_2018_ProcessingAndUnderstandingText.ipynb notebook](./NLP_2018_ProcessingAndUnderstandingText.ipynb) explores the code and updates it for 2023.\n",
    "\n",
    "One of the confusing things about StanfordNLP is that the StanfordNLP group authors toolkits with varying limitations\n",
    " - Stanford CoreNLP - _The original java codebase which is offered as a server or used as a jar_. While more complete, this seems to be a _cpu-only_ solution so should be quite slow.\n",
    " - StanfordNLP - The python NLP library which has pytorch based _(so much faster)_ `neural pipeline` and provides an client interface to the `CoreNLP`\n",
    " - Stanza - Replaces `StanfordNLP`python library.\n",
    "   - dropped it in 2022 after some analysis as it did not have any CoRef modules. Coref needed calling CoreNLP's java server\n",
    "   - Thought I'd switch to Spacy since Spacy had HuggingFace's neural CoRef\n",
    "   - After analyzing Amazon Research's TANL, Oct 2024, turns out Stanza has been updated quite a bit and now has a CoRef release. Revisiting this\n",
    "\n",
    "## Resources\n",
    " - [Stanza Overview](https://stanfordnlp.github.io/stanza/index.html)\n",
    " - [Stanza Beginners guide notebook](https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_Beginners_Guide.ipynb)\n",
    " - [Universal Dependencies](https://universaldependencies.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_js_diagrammers\n",
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output for /home/vamsi/bitbucket/hillops/nbs/NLP/17ef1890-1688-4625-bbc4-2139557c7a2c.uml to 17ef1890-1688-4625-bbc4-2139557c7a2c.svg\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" contentStyleType=\"text/css\" height=\"1029px\" preserveAspectRatio=\"none\" style=\"width:778px;height:1029px;background:#FFFFFF;\" version=\"1.1\" viewBox=\"0 0 778 1029\" width=\"778px\" zoomAndPan=\"magnify\"><defs/><g><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"111.1299\" x=\"10\" y=\"495.5625\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"91.1299\" x=\"20\" y=\"518.5576\">Stanford NLP</text><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"67.7559\" x=\"171.1299\" y=\"338.5234\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"47.7559\" x=\"181.1299\" y=\"361.5186\">Stanza</text><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"197.5293\" x=\"288.8857\" y=\"20\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"177.5293\" x=\"298.8857\" y=\"42.9951\">Neural pipeline and Cuda</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,38.1484 278.8857,38.1484 L288.8857,38.1484 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"49.0459\" x=\"288.8857\" y=\"76.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"29.0459\" x=\"298.8857\" y=\"99.292\">NER</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,94.4453 278.8857,94.4453 L288.8857,94.4453 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"126.6406\" x=\"288.8857\" y=\"132.5938\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"106.6406\" x=\"298.8857\" y=\"155.5889\">POS (treebank)</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,150.7422 278.8857,150.7422 L288.8857,150.7422 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"297.3545\" x=\"288.8857\" y=\"188.8906\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"277.3545\" x=\"298.8857\" y=\"211.8857\">Universal Dependencies (enhanced++)</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,207.0391 278.8857,207.0391 L288.8857,207.0391 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"154.8525\" x=\"288.8857\" y=\"245.1875\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"134.8525\" x=\"298.8857\" y=\"268.1826\">Constituency Parse</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,263.3359 278.8857,263.3359 L288.8857,263.3359 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"52.5938\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"343.5996\" x=\"288.8857\" y=\"301.4844\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4.4502\" x=\"298.8857\" y=\"324.4795\"> </text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" text-decoration=\"line-through\" textLength=\"8.4014\" x=\"303.3359\" y=\"324.4795\">❌</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" text-decoration=\"line-through\" textLength=\"72.4678\" x=\"320.6377\" y=\"324.4795\">No CoRef</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" text-decoration=\"line-through\" textLength=\"211.1211\" x=\"397.5557\" y=\"324.4795\">processor. Needs client call to</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4.4502\" x=\"298.8857\" y=\"340.7764\"> </text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" text-decoration=\"line-through\" textLength=\"319.1494\" x=\"303.3359\" y=\"340.7764\">CoreNLP which has cpu coref implementation</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,327.7813 278.8857,327.7813 L288.8857,327.7813 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#90EE90\" height=\"134.0781\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"411.0771\" x=\"288.8857\" y=\"374.0781\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"276.7256\" x=\"303.3359\" y=\"397.0732\">✔️ 2024 Seems to have CoRef. Evaluate</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"200.7988\" x=\"298.8857\" y=\"413.3701\">Yay! Seems to be part of 1.7</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"323.9551\" x=\"298.8857\" y=\"429.667\">https://stanfordnlp.github.io/stanza/coref.html</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"96.3389\" x=\"303.3359\" y=\"445.9639\">- Only english</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"159.3184\" x=\"303.3359\" y=\"462.2607\">- Trained on ontonotes</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"159.6191\" x=\"303.3359\" y=\"478.5576\">- Based on 2023 paper</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"377.7266\" x=\"312.2363\" y=\"494.8545\">Conjunction Aware word-level Coreference Resolution</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,441.1172 278.8857,441.1172 L288.8857,441.1172 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#90EE90\" height=\"52.5938\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"272.6426\" x=\"288.8857\" y=\"528.1563\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"196.5879\" x=\"298.8857\" y=\"551.1514\">Uses brat and Dagre for viz.</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"252.6426\" x=\"298.8857\" y=\"567.4482\">See stanza/pipeline/demo on github</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,554.4531 278.8857,554.4531 L288.8857,554.4531 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"413.1963\" x=\"288.8857\" y=\"600.75\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" text-decoration=\"line-through\" textLength=\"388.7461\" x=\"298.8857\" y=\"623.7451\">Use spacy or hugginface coref with a custom processor</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4.4502\" x=\"687.6318\" y=\"623.7451\"> </text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,618.8984 278.8857,618.8984 L288.8857,618.8984 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"477.5771\" x=\"288.8857\" y=\"657.0469\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"457.5771\" x=\"298.8857\" y=\"680.042\">✔️ Uses a very comprehensice visualizer that I think I can borrow.</text><path d=\"M238.8857,356.6719 L248.8857,356.6719 C263.8857,356.6719 263.8857,675.1953 278.8857,675.1953 L288.8857,675.1953 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M121.1299,513.7109 L131.1299,513.7109 C146.1299,513.7109 146.1299,356.6719 161.1299,356.6719 L171.1299,356.6719 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"79.4248\" x=\"171.1299\" y=\"741.4922\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"59.4248\" x=\"181.1299\" y=\"764.4873\">CoreNLP</text><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"137.0723\" x=\"300.5547\" y=\"713.3438\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"117.0723\" x=\"310.5547\" y=\"736.3389\">❌ Slow, CPU only</text><path d=\"M250.5547,759.6406 L260.5547,759.6406 C275.5547,759.6406 275.5547,731.4922 290.5547,731.4922 L300.5547,731.4922 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"242.6396\" x=\"300.5547\" y=\"769.6406\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"218.1895\" x=\"310.5547\" y=\"792.6357\">✔️ Has Coref, dcoref processors</text><path d=\"M250.5547,759.6406 L260.5547,759.6406 C275.5547,759.6406 275.5547,787.7891 290.5547,787.7891 L300.5547,787.7891 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M121.1299,513.7109 L131.1299,513.7109 C146.1299,513.7109 146.1299,759.6406 161.1299,759.6406 L171.1299,759.6406 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#90EE90\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"78.5156\" x=\"171.1299\" y=\"898.5313\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"58.5156\" x=\"181.1299\" y=\"921.5264\">✔️ Spacy</text><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"111.3076\" x=\"299.6455\" y=\"825.9375\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"91.3076\" x=\"309.6455\" y=\"848.9326\">✔️ Python API</text><path d=\"M249.6455,916.6797 L259.6455,916.6797 C274.6455,916.6797 274.6455,844.0859 289.6455,844.0859 L299.6455,844.0859 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"131.0088\" x=\"299.6455\" y=\"882.2344\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"111.0088\" x=\"309.6455\" y=\"905.2295\">✔️ cuda enabled</text><path d=\"M249.6455,916.6797 L259.6455,916.6797 C274.6455,916.6797 274.6455,900.3828 289.6455,900.3828 L299.6455,900.3828 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"68.8906\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"312.1543\" x=\"299.6455\" y=\"938.5313\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"166.332\" x=\"314.0957\" y=\"961.5264\">Huggingface developed</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"292.1543\" x=\"309.6455\" y=\"977.8232\">neural coref model based on embeddings</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"256.8125\" x=\"309.6455\" y=\"994.1201\">with full training method on medium</text><path d=\"M249.6455,916.6797 L259.6455,916.6797 C274.6455,916.6797 274.6455,972.9766 289.6455,972.9766 L299.6455,972.9766 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M121.1299,513.7109 L131.1299,513.7109 C146.1299,513.7109 146.1299,916.6797 161.1299,916.6797 L171.1299,916.6797 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><!--SRC=[RLDBRXD14DttAGgnIKncma-0d0tWX4_8nuen4a8iQcRACqrwgaVziUMS0OaDIxW7vya5k0BLCpO94bAaYQlhy-gzLyzzG1TQpLMBdHh1B20lhAlWx6Igr6W8NA7yDu8pYWuDTBeZeva0kO99h3QFHnVzTte-WvtWY0haozq-D6Uz9EUbz90wuegurEHXXxX1BgcwTs_8crZsGOSexskOelEKmcF8ikkVNm56epChEHUqaDpEsPAyjouNM5HvA8qc3b2YCH1ibYafcrX7iWUi6bqsqA1aTH5AYIv0jvsXLaemQCjPTf1cVRfhTDs4Mk3pvp5S__Z- -zStU7Z_uMEO4RLUEakR9GqmSZXQeeaO2DH7ND-voIa8Eg4Lx08Uv4zL4qBdn_lxVaCkcowlTMXYaMlRHwzmluULDw4r0XtEsQo1k3RQD-dtUuT2UGMMvIzOje5yYhz4FqG5v2CPsf431P2u_1AvJBl1YnKwWfKCpWmjoK3YPK5EU2Qu86zDJ7d_OM3kHQV2OUYrFaG9Wom0ItsLWv9rOGD-wudzYbgRu0ph7QXUs2oJHk0x55sbkebrhNc1CdtGOYN9W5167snx8ssMGL- -aQ47Wo1MMajTspbgYBsMBalj8nfz9Rk7Hi2-bO_cI_cMo51Ov-mgxzsySSJGLamrCtQr1vFfN32RzUrfRyGjFKzxK0qe_-Boa7hzmzIsQfPslDrckWvDac3wzdQqbAiHTR4mL0qUVzDpKlUaL4aasyaJ3nStp6zj9TeLMy6fBQYgfCQhdhz5JCvFDf4Oj2Hp-xoMAXtR0_LSBczxvsfgaCFyz2InwPD77kJ9vVcpWW8-sfdp9TiLzpHhGn1qIxjgv_Nq1BoDBc7KFZXTzCRPLSUuHBY8dFB6S2v7VdpuRXk08rvgPpaTcph-S7gJyEHnzb87EHidCE33gJga1KOJfAAqQRKnpD-_ofwf4-GwOYszYTN4Ib-tbhVP7m00]--></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "\n",
    "@startmindmap\n",
    "* Stanford NLP\n",
    "\n",
    "** Stanza\n",
    "*** Neural pipeline and Cuda\n",
    "*** NER\n",
    "*** POS (treebank)\n",
    "*** Universal Dependencies (enhanced++)\n",
    "*** Constituency Parse\n",
    "***: --❌  **No CoRef** processor. Needs client call to--\n",
    " --CoreNLP which has cpu coref implementation--;\n",
    "***[#lightgreen]: ✔️ 2024 Seems to have CoRef. Evaluate \n",
    "Yay! Seems to be part of 1.7\n",
    "https://stanfordnlp.github.io/stanza/coref.html\n",
    " - Only english\n",
    " - Trained on ontonotes\n",
    " - Based on 2023 paper \n",
    "   Conjunction Aware word-level Coreference Resolution;\n",
    "***[#lightgreen]:Uses brat and Dagre for viz. \n",
    "See stanza/pipeline/demo on github;\n",
    "\n",
    "*** --Use spacy or hugginface coref with a custom processor-- \n",
    "*** ✔️ Uses a very comprehensice visualizer that I think I can borrow.\n",
    "\n",
    "** CoreNLP\n",
    "*** ❌ Slow, CPU only\n",
    "*** ✔️ Has Coref, dcoref processors \n",
    "\n",
    "**[#lightgreen] ✔️ Spacy\n",
    "*** ✔️ Python API\n",
    "*** ✔️ cuda enabled\n",
    "***: Huggingface developed\n",
    "neural coref model based on embeddings\n",
    "with full training method on medium;\n",
    "@endmindmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not clear yet about all the NLP needs I have. To animate a story, I certainly need\n",
    " - NER\n",
    " - POS\n",
    " - Enhanced++ Universal dependencies\n",
    " - _maybe_ Constituency Parsing\n",
    " - Coref Resolution\n",
    "\n",
    " ~~While CoRef resolution is slow on the CoreNLP side, it is there and a combination of `Stanza` for most things and `CoreNLP` for coref should be a usable starting point~~ With CoRef in Oct 2024, Stanza should be a fairly complete NLP solution. With UniversalDependencies, it is also a more complete set.\n",
    "\n",
    "On the other hand, if `SpaCy` has everything I need and has a neural coref model, it should be fast and also an option.\n",
    "\n",
    "## Stanza-coref\n",
    "\n",
    "The coref processor is listed in the code but has not made it into their tutorials as of Oct 2024. \n",
    " - Based on a 2023 paper called [CAW-Coref](https://arxiv.org/pdf/2310.06165) which itself fixes a small issue with a 2021 paper [World level Coreference Resolution - Dobrovolskii](https://aclanthology.org/2021.emnlp-main.605.pdf)\n",
    "   - Single pass on 355M RoBerta-large. 80.7% F1 on Ontonotes\n",
    " - Sota seems to be the 2023 paper [Coreference Resolution through a seq2seq Transition-Based System - Bohnet - Google](https://arxiv.org/pdf/2211.12142)\n",
    "   - Multiple passes on a 13B T5-XXL\n",
    "   - 83.3% F1 on Ontonotes\n",
    "   - seq2seq seems TANL adjacent. Need to check out TANL performance in CoRef on ontonotes. _(TANL does not use ontonotes)_\n",
    " - Achieves 96% F1 of current SOTA with much more efficiency\n",
    "\n",
    "# Stanza\n",
    "\n",
    "## Installation\n",
    "```console\n",
    "mamba install -c stanfordnlp stanza\n",
    "mamba install peft\n",
    "mamba install nltk\n",
    "```\n",
    "\n",
    " - `peft` is needed when the coref processos is used since the model is a LORA model on RoBerta\n",
    " - First time use also requires the download of the language models needed using the following code.\n",
    "\n",
    "```python\n",
    "import stanza\n",
    "stanza.download('en')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import stanza\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza english resources and models are already downlaoded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "stanza_en_dir = Path(\"~/stanza_resources/en\").expanduser()\n",
    "if not stanza_en_dir.exists():\n",
    "    # one time download of english language model. There are many other models\n",
    "    # These are saved under ~/stanza_resources\n",
    "    #\n",
    "    # There are some 60+ models.\n",
    "    # Check https://stanfordnlp.github.io/stanza/models.html for others.\n",
    "    print(\"Downloading Stanza english resources and models\")\n",
    "    stanza.download('en')\n",
    "else:\n",
    "    print(\"Stanza english resources and models are already downlaoded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tag notation\n",
    "\n",
    "Looks like between 2018 and 2023, the tagging notation also changed from Penn Treebank notation to [Universal Dependencies](https://universaldependencies.org/). This is quite different from Penn Treebank tags since it wants to apply to multiple languages. Some more resources\n",
    " - [MIT article on universal dependencies](https://direct.mit.edu/coli/article/47/2/255/98516/Universal-Dependencies)\n",
    " - Reading some old forum articles, there is some confusion between `Universal Dependencies`, `Dependencies`, `Constituency Parsing` and such.\n",
    "   - 2023 atleast, for each parsed sentence\n",
    "     - `constituency` tree is avaiable. See [Stanza Constituency Parser](https://stanfordnlp.github.io/stanza/constituency.html)\n",
    "     - `dependency` shows the basic set of 37 universal dependencies. There is some extra work needed to get to the `enhanced++` dependencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parse\n",
    "\n",
    "Setup the pipeline and invoke it.\n",
    "\n",
    "### Setting up the pipeline\n",
    "\n",
    "The simplest way of setting up the pipeline is to specify only the language: `nlp = stanza.Pipeline('en')`. This uses a default set of processors which are listed in the output. _See the output of the cell below_. These are \n",
    " - tokenize\n",
    " - pos\n",
    " - lemma\n",
    " - constituency\n",
    " - depparse\n",
    " - sentiment\n",
    " - ner\n",
    "\n",
    " To specify the list of processors, see [stanza processor availability](https://stanfordnlp.github.io/stanza/pipeline.html#processors), keep in mind that some processors need others and such dependencies should be listed earlier and invoke in this fashion\n",
    "\n",
    " `nlp = stanza.Pipeline('de', processors='tokenize,mwt', package='gsd')`\n",
    "\n",
    " See [Stanza Getting Started](https://stanfordnlp.github.io/stanza/getting_started.html) for more info.\n",
    "\n",
    "### Annotating the document\n",
    "\n",
    "Annotation is as simple as simply passing a string into the pipeline. This runs all the specified _(or default)_ processors on the input and returns annotation information in the returned structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:05:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7574b29a5846858935f5a48524f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:05:15 INFO: Downloaded file to /home/vamsi/stanza_resources/resources.json\n",
      "2024-10-09 21:05:16 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-09 21:05:16 INFO: Using device: cuda\n",
      "2024-10-09 21:05:16 INFO: Loading: tokenize\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:17 INFO: Loading: mwt\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:17 INFO: Loading: pos\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:17 INFO: Loading: lemma\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:17 INFO: Loading: constituency\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:18 INFO: Loading: depparse\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:18 INFO: Loading: sentiment\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:18 INFO: Loading: ner\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:05:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', 5, 'nsubj')\n",
      "('Obama', 1, 'flat')\n",
      "('is', 5, 'cop')\n",
      "('the', 5, 'det')\n",
      "('president', 0, 'root')\n",
      "('of', 9, 'case')\n",
      "('the', 9, 'det')\n",
      "('United', 9, 'amod')\n",
      "('States', 5, 'nmod')\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza_nlp = stanza.Pipeline('en')  # Setup a neural pipeline in english\n",
    "doc = stanza_nlp(\"Barack Obama is the president of the United States\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza API study\n",
    "\n",
    "Some notable methods from the Help on the sentence object. Sentence is obtained by `doc.sentences[0]`\n",
    " - print_dependencies()\n",
    " - print_tokens()\n",
    " - print_words()\n",
    " - to_dict()\n",
    " - constituency\n",
    " - dependencies\n",
    " - entities/ents\n",
    " - sentiment\n",
    " \n",
    "\n",
    "## Constituency Parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNP Barack) (NNP Obama))\n",
      "    (VP\n",
      "      (VBZ is)\n",
      "      (NP\n",
      "        (NP (DT the) (NN president))\n",
      "        (PP\n",
      "          (IN of)\n",
      "          (NP (DT the) (NNP United) (NNPS States)))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can print out the constituency graph. Phrase structure Parser\n",
    "print(doc.sentences[0].constituency.pretty_print())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out entire parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"deprel\": \"nsubj\",\n",
      "        \"end_char\": 6,\n",
      "        \"feats\": \"Number=Sing\",\n",
      "        \"head\": 5,\n",
      "        \"id\": 1,\n",
      "        \"lemma\": \"Barack\",\n",
      "        \"multi_ner\": [\n",
      "            \"B-PERSON\"\n",
      "        ],\n",
      "        \"ner\": \"B-PERSON\",\n",
      "        \"start_char\": 0,\n",
      "        \"text\": \"Barack\",\n",
      "        \"upos\": \"PROPN\",\n",
      "        \"xpos\": \"NNP\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"flat\",\n",
      "        \"end_char\": 12,\n",
      "        \"feats\": \"Number=Sing\",\n",
      "        \"head\": 1,\n",
      "        \"id\": 2,\n",
      "        \"lemma\": \"Obama\",\n",
      "        \"multi_ner\": [\n",
      "            \"E-PERSON\"\n",
      "        ],\n",
      "        \"ner\": \"E-PERSON\",\n",
      "        \"start_char\": 7,\n",
      "        \"text\": \"Obama\",\n",
      "        \"upos\": \"PROPN\",\n",
      "        \"xpos\": \"NNP\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"cop\",\n",
      "        \"end_char\": 15,\n",
      "        \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "        \"head\": 5,\n",
      "        \"id\": 3,\n",
      "        \"lemma\": \"be\",\n",
      "        \"multi_ner\": [\n",
      "            \"O\"\n",
      "        ],\n",
      "        \"ner\": \"O\",\n",
      "        \"start_char\": 13,\n",
      "        \"text\": \"is\",\n",
      "        \"upos\": \"AUX\",\n",
      "        \"xpos\": \"VBZ\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"det\",\n",
      "        \"end_char\": 19,\n",
      "        \"feats\": \"Definite=Def|PronType=Art\",\n",
      "        \"head\": 5,\n",
      "        \"id\": 4,\n",
      "        \"lemma\": \"the\",\n",
      "        \"multi_ner\": [\n",
      "            \"O\"\n",
      "        ],\n",
      "        \"ner\": \"O\",\n",
      "        \"start_char\": 16,\n",
      "        \"text\": \"the\",\n",
      "        \"upos\": \"DET\",\n",
      "        \"xpos\": \"DT\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"root\",\n",
      "        \"end_char\": 29,\n",
      "        \"feats\": \"Number=Sing\",\n",
      "        \"head\": 0,\n",
      "        \"id\": 5,\n",
      "        \"lemma\": \"president\",\n",
      "        \"multi_ner\": [\n",
      "            \"O\"\n",
      "        ],\n",
      "        \"ner\": \"O\",\n",
      "        \"start_char\": 20,\n",
      "        \"text\": \"president\",\n",
      "        \"upos\": \"NOUN\",\n",
      "        \"xpos\": \"NN\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"case\",\n",
      "        \"end_char\": 32,\n",
      "        \"head\": 9,\n",
      "        \"id\": 6,\n",
      "        \"lemma\": \"of\",\n",
      "        \"multi_ner\": [\n",
      "            \"O\"\n",
      "        ],\n",
      "        \"ner\": \"O\",\n",
      "        \"start_char\": 30,\n",
      "        \"text\": \"of\",\n",
      "        \"upos\": \"ADP\",\n",
      "        \"xpos\": \"IN\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"det\",\n",
      "        \"end_char\": 36,\n",
      "        \"feats\": \"Definite=Def|PronType=Art\",\n",
      "        \"head\": 9,\n",
      "        \"id\": 7,\n",
      "        \"lemma\": \"the\",\n",
      "        \"multi_ner\": [\n",
      "            \"B-GPE\"\n",
      "        ],\n",
      "        \"ner\": \"B-GPE\",\n",
      "        \"start_char\": 33,\n",
      "        \"text\": \"the\",\n",
      "        \"upos\": \"DET\",\n",
      "        \"xpos\": \"DT\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"amod\",\n",
      "        \"end_char\": 43,\n",
      "        \"feats\": \"Tense=Past|VerbForm=Part\",\n",
      "        \"head\": 9,\n",
      "        \"id\": 8,\n",
      "        \"lemma\": \"Unite\",\n",
      "        \"multi_ner\": [\n",
      "            \"I-GPE\"\n",
      "        ],\n",
      "        \"ner\": \"I-GPE\",\n",
      "        \"start_char\": 37,\n",
      "        \"text\": \"United\",\n",
      "        \"upos\": \"VERB\",\n",
      "        \"xpos\": \"NNP\"\n",
      "    },\n",
      "    {\n",
      "        \"deprel\": \"nmod\",\n",
      "        \"end_char\": 50,\n",
      "        \"feats\": \"Number=Plur\",\n",
      "        \"head\": 5,\n",
      "        \"id\": 9,\n",
      "        \"lemma\": \"State\",\n",
      "        \"multi_ner\": [\n",
      "            \"E-GPE\"\n",
      "        ],\n",
      "        \"ner\": \"E-GPE\",\n",
      "        \"start_char\": 44,\n",
      "        \"text\": \"States\",\n",
      "        \"upos\": \"PROPN\",\n",
      "        \"xpos\": \"NNPS\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Entire parse as a dict\n",
    "# UPOS   : Universal POS tag\n",
    "# XPOS   : Treebank-specific tag\n",
    "# UFeats : Universal morphological features\n",
    "# head   : syntactic head\n",
    "# deprel : \n",
    "import json\n",
    "print(\n",
    "    json.dumps(doc.sentences[0].to_dict(), sort_keys=True, indent=4)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print sentiment analysis annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 0 - Negative\n",
    "# 1 - Neutral\n",
    "# 2 - Positive\n",
    "print(\n",
    "    doc.sentences[0].sentiment\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the constituency tree\n",
    "\n",
    "The toolslisted in Dipanjan's articles use `nltk` tree display. However, it is fairly easy to transform the stanford tree into nltk tree for display. Note that stanza has one or more visualizers as part of their demo server. No idea how to get them into jupyter tho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,488.0,360.0\" width=\"488px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ROOT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"24.5902%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Barack</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Obama</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.2951%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.4098%\" x=\"24.5902%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"10.8696%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.43478%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"89.1304%\" x=\"10.8696%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"39.0244%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68.75%\" x=\"31.25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">president</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.625%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.5122%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"60.9756%\" x=\"39.0244%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"16%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"84%\" x=\"16%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"23.8095%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.9048%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"23.8095%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">United</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"61.9048%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNPS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">States</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.9524%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.5122%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.4348%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.2951%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Barack']), Tree('NNP', ['Obama'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['president'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['the']), Tree('NNP', ['United']), Tree('NNPS', ['States'])])])])])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "display(Tree.fromstring(str(doc.sentences[0].constituency)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza Coreference resolution\n",
    "\n",
    "\n",
    "While the docs and tutorials do not reference `coref`, github shows a `coref_processor`. Seems to work.\n",
    "\n",
    "On further digging tho, [spacy coref](https://explosion.ai/blog/coref) has a beautiful long blog post\n",
    " - They use the same Dobsroskii paper\n",
    " - Lot more future direction work.\n",
    " - Existing pipeline ready to use\n",
    "\n",
    "Stanza seems like a research project that is barely supported. Spacy would be much more commercially viable and way better documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:25:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f0d0a2dc3642259ffd4253ede90378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:25:49 INFO: Downloaded file to /home/vamsi/stanza_resources/resources.json\n",
      "2024-10-09 21:25:49 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-10-09 21:25:49 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | combined                 |\n",
      "| mwt       | combined                 |\n",
      "| pos       | combined_charlm          |\n",
      "| coref     | udcoref_xlm-roberta-lora |\n",
      "========================================\n",
      "\n",
      "2024-10-09 21:25:49 INFO: Using device: cuda\n",
      "2024-10-09 21:25:49 INFO: Loading: tokenize\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:25:49 INFO: Loading: mwt\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:25:49 INFO: Loading: pos\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-09 21:25:49 INFO: Loading: coref\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/stanza/models/coref/model.py:301: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dicts = torch.load(path, map_location=map_location)\n",
      "/home/vamsi/mambaforge/envs/ml/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-09 21:25:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Coref dowloads the uccoref LORA on a roberta-large 2.24G in size!\n",
    "import stanza\n",
    "stanza_nlp = stanza.Pipeline('en', processors='tokenize,pos,coref')\n",
    "doc = stanza_nlp(\"Barack Obama is the president of the United States. He is a great guy!\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"Barack\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 0,\n",
      "    \"end_char\": 6,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_start\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"Obama\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 7,\n",
      "    \"end_char\": 12,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_end\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"is\",\n",
      "    \"upos\": \"AUX\",\n",
      "    \"xpos\": \"VBZ\",\n",
      "    \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "    \"start_char\": 13,\n",
      "    \"end_char\": 15,\n",
      "    \"coref_chains\": []\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"the\",\n",
      "    \"upos\": \"DET\",\n",
      "    \"xpos\": \"DT\",\n",
      "    \"feats\": \"Definite=Def|PronType=Art\",\n",
      "    \"start_char\": 16,\n",
      "    \"end_char\": 19,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_start\": true,\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"text\": \"president\",\n",
      "    \"upos\": \"NOUN\",\n",
      "    \"xpos\": \"NN\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 20,\n",
      "    \"end_char\": 29,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"text\": \"of\",\n",
      "    \"upos\": \"ADP\",\n",
      "    \"xpos\": \"IN\",\n",
      "    \"start_char\": 30,\n",
      "    \"end_char\": 32,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 7,\n",
      "    \"text\": \"the\",\n",
      "    \"upos\": \"DET\",\n",
      "    \"xpos\": \"DT\",\n",
      "    \"feats\": \"Definite=Def|PronType=Art\",\n",
      "    \"start_char\": 33,\n",
      "    \"end_char\": 36,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_representative\": true\n",
      "      },\n",
      "      {\n",
      "        \"index\": 1,\n",
      "        \"representative_text\": \"the United States\",\n",
      "        \"is_start\": true,\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 8,\n",
      "    \"text\": \"United\",\n",
      "    \"upos\": \"ADJ\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Degree=Pos\",\n",
      "    \"start_char\": 37,\n",
      "    \"end_char\": 43,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_representative\": true\n",
      "      },\n",
      "      {\n",
      "        \"index\": 1,\n",
      "        \"representative_text\": \"the United States\",\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 9,\n",
      "    \"text\": \"States\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNPS\",\n",
      "    \"feats\": \"Number=Plur\",\n",
      "    \"start_char\": 44,\n",
      "    \"end_char\": 50,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_end\": true,\n",
      "        \"is_representative\": true\n",
      "      },\n",
      "      {\n",
      "        \"index\": 1,\n",
      "        \"representative_text\": \"the United States\",\n",
      "        \"is_end\": true,\n",
      "        \"is_representative\": true\n",
      "      }\n",
      "    ],\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 10,\n",
      "    \"text\": \".\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"xpos\": \".\",\n",
      "    \"start_char\": 50,\n",
      "    \"end_char\": 51,\n",
      "    \"coref_chains\": []\n",
      "  }\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"He\",\n",
      "    \"upos\": \"PRON\",\n",
      "    \"xpos\": \"PRP\",\n",
      "    \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
      "    \"start_char\": 52,\n",
      "    \"end_char\": 54,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_start\": true,\n",
      "        \"is_end\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"is\",\n",
      "    \"upos\": \"AUX\",\n",
      "    \"xpos\": \"VBZ\",\n",
      "    \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "    \"start_char\": 55,\n",
      "    \"end_char\": 57,\n",
      "    \"coref_chains\": []\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"a\",\n",
      "    \"upos\": \"DET\",\n",
      "    \"xpos\": \"DT\",\n",
      "    \"feats\": \"Definite=Ind|PronType=Art\",\n",
      "    \"start_char\": 58,\n",
      "    \"end_char\": 59,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_start\": true\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"great\",\n",
      "    \"upos\": \"ADJ\",\n",
      "    \"xpos\": \"JJ\",\n",
      "    \"feats\": \"Degree=Pos\",\n",
      "    \"start_char\": 60,\n",
      "    \"end_char\": 65,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"text\": \"guy\",\n",
      "    \"upos\": \"NOUN\",\n",
      "    \"xpos\": \"NN\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"start_char\": 66,\n",
      "    \"end_char\": 69,\n",
      "    \"coref_chains\": [\n",
      "      {\n",
      "        \"index\": 0,\n",
      "        \"representative_text\": \"the president of the United States\",\n",
      "        \"is_end\": true\n",
      "      }\n",
      "    ],\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"text\": \"!\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"xpos\": \".\",\n",
      "    \"start_char\": 69,\n",
      "    \"end_char\": 70,\n",
      "    \"coref_chains\": [],\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Without a proper example\n",
    "# I am looking at https://github.com/stanfordnlp/stanza/blob/main/stanza/pipeline/coref_processor.py\n",
    "# doc.coref: List[CorefChain]\n",
    "dir(doc.coref)\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stanza dependencies\n",
    "\n",
    "> Anything beyond the standard dependencies needs Java and the coreNLP package to be installed\n",
    ">\n",
    "> use `stanza.install_corenlp()` which installs it into `~/stanza_corenlp`\n",
    "\n",
    "The Stanza Python API by default only supports `standard Universal Dependencies`. The CoreNLP server product _(java)_ however supports multiple types of universal dependencies (all through the same `depparse` module that is used by stanza)\n",
    " - BasicDependenciesAnnotation\n",
    "   - _(aka basic)_ - just the 37 standard UD dependencies\n",
    "   - Easy to parse, small set\n",
    " - EnhancedDependenciesAnnotation : _(aka enhanced)_\n",
    "   - From the JSON response, this is **likely** available under the `sentence[enhancedDependencies]` key.\n",
    " - EnhancedPlusPlusDependenciesAnnotation : _(aka enhanced++)_\n",
    "   - From the JSON response, this is available under the `sentence[enhancedPlusPlusDependencies]` key.\n",
    "   - I heard about this from [coreNLP Dtree Visualizer](https://github.com/doug919/corenlp_dtree_visualizer) which converts the CoreNLP dependency tree into a format that can be visualized by spacy!.\n",
    "   - Deterministic transformations on the basic dependencies\n",
    "   - Lot more semantic information\n",
    "\n",
    "```python\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "with ud_enhancer.UniversalEnhancer(language=\"en\") as enhancer:\n",
    "    depparseFromStanza = nlp(\"This is a test\")\n",
    "    depparseEnhanced = enhancer.process(depparseFromStanza)\n",
    ")\n",
    "```\n",
    "\n",
    "### Stanza basic dependencies\n",
    "\n",
    "These are part of the 37 basic universal dependency relations. Meant to be \n",
    " - concise\n",
    " - easy to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', 5, 'nsubj')\n",
      "('Obama', 1, 'flat')\n",
      "('is', 5, 'cop')\n",
      "('the', 5, 'det')\n",
      "('president', 0, 'root')\n",
      "('of', 9, 'case')\n",
      "('the', 9, 'det')\n",
      "('United', 9, 'amod')\n",
      "('States', 5, 'nmod')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza enhanced dependencies\n",
    "\n",
    "EnhancedDependencies were created to fix weaknesses in basic dependencies _(maybe for some languages or for english itself)_. However, apparently even these were not enough and eventually the world got _enhanced++Dependencies__ which is what everyone seems to use now _(judging from examples)_\n",
    "\n",
    "This is implemented as an enhancer on top of the basic dependencies and for stanza, this requires the use of the CoreNLP java package via stanza. use `stanza.install_corenlp()` to install it from within jupyter or python repl. Two ways of invoking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classpath is None,  Perhaps you need to set the $CLASSPATH or $CORENLP_HOME environment variable to point to a CoreNLP install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Ref: https://github.com/stanfordnlp/stanza/issues/359\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# To get enhanced stanza dependencies. This is one way\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Needs the StanfordCoreNLP java process to be running\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanza\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mud_enhancer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mud_enhancer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m ud_enhancer\u001b[39m.\u001b[39;49mprocess_doc(doc, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/stanza/server/ud_enhancer.py:51\u001b[0m, in \u001b[0;36mprocess_doc\u001b[0;34m(doc, language, pronouns_pattern)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_doc\u001b[39m(doc, language\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pronouns_pattern\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     request \u001b[39m=\u001b[39m build_enhancer_request(doc, language, pronouns_pattern)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m send_request(request, Document, ENHANCER_JAVA, \u001b[39m\"\u001b[39;49m\u001b[39m$CLASSPATH\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/stanza/server/java_protobuf_requests.py:16\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(request, response_type, java_main, classpath)\u001b[0m\n\u001b[1;32m     14\u001b[0m classpath \u001b[39m=\u001b[39m resolve_classpath(classpath)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m classpath \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mClasspath is None,  Perhaps you need to set the $CLASSPATH or $CORENLP_HOME environment variable to point to a CoreNLP install.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m pipe \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mrun([\u001b[39m\"\u001b[39m\u001b[39mjava\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-cp\u001b[39m\u001b[39m\"\u001b[39m, classpath, java_main],\n\u001b[1;32m     18\u001b[0m                       \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mSerializeToString(),\n\u001b[1;32m     19\u001b[0m                       stdout\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE,\n\u001b[1;32m     20\u001b[0m                       check\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m response \u001b[39m=\u001b[39m response_type()\n",
      "\u001b[0;31mValueError\u001b[0m: Classpath is None,  Perhaps you need to set the $CLASSPATH or $CORENLP_HOME environment variable to point to a CoreNLP install."
     ]
    }
   ],
   "source": [
    "# Note that stanza.install_corenlp() installs it into ~/stanza_corenlp\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = \"/home/vamsi/stanza_corenlp\"\n",
    "\n",
    "# Ref: https://github.com/stanfordnlp/stanza/issues/359\n",
    "# To get enhanced stanza dependencies. This is one way\n",
    "# Needs the StanfordCoreNLP java process to be running\n",
    "import stanza.server.ud_enhancer as ud_enhancer\n",
    "ud_enhancer.process_doc(doc, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 23:30:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3a58b23c1b40a18ede5eec92ebfdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 23:30:25 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-06-11 23:30:25 INFO: Using device: cuda\n",
      "2023-06-11 23:30:25 INFO: Loading: tokenize\n",
      "2023-06-11 23:30:25 INFO: Loading: pos\n",
      "2023-06-11 23:30:25 INFO: Loading: lemma\n",
      "2023-06-11 23:30:25 INFO: Loading: depparse\n",
      "2023-06-11 23:30:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 1\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 2\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 3\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 4\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 5\n",
       "}\n",
       "node {\n",
       "  sentenceIndex: 0\n",
       "  index: 6\n",
       "}\n",
       "edge {\n",
       "  source: 2\n",
       "  target: 1\n",
       "  dep: \"nsubj\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 2\n",
       "  target: 5\n",
       "  dep: \"ccomp\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 5\n",
       "  target: 3\n",
       "  dep: \"mark\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 5\n",
       "  target: 4\n",
       "  dep: \"nsubj\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "edge {\n",
       "  source: 5\n",
       "  target: 6\n",
       "  dep: \"obj\"\n",
       "  isExtra: false\n",
       "  sourceCopy: 0\n",
       "  targetCopy: 0\n",
       "  language: UniversalEnglish\n",
       "}\n",
       "root: 2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza.server.ud_enhancer as ud_enhancer\n",
    "\n",
    "# Ref: https://github.com/stanfordnlp/stanza/issues/359\n",
    "enh_ud_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "with ud_enhancer.UniversalEnhancer(language=\"en\") as enhancer:\n",
    "    depparseFromStanza = enh_ud_nlp(\"John said that he loved mila\")\n",
    "    depparseEnhanced = enhancer.process(depparseFromStanza)\n",
    "\n",
    "# This gets the enhancedPlusPludDependencies\n",
    "depparseEnhanced.sentence[0].enhancedPlusPlusDependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize enhanced dependencies \n",
    "\n",
    "> A lot more information about visualization at [NLP_AnnotationRendering.ipynb](./NLP_AnnotationRendering.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query stanza resources\n",
    "\n",
    "Includes the names of processors, models etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"backward_charlm\": {\n",
      "        \"1billion\": {\n",
      "            \"md5\": \"1405948b125b4264fd17509d1b6175ca\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"md5\": \"19a16ba4b8526e54ef8ab21ca33cc423\"\n",
      "        },\n",
      "        \"pubmed\": {\n",
      "            \"md5\": \"f2c42f6b2f7205d2b254d6ab0fb13303\"\n",
      "        }\n",
      "    },\n",
      "    \"constituency\": {\n",
      "        \"ptb3_bert\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"dbfddfd4174f838e50426e6d04c4ccbd\"\n",
      "        },\n",
      "        \"wsj\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"c45dda8db73b10c89aa9c6dc51934e1d\"\n",
      "        },\n",
      "        \"wsj_bert\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"ad0f352e3917bd346ef66d1437002268\"\n",
      "        }\n",
      "    },\n",
      "    \"default_dependencies\": {\n",
      "        \"constituency\": [\n",
      "            {\n",
      "                \"model\": \"pretrain\",\n",
      "                \"package\": \"combined\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"forward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"backward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            }\n",
      "        ],\n",
      "        \"depparse\": [\n",
      "            {\n",
      "                \"model\": \"pretrain\",\n",
      "                \"package\": \"combined\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"forward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"backward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            }\n",
      "        ],\n",
      "        \"ner\": [\n",
      "            {\n",
      "                \"model\": \"pretrain\",\n",
      "                \"package\": \"fasttextcrawl\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"forward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"backward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            }\n",
      "        ],\n",
      "        \"pos\": [\n",
      "            {\n",
      "                \"model\": \"pretrain\",\n",
      "                \"package\": \"combined\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"forward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"backward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            }\n",
      "        ],\n",
      "        \"sentiment\": [\n",
      "            {\n",
      "                \"model\": \"pretrain\",\n",
      "                \"package\": \"combined\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"forward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            },\n",
      "            {\n",
      "                \"model\": \"backward_charlm\",\n",
      "                \"package\": \"1billion\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"default_md5\": \"cf3a792312e690b0dbdc2d17aa0bb792\",\n",
      "    \"default_processors\": {\n",
      "        \"constituency\": \"wsj\",\n",
      "        \"depparse\": \"combined\",\n",
      "        \"lemma\": \"combined\",\n",
      "        \"ner\": \"ontonotes\",\n",
      "        \"pos\": \"combined\",\n",
      "        \"sentiment\": \"sstplus\",\n",
      "        \"tokenize\": \"combined\"\n",
      "    },\n",
      "    \"depparse\": {\n",
      "        \"combined\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"3fbbd00f6ea0345d0f4c8324fe9200b7\"\n",
      "        },\n",
      "        \"craft\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"d98d891e43b217473d013c42aa18ed44\"\n",
      "        },\n",
      "        \"ewt\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"ewt\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"dbdbbc200ae155894eb1644276e8d3d8\"\n",
      "        },\n",
      "        \"genia\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"genia\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"5c675bf97180c37ea6d503c69195ffc1\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"gum\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"4fc0ddd88d6f723789f4920998d47c30\"\n",
      "        },\n",
      "        \"lines\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"lines\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"cc0a1f8240b84a2682d75f4c2b54f884\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"457b7bf1129248429baaf4cfeaab42da\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"partut\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"a211301cf85241f86fb473d931af6a53\"\n",
      "        }\n",
      "    },\n",
      "    \"forward_charlm\": {\n",
      "        \"1billion\": {\n",
      "            \"md5\": \"468b3377455fa0311565d46865f55afb\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"md5\": \"3d3863e27afd67bf356354a728b0fb76\"\n",
      "        },\n",
      "        \"pubmed\": {\n",
      "            \"md5\": \"3f734806be2c2f62c82fbb01421f78ec\"\n",
      "        }\n",
      "    },\n",
      "    \"lang_name\": \"English\",\n",
      "    \"lemma\": {\n",
      "        \"combined\": {\n",
      "            \"md5\": \"650a4859e8f99e283cb1e900cf537010\"\n",
      "        },\n",
      "        \"craft\": {\n",
      "            \"md5\": \"fe602af4a87e923faf5d4ce597a8be08\"\n",
      "        },\n",
      "        \"ewt\": {\n",
      "            \"md5\": \"bf346431a3b8abd12cdbe6abfa52abe3\"\n",
      "        },\n",
      "        \"genia\": {\n",
      "            \"md5\": \"b64c1c9433acaa7d14090ae07aa8eb08\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"md5\": \"0e53aaeb005eafa4c73e3fef5d167825\"\n",
      "        },\n",
      "        \"lines\": {\n",
      "            \"md5\": \"e7a2e8e130251c5e937dcc8e1fc373a5\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"md5\": \"4701f9523bad5ec916785387938deec4\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"md5\": \"85f9d05bf821cb8b248e7d0f0f83dc18\"\n",
      "        }\n",
      "    },\n",
      "    \"mwt\": {\n",
      "        \"ewt\": {\n",
      "            \"md5\": \"a6c2abd93ef4d65bcb985821dfcb6e2b\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"md5\": \"44eeb621251afc64f7d521763f4f0375\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"md5\": \"aecedcf1b2965b852aca5865ff574685\"\n",
      "        }\n",
      "    },\n",
      "    \"ner\": {\n",
      "        \"anatem\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"8b4b13670ba8cccde5645d9378071c19\"\n",
      "        },\n",
      "        \"bc4chemd\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"5cc779906b9e6fa1078b423c428b362c\"\n",
      "        },\n",
      "        \"bc5cdr\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"b0744897baf9d40165aca02f25e3f68d\"\n",
      "        },\n",
      "        \"bionlp13cg\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"980327713d51b2a161e26520be148413\"\n",
      "        },\n",
      "        \"conll03\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"glove\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"adc994f36ee4f8fcc5e5bceb3597f02e\"\n",
      "        },\n",
      "        \"conll03_bert\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"892891610a5d66635904081fd6376178\"\n",
      "        },\n",
      "        \"i2b2\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"d860eeb99b2fd96e5e26fc1bb821eda5\"\n",
      "        },\n",
      "        \"jnlpba\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"95cb87ec78ef4f9f98697e3fc6ff6144\"\n",
      "        },\n",
      "        \"linnaeus\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"a172a0814335da3c59093d3ce67c8bea\"\n",
      "        },\n",
      "        \"ncbi_disease\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"f426680a5069aad5b7ca8145448d25da\"\n",
      "        },\n",
      "        \"ontonotes\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"fasttextcrawl\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"83d88f840ed82d8457e93e62e65824c7\"\n",
      "        },\n",
      "        \"ontonotes_bert\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"0cdcfdbf4e001084820a520572bfba64\"\n",
      "        },\n",
      "        \"radiology\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"043dd91c838e272dbde3d28f7bcc9ed2\"\n",
      "        },\n",
      "        \"s800\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"pubmed\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"1f8d6dc302e9237c8d5db17be22de5d5\"\n",
      "        }\n",
      "    },\n",
      "    \"pos\": {\n",
      "        \"combined\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"512c0f22b8166d891103712620b32739\"\n",
      "        },\n",
      "        \"combined_electra\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"97ae0dae74a9c7c77df7a5b309d1c508\"\n",
      "        },\n",
      "        \"combined_roberta\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"3497a77b4899132b36c3277755aea4e5\"\n",
      "        },\n",
      "        \"craft\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"craft\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"4bf2f1a986f25e40c9f8311bd7007306\"\n",
      "        },\n",
      "        \"ewt\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"ewt\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"19928131f06f0e36d5121b9e001be99e\"\n",
      "        },\n",
      "        \"genia\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"genia\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"63601e6d3bd5b52028d80d609b10701e\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"gum\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"ec3627e756583ab5eefde85341cec407\"\n",
      "        },\n",
      "        \"lines\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"lines\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"6b9fa1e10ab7dd1066bbe9caa38c1f21\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"mimic\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"76782c1e585c9c953a9dbaa097c014e5\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"partut\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"62e87ca623f9faf03ce6c1a6ea20b120\"\n",
      "        }\n",
      "    },\n",
      "    \"pretrain\": {\n",
      "        \"combined\": {\n",
      "            \"md5\": \"3b77b53669901e532ead7c9cd7360c46\"\n",
      "        },\n",
      "        \"craft\": {\n",
      "            \"md5\": \"e4ba08c4922e4cf7141ab2a1ab20e8de\"\n",
      "        },\n",
      "        \"ewt\": {\n",
      "            \"md5\": \"3b77b53669901e532ead7c9cd7360c46\"\n",
      "        },\n",
      "        \"fasttextcrawl\": {\n",
      "            \"md5\": \"e4bd9507069f1233f802ef3219c05873\"\n",
      "        },\n",
      "        \"genia\": {\n",
      "            \"md5\": \"e4ba08c4922e4cf7141ab2a1ab20e8de\"\n",
      "        },\n",
      "        \"glove\": {\n",
      "            \"md5\": \"812e5bfad538ab7931cae586b0648f7a\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"md5\": \"3b77b53669901e532ead7c9cd7360c46\"\n",
      "        },\n",
      "        \"lines\": {\n",
      "            \"md5\": \"3b77b53669901e532ead7c9cd7360c46\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"md5\": \"1a32d1459a4bd7148920a250a189d01c\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"md5\": \"3b77b53669901e532ead7c9cd7360c46\"\n",
      "        }\n",
      "    },\n",
      "    \"sentiment\": {\n",
      "        \"sstplus\": {\n",
      "            \"dependencies\": [\n",
      "                {\n",
      "                    \"model\": \"pretrain\",\n",
      "                    \"package\": \"combined\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"forward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                },\n",
      "                {\n",
      "                    \"model\": \"backward_charlm\",\n",
      "                    \"package\": \"1billion\"\n",
      "                }\n",
      "            ],\n",
      "            \"md5\": \"7090bffc568b3a568236ffc1a13aca2d\"\n",
      "        }\n",
      "    },\n",
      "    \"tokenize\": {\n",
      "        \"combined\": {\n",
      "            \"md5\": \"ba418adac5f81e13ba17df7fc446880d\"\n",
      "        },\n",
      "        \"craft\": {\n",
      "            \"md5\": \"fc95d998b9ad6c94e4d683a2e22f4b83\"\n",
      "        },\n",
      "        \"ewt\": {\n",
      "            \"md5\": \"d91c317a1b54d4d867e5fce378acce15\"\n",
      "        },\n",
      "        \"genia\": {\n",
      "            \"md5\": \"1f9e66486fc7e38917c817e65fa05501\"\n",
      "        },\n",
      "        \"gum\": {\n",
      "            \"md5\": \"897d5e8541f8af1d5991a27898e31f91\"\n",
      "        },\n",
      "        \"lines\": {\n",
      "            \"md5\": \"b1235801e38ee5311dc1b18ee13e17b8\"\n",
      "        },\n",
      "        \"mimic\": {\n",
      "            \"md5\": \"2ff39c4010bd9b96ecd03835206e4589\"\n",
      "        },\n",
      "        \"partut\": {\n",
      "            \"md5\": \"61d892f9514b44d054b6cc829051e688\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from stanza.resources.common import load_resources_json\n",
    "\n",
    "def print_stanza_resources_for_lang(lang):\n",
    "    resources = load_resources_json()\n",
    "    if lang in resources:\n",
    "        print( json.dumps(resources[lang], sort_keys=True, indent=4))\n",
    "    else:\n",
    "        print(f\"No such language: {lang}\")\n",
    "    \n",
    "print_stanza_resources_for_lang('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
