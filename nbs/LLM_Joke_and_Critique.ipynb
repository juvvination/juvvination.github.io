{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke And Critique\n",
    "\n",
    "This notebook exercises multiple aspects of experimenting with basic LLM prompting\n",
    "\n",
    " - Initialization and use of python logging to keep dev logs separate from your jupyter cell output\n",
    "   - Collecting OpenAI low level logs if needed\n",
    "   - Generating app-level logs\n",
    " - Basic OpenAI ChatEndpoint API calls\n",
    "   - Turn 1: create a joke\n",
    "   - Turn 2: Critique that joke\n",
    " - Gradio UI with an input widget and two customized textarea widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "os.environ[\"OPENAI_LOG\"]=\"debug\"\n",
    "\n",
    "# Setup logging \n",
    "# Note that module needs to be reloaded for our config to take as Jupyter already configures it\n",
    "# which makes all future configs no-ops.\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.DEBUG, \n",
    "                    datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:55:20 DEBUG:Checking if OPENAI_API_KEY is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# You could do one of the two.\n",
    "# Either paste your OpenAI Key here or put it in secrets\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import userdata\n",
    "  logging.debug(\"Tryign to fetch OPENAI_API_KEY from your secrets. Remember to make it available to this notebook\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "logging.debug(\"Checking if OPENAI_API_KEY is available\")\n",
    "assert(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots has changed recently\n",
    "# See https://github.com/openai/openai-python/discussions/742 for migration to the new API\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0) -> str:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Use this to explore available loggers\n",
    "# If there is a logger and you are not provided an env-var top control log level, \n",
    "# you can directly call logger.setLevel(Logging.DEBUG) to collect logs.\n",
    "def get_available_loggers():\n",
    "    return [logging.getLogger(name) for name in logging.root.manager.loggerDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joke_and_critique(topic:str) -> str:\n",
    "    logging.debug(\"Starting joke_and_critique\")\n",
    "    joke = get_completion(\n",
    "        f\"\"\"Write your best joke about the following: {topic}\"\"\"\n",
    "    )\n",
    "    logging.debug(\"Done with Joke Generation\")\n",
    "\n",
    "    critique = get_completion(\n",
    "        f\"Give a thorough analysis and critique of the following joke: {joke}\")\n",
    "    logging.debug(\"Done with Critique Generation\")\n",
    "\n",
    "    return joke, critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:17:56 DEBUG:connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "06:17:56 DEBUG:Importing BlpImagePlugin\n",
      "06:17:56 DEBUG:Importing BmpImagePlugin\n",
      "06:17:56 DEBUG:Importing BufrStubImagePlugin\n",
      "06:17:56 DEBUG:Importing CurImagePlugin\n",
      "06:17:56 DEBUG:Importing DcxImagePlugin\n",
      "06:17:56 DEBUG:Importing DdsImagePlugin\n",
      "06:17:56 DEBUG:Importing EpsImagePlugin\n",
      "06:17:56 DEBUG:Importing FitsImagePlugin\n",
      "06:17:56 DEBUG:Importing FliImagePlugin\n",
      "06:17:56 DEBUG:Importing FpxImagePlugin\n",
      "06:17:56 DEBUG:Image: failed to import FpxImagePlugin: No module named 'olefile'\n",
      "06:17:56 DEBUG:Importing FtexImagePlugin\n",
      "06:17:56 DEBUG:Importing GbrImagePlugin\n",
      "06:17:56 DEBUG:Importing GifImagePlugin\n",
      "06:17:56 DEBUG:Importing GribStubImagePlugin\n",
      "06:17:56 DEBUG:Importing Hdf5StubImagePlugin\n",
      "06:17:56 DEBUG:Importing IcnsImagePlugin\n",
      "06:17:56 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd8623bcb0>\n",
      "06:17:56 DEBUG:Importing IcoImagePlugin\n",
      "06:17:56 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fcd8641c290> server_hostname='api.gradio.app' timeout=3\n",
      "06:17:56 DEBUG:Importing ImImagePlugin\n",
      "06:17:56 DEBUG:Importing ImtImagePlugin\n",
      "06:17:56 DEBUG:Importing IptcImagePlugin\n",
      "06:17:56 DEBUG:Importing JpegImagePlugin\n",
      "06:17:56 DEBUG:Importing Jpeg2KImagePlugin\n",
      "06:17:56 DEBUG:Importing McIdasImagePlugin\n",
      "06:17:56 DEBUG:Importing MicImagePlugin\n",
      "06:17:56 DEBUG:Image: failed to import MicImagePlugin: No module named 'olefile'\n",
      "06:17:56 DEBUG:Importing MpegImagePlugin\n",
      "06:17:56 DEBUG:Importing MpoImagePlugin\n",
      "06:17:56 DEBUG:Importing MspImagePlugin\n",
      "06:17:56 DEBUG:Importing PalmImagePlugin\n",
      "06:17:56 DEBUG:Importing PcdImagePlugin\n",
      "06:17:56 DEBUG:Importing PcxImagePlugin\n",
      "06:17:56 DEBUG:Importing PdfImagePlugin\n",
      "06:17:56 DEBUG:Importing PixarImagePlugin\n",
      "06:17:56 DEBUG:Importing PngImagePlugin\n",
      "06:17:56 DEBUG:Importing PpmImagePlugin\n",
      "06:17:56 DEBUG:Importing PsdImagePlugin\n",
      "06:17:56 DEBUG:Importing QoiImagePlugin\n",
      "06:17:56 DEBUG:Importing SgiImagePlugin\n",
      "06:17:56 DEBUG:Importing SpiderImagePlugin\n",
      "06:17:56 DEBUG:Importing SunImagePlugin\n",
      "06:17:56 DEBUG:Importing TgaImagePlugin\n",
      "06:17:56 DEBUG:Importing TiffImagePlugin\n",
      "06:17:56 DEBUG:Importing WebPImagePlugin\n",
      "06:17:56 DEBUG:Importing WmfImagePlugin\n",
      "06:17:56 DEBUG:Importing XbmImagePlugin\n",
      "06:17:56 DEBUG:Importing XpmImagePlugin\n",
      "06:17:56 DEBUG:Importing XVThumbImagePlugin\n",
      "06:17:56 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd85969450>\n",
      "06:17:56 DEBUG:send_request_headers.started request=<Request [b'GET']>\n",
      "06:17:56 DEBUG:send_request_headers.complete\n",
      "06:17:56 DEBUG:send_request_body.started request=<Request [b'GET']>\n",
      "06:17:56 DEBUG:send_request_body.complete\n",
      "06:17:56 DEBUG:receive_response_headers.started request=<Request [b'GET']>\n",
      "06:17:56 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 14:17:57 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'3'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "06:17:56 INFO:HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
      "06:17:56 DEBUG:receive_response_body.started request=<Request [b'GET']>\n",
      "06:17:56 DEBUG:receive_response_body.complete\n",
      "06:17:56 DEBUG:response_closed.started\n",
      "06:17:56 DEBUG:response_closed.complete\n",
      "06:17:56 DEBUG:close.started\n",
      "06:17:56 DEBUG:close.complete\n",
      "06:17:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "06:17:56 DEBUG:connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "06:17:57 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd7deff9d0>\n",
      "06:17:57 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fcd7f1aef00> server_hostname='api.gradio.app' timeout=3\n",
      "06:17:57 DEBUG:connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None\n",
      "06:17:57 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd7de26190>\n",
      "06:17:57 DEBUG:send_request_headers.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:send_request_headers.complete\n",
      "06:17:57 DEBUG:send_request_body.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:send_request_body.complete\n",
      "06:17:57 DEBUG:receive_response_headers.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 05 Mar 2025 14:17:57 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "06:17:57 INFO:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "06:17:57 DEBUG:receive_response_body.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:receive_response_body.complete\n",
      "06:17:57 DEBUG:response_closed.started\n",
      "06:17:57 DEBUG:response_closed.complete\n",
      "06:17:57 DEBUG:close.started\n",
      "06:17:57 DEBUG:close.complete\n",
      "06:17:57 DEBUG:connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
      "06:17:57 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd7de27100>\n",
      "06:17:57 DEBUG:send_request_headers.started request=<Request [b'HEAD']>\n",
      "06:17:57 DEBUG:send_request_headers.complete\n",
      "06:17:57 DEBUG:send_request_body.started request=<Request [b'HEAD']>\n",
      "06:17:57 DEBUG:send_request_body.complete\n",
      "06:17:57 DEBUG:receive_response_headers.started request=<Request [b'HEAD']>\n",
      "06:17:57 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 05 Mar 2025 14:17:57 GMT'), (b'server', b'uvicorn'), (b'content-length', b'12592'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "06:17:57 INFO:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "06:17:57 DEBUG:receive_response_body.started request=<Request [b'HEAD']>\n",
      "06:17:57 DEBUG:receive_response_body.complete\n",
      "06:17:57 DEBUG:response_closed.started\n",
      "06:17:57 DEBUG:response_closed.complete\n",
      "06:17:57 DEBUG:close.started\n",
      "06:17:57 DEBUG:close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:17:57 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:17:57 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd7d47a690>\n",
      "06:17:57 DEBUG:send_request_headers.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:send_request_headers.complete\n",
      "06:17:57 DEBUG:send_request_body.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:send_request_body.complete\n",
      "06:17:57 DEBUG:receive_response_headers.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 14:17:57 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "06:17:57 INFO:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "06:17:57 DEBUG:receive_response_body.started request=<Request [b'GET']>\n",
      "06:17:57 DEBUG:receive_response_body.complete\n",
      "06:17:57 DEBUG:response_closed.started\n",
      "06:17:57 DEBUG:response_closed.complete\n",
      "06:17:57 DEBUG:close.started\n",
      "06:17:57 DEBUG:close.complete\n",
      "06:17:57 DEBUG:https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "06:17:57 DEBUG:https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "06:18:59 DEBUG:Starting joke_and_critique\n",
      "06:18:59 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Write your best joke about the following: Politician'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "06:18:59 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "06:18:59 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "06:18:59 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcd7f2147c0>\n",
      "06:18:59 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fcd7d45cd40> server_hostname='api.openai.com' timeout=5.0\n",
      "06:18:59 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fcdfc36abe0>\n",
      "06:18:59 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:send_request_headers.complete\n",
      "06:18:59 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:send_request_body.complete\n",
      "06:18:59 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 14:19:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'432'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199970'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'9ms'), (b'x-request-id', b'req_59f14514a1bb368c0b873f9ad209f2dd'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=6AHLx1U1R7foOosXnhxDKWZUbYZIaPPG.m7UHLsGyXE-1741184340-1.0.1.1-z2kKGUmFTFm0BrwQFCTvX_1AYl.wu3ZqyjmzYBLACCxHO52e_LCxM74pTCsWzkbySGyYI.yGVqIS23zooyiK90.IUcyOsGjZCjq8SRkTECc; path=/; expires=Wed, 05-Mar-25 14:49:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=eyoT5jn.N1QkT9uZbry3rOpw2xXD8fWOvehuV9QJN7o-1741184340253-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91ba3eeb8f1edbaa-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "06:18:59 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "06:18:59 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:receive_response_body.complete\n",
      "06:18:59 DEBUG:response_closed.started\n",
      "06:18:59 DEBUG:response_closed.complete\n",
      "06:18:59 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 05 Mar 2025 14:19:00 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-uxl7oko9mdo17utucmetfrwn'), ('openai-processing-ms', '432'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199970'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '9ms'), ('x-request-id', 'req_59f14514a1bb368c0b873f9ad209f2dd'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=6AHLx1U1R7foOosXnhxDKWZUbYZIaPPG.m7UHLsGyXE-1741184340-1.0.1.1-z2kKGUmFTFm0BrwQFCTvX_1AYl.wu3ZqyjmzYBLACCxHO52e_LCxM74pTCsWzkbySGyYI.yGVqIS23zooyiK90.IUcyOsGjZCjq8SRkTECc; path=/; expires=Wed, 05-Mar-25 14:49:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=eyoT5jn.N1QkT9uZbry3rOpw2xXD8fWOvehuV9QJN7o-1741184340253-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '91ba3eeb8f1edbaa-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "06:18:59 DEBUG:request_id: req_59f14514a1bb368c0b873f9ad209f2dd\n",
      "06:18:59 DEBUG:Done with Joke Generation\n",
      "06:18:59 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Give a thorough analysis and critique of the following joke: Why did the politician bring a ladder to the debate?\\n\\nBecause they wanted to reach new heights in their arguments!'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "06:18:59 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "06:18:59 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:send_request_headers.complete\n",
      "06:18:59 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "06:18:59 DEBUG:send_request_body.complete\n",
      "06:18:59 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "06:19:10 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 14:19:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'8084'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199938'), (b'x-ratelimit-reset-requests', b'16.756s'), (b'x-ratelimit-reset-tokens', b'18ms'), (b'x-request-id', b'req_7d7743f858ecd2a6394cc44926a97ab3'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91ba3eeeca18dbaa-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "06:19:10 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "06:19:10 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "06:19:10 DEBUG:receive_response_body.complete\n",
      "06:19:10 DEBUG:response_closed.started\n",
      "06:19:10 DEBUG:response_closed.complete\n",
      "06:19:10 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 14:19:10 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '8084', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199938', 'x-ratelimit-reset-requests': '16.756s', 'x-ratelimit-reset-tokens': '18ms', 'x-request-id': 'req_7d7743f858ecd2a6394cc44926a97ab3', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91ba3eeeca18dbaa-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "06:19:10 DEBUG:request_id: req_7d7743f858ecd2a6394cc44926a97ab3\n",
      "06:19:10 DEBUG:Done with Critique Generation\n",
      "06:21:24 INFO:127.0.0.1:57282 - \"GET / HTTP/1.1\" 200\n",
      "06:21:24 INFO:127.0.0.1:57282 - \"GET /theme.css?v=63194d3741d384f9f85db890247b6c0ef9e7abac0f297f40a15c59fe4baba916 HTTP/1.1\" 200\n"
     ]
    }
   ],
   "source": [
    "# This is somehow completely broken on recent mamba release (3.12.5 and 3.13.2)\n",
    "# start with a new penv and use pip\n",
    "import gradio as gr\n",
    "\n",
    "ui = gr.Interface(fn=joke_and_critique, \n",
    "                  inputs=gr.Textbox(lines=1, placeholder=\"Joke Topic\", label=\"Joke Topic\"),\n",
    "                  outputs=[\n",
    "                      gr.Textbox(lines=3, placeholder=\"Joke\", label=\"Joke\"),\n",
    "                      gr.Textbox(lines=6, placeholder=\"Joke Critique\", label=\"Critique\"),\n",
    "                  ])\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
