{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke And Critique\n",
    "\n",
    "This notebook exercises multiple aspects of experimenting with basic LLM prompting\n",
    "\n",
    " - Initialization and use of python logging to keep dev logs separate from your jupyter cell output\n",
    "   - Collecting OpenAI low level logs if needed\n",
    "   - Generating app-level logs\n",
    " - Basic OpenAI ChatEndpoint API calls\n",
    "   - Turn 1: create a joke\n",
    "   - Turn 2: Critique that joke\n",
    " - Gradio UI with an input widget and two customized textarea widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "os.environ[\"OPENAI_LOG\"]=\"error\"\n",
    "\n",
    "# Setup logging \n",
    "# Note that module needs to be reloaded for our config to take as Jupyter already configures it\n",
    "# which makes all future configs no-ops.\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.DEBUG, \n",
    "                    datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:19 DEBUG:Checking if OPENAI_API_KEY is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# You could do one of the two.\n",
    "# Either paste your OpenAI Key here or put it in secrets\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import userdata\n",
    "  logging.debug(\"Tryign to fetch OPENAI_API_KEY from your secrets. Remember to make it available to this notebook\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "logging.debug(\"Checking if OPENAI_API_KEY is available\")\n",
    "assert(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots has changed recently\n",
    "# See https://github.com/openai/openai-python/discussions/742 for migration to the new API\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0) -> str:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joke_and_critique(topic:str) -> str:\n",
    "    logging.debug(\"Starting joke_and_critique\")\n",
    "    joke = get_completion(\n",
    "        f\"\"\"Write your best joke about the following: {topic}\"\"\"\n",
    "    )\n",
    "    logging.debug(\"Done with Joke Generation\")\n",
    "\n",
    "    critique = get_completion(\n",
    "        f\"Give a thorough analysis and critique of the following joke: {joke}\")\n",
    "    logging.debug(\"Done with Critique Generation\")\n",
    "\n",
    "    return joke, critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:22 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:11:22 DEBUG:connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "02:11:22 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce0aab4d0>\n",
      "02:11:22 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f3d60139d90> server_hostname='api.gradio.app' timeout=3\n",
      "02:11:22 DEBUG:connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None\n",
      "02:11:22 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce0862ea0>\n",
      "02:11:22 DEBUG:send_request_headers.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:send_request_headers.complete\n",
      "02:11:22 DEBUG:send_request_body.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:send_request_body.complete\n",
      "02:11:22 DEBUG:receive_response_headers.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 05 Mar 2025 22:11:22 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "02:11:22 INFO:HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "02:11:22 DEBUG:receive_response_body.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:receive_response_body.complete\n",
      "02:11:22 DEBUG:response_closed.started\n",
      "02:11:22 DEBUG:response_closed.complete\n",
      "02:11:22 DEBUG:close.started\n",
      "02:11:22 DEBUG:close.complete\n",
      "02:11:22 DEBUG:connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None\n",
      "02:11:22 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce0863e10>\n",
      "02:11:22 DEBUG:send_request_headers.started request=<Request [b'HEAD']>\n",
      "02:11:22 DEBUG:send_request_headers.complete\n",
      "02:11:22 DEBUG:send_request_body.started request=<Request [b'HEAD']>\n",
      "02:11:22 DEBUG:send_request_body.complete\n",
      "02:11:22 DEBUG:receive_response_headers.started request=<Request [b'HEAD']>\n",
      "02:11:22 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 05 Mar 2025 22:11:22 GMT'), (b'server', b'uvicorn'), (b'content-length', b'12591'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "02:11:22 INFO:HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n",
      "02:11:22 DEBUG:receive_response_body.started request=<Request [b'HEAD']>\n",
      "02:11:22 DEBUG:receive_response_body.complete\n",
      "02:11:22 DEBUG:response_closed.started\n",
      "02:11:22 DEBUG:response_closed.complete\n",
      "02:11:22 DEBUG:close.started\n",
      "02:11:22 DEBUG:close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:22 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce09ce450>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:22 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:22 DEBUG:send_request_headers.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:send_request_headers.complete\n",
      "02:11:22 DEBUG:send_request_body.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:send_request_body.complete\n",
      "02:11:22 DEBUG:receive_response_headers.started request=<Request [b'GET']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:11:22 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 22:11:24 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "02:11:22 INFO:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "02:11:22 DEBUG:receive_response_body.started request=<Request [b'GET']>\n",
      "02:11:22 DEBUG:receive_response_body.complete\n",
      "02:11:22 DEBUG:response_closed.started\n",
      "02:11:22 DEBUG:response_closed.complete\n",
      "02:11:22 DEBUG:close.started\n",
      "02:11:22 DEBUG:close.complete\n",
      "02:11:22 DEBUG:https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "02:11:23 DEBUG:https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "02:11:29 DEBUG:Starting joke_and_critique\n",
      "02:11:29 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Write your best joke about the following: sailors'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "02:11:29 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "02:11:29 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "02:11:29 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce0a3fdf0>\n",
      "02:11:29 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f3ce08d2720> server_hostname='api.openai.com' timeout=5.0\n",
      "02:11:29 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3ce0a3e470>\n",
      "02:11:29 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:send_request_headers.complete\n",
      "02:11:29 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:send_request_body.complete\n",
      "02:11:29 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 22:11:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'425'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199970'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'9ms'), (b'x-request-id', b'req_092c754b07c53e0b508f4449ad286345'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=wdU8FogEdGaDQHcN0MtPLyAeOVwb2dapB6SrvSvQros-1741212691-1.0.1.1-V5bp9JnsqX1n3RRxw8Qj_.ipzOqm9437tLTjxZceJLznvFhpaRFXlg54l5wKckrI9elUOvbG51y0SYcPh_yyvn6oLdY3OLGgKqU5W.sqsQU; path=/; expires=Wed, 05-Mar-25 22:41:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=3nogbg240.To3Cc9ZecyandrKUwQmgNJ3j3MgyJBxYw-1741212691143-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bcf3149e4e2eab-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "02:11:29 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "02:11:29 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:receive_response_body.complete\n",
      "02:11:29 DEBUG:response_closed.started\n",
      "02:11:29 DEBUG:response_closed.complete\n",
      "02:11:29 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 05 Mar 2025 22:11:31 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-uxl7oko9mdo17utucmetfrwn'), ('openai-processing-ms', '425'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199970'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '9ms'), ('x-request-id', 'req_092c754b07c53e0b508f4449ad286345'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=wdU8FogEdGaDQHcN0MtPLyAeOVwb2dapB6SrvSvQros-1741212691-1.0.1.1-V5bp9JnsqX1n3RRxw8Qj_.ipzOqm9437tLTjxZceJLznvFhpaRFXlg54l5wKckrI9elUOvbG51y0SYcPh_yyvn6oLdY3OLGgKqU5W.sqsQU; path=/; expires=Wed, 05-Mar-25 22:41:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=3nogbg240.To3Cc9ZecyandrKUwQmgNJ3j3MgyJBxYw-1741212691143-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '91bcf3149e4e2eab-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "02:11:29 DEBUG:request_id: req_092c754b07c53e0b508f4449ad286345\n",
      "02:11:29 DEBUG:Done with Joke Generation\n",
      "02:11:29 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Give a thorough analysis and critique of the following joke: Why did the sailor bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "02:11:29 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "02:11:29 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:send_request_headers.complete\n",
      "02:11:29 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "02:11:29 DEBUG:send_request_body.complete\n",
      "02:11:29 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "02:11:38 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 22:11:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'8844'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199943'), (b'x-ratelimit-reset-requests', b'16.761s'), (b'x-ratelimit-reset-tokens', b'16ms'), (b'x-request-id', b'req_e411dad7fc4511d93746e2abc8bfaded'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bcf317db9b2eab-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "02:11:38 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "02:11:38 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "02:11:38 DEBUG:receive_response_body.complete\n",
      "02:11:38 DEBUG:response_closed.started\n",
      "02:11:38 DEBUG:response_closed.complete\n",
      "02:11:38 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 22:11:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '8844', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199943', 'x-ratelimit-reset-requests': '16.761s', 'x-ratelimit-reset-tokens': '16ms', 'x-request-id': 'req_e411dad7fc4511d93746e2abc8bfaded', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bcf317db9b2eab-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "02:11:38 DEBUG:request_id: req_e411dad7fc4511d93746e2abc8bfaded\n",
      "02:11:38 DEBUG:Done with Critique Generation\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import gradio as gr\n",
    "except:\n",
    "  !pip install gradio\n",
    "  \n",
    "# This is somehow completely broken on recent mamba python releases (3.12.5 and 3.13.2)\n",
    "# start with a new penv and use pip\n",
    "import gradio as gr\n",
    "\n",
    "ui = gr.Interface(fn=joke_and_critique, \n",
    "                  inputs=gr.Textbox(lines=1, placeholder=\"Joke Topic\", label=\"Joke Topic\"),\n",
    "                  outputs=[\n",
    "                      gr.Textbox(lines=3, placeholder=\"Joke\", label=\"Joke\"),\n",
    "                      gr.Textbox(lines=6, placeholder=\"Joke Critique\", label=\"Critique\"),\n",
    "                  ])\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
